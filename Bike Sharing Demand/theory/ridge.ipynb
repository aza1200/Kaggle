{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input directory path : ../input/\n",
      "train shape : (10886, 12)\n",
      "test_shape : (6493, 9)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 캐글 ↓제출경로\n",
    "data_path = '/kaggle/input/bike-sharing-demand/'\n",
    "if not path.exists(data_path):\n",
    "    data_path = '../input/'\n",
    "print(f'input directory path : {data_path}')\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv')\n",
    "test = pd.read_csv(data_path + 'test.csv')\n",
    "submission = pd.read_csv(data_path + 'sampleSubmission.csv')\n",
    "\n",
    "print(f'train shape : {train.shape}')\n",
    "print(f'test_shape : {test.shape}')\n",
    "\n",
    "# 날씨 weather ==> 4 인것 제거,\n",
    "train = train[train['weather'] != 4]\n",
    "\n",
    "# train 과 test 데이터 합치기 (index 역시 합쳐야 한다.)\n",
    "\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# 날짜 피처 생성\n",
    "all_data['date'] = all_data['datetime'].apply(lambda x : x.split()[0])\n",
    "\n",
    "# 연도 피처\n",
    "all_data['year'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[0])\n",
    "\n",
    "# 월피처\n",
    "all_data['month'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[1])\n",
    "\n",
    "# 시 피처 생성\n",
    "all_data['hour'] = all_data['datetime'].apply(lambda x: x.split()[1].split(':')[0])\n",
    "\n",
    "# 요일 피처 생성\n",
    "all_data['weekday'] = all_data['date'].apply(lambda dateString : datetime.strptime(dateString, \"%Y-%m-%d\").weekday())\n",
    "\n",
    "# 훈련데이터 매달 1일~19일, 테스트 데이터 매달 20일~ 이므로 day 사용필요 x \n",
    "# minute, second 역시 사용필요 x (상식적)\n",
    "all_data.head()\n",
    "\n",
    "\n",
    "# all_data['datetime'] = pd.to_datetime(all_data['datetime'])\n",
    "# all_data['year'] = all_data['datetime'].dt.year , dt.month, dt.hour, dt.weekday 로 변경가능\n",
    "\n",
    "\n",
    "# 필요 없는 피처 제거\n",
    "\n",
    "# 1. casual 과 registered 피처는 테스트 데이터 에 없는 피처이므로 제거\n",
    "# 2. datetime 피처는  date(year,month,day) 피처가 가지고 있기에 필요 x\n",
    "# 3. season 피처가 month 대분류이기에 month 제거\n",
    "# 4. windspeed 상관관계 약하기에 제거 한다 \n",
    "\n",
    "drop_features = ['casual', 'registered', 'datetime', 'date', 'month', 'windspeed']\n",
    "all_data = all_data.drop(drop_features, axis=1)\n",
    "\n",
    "all_data\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X_train = all_data[~pd.isnull(all_data['count'])]\n",
    "X_test = all_data[pd.isnull(all_data['count'])]\n",
    "\n",
    "# 타깃값 count 제거\n",
    "X_train = X_train.drop(['count'], axis=1)\n",
    "X_test = X_test.drop(['count'], axis=1)\n",
    "\n",
    "y = train['count'] # 타깃값 \n",
    "\n",
    "X_train.head()\n",
    "\n",
    "# 테스트 데이터 왜있는지....\n",
    "# 평가 지표 계싼 함수 작성\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rmsle(y_true, y_pred, convertExp=True):\n",
    "    # 지수변환\n",
    "    if convertExp:\n",
    "        y_true = np.exp(y_true)\n",
    "        y_pred = np.exp(y_pred)\n",
    "        \n",
    "    log_true = np.nan_to_num(np.log(y_true+1))\n",
    "    log_pred = np.nan_to_num(np.log(y_pred+1))\n",
    "    \n",
    "    # RMSLE 계산\n",
    "    output = np.sqrt(np.mean((log_true -log_pred)**2))\n",
    "    return output\n",
    "\n",
    "# sklearn.metrics.mean_squared_log_error 이용시 대체가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10885,11) and (6,) not aligned: 11 (dim 1) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43204\\767833539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Calculate the current cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_log_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Calculate the current gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_log_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43204\\767833539.py\u001b[0m in \u001b[0;36mridge_cost\u001b[1;34m(X, y, weights, alpha)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mridge_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Calculate the mean squared error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Add the L2 regularization term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0ml2_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10885,11) and (6,) not aligned: 11 (dim 1) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "np_x_train = X_train.to_numpy().astype(float)\n",
    "\n",
    "log_y = np.log(y)\n",
    "np_log_y = log_y.to_numpy().reshape(len(log_y),1)\n",
    "insert_column = np.ones_like(np_log_y)\n",
    "np_x_train = np.c_[np_x_train, insert_column]\n",
    "\n",
    "alpha = 0.1\n",
    "max_iter = 3000\n",
    "\n",
    "# Define the learning rate for gradient descent\n",
    "eta = 0.001\n",
    "\n",
    "# Initialize the weights with random values\n",
    "weights = np.zeros((1,11))\n",
    "\n",
    "# Define the cost function for Ridge regression\n",
    "def ridge_cost(X, y, weights, alpha):\n",
    "    # Calculate the mean squared error\n",
    "    mse = np.mean((y - X.dot(weights)) ** 2)\n",
    "    # Add the L2 regularization term\n",
    "    l2_reg = alpha * np.sum(weights[:-1] ** 2)\n",
    "    # Return the total cost\n",
    "    return mse + l2_reg\n",
    "\n",
    "# Define the gradient function for Ridge regression\n",
    "def ridge_gradient(X, y, weights, alpha):\n",
    "    # Calculate the residual\n",
    "    residual = X.dot(weights) - y\n",
    "    # Calculate the gradient for the weights\n",
    "    grad = 2 * X.T.dot(residual)\n",
    "    # Add the L2 regularization term\n",
    "    grad[:-1] += 2 * alpha * weights[:-1]\n",
    "    # Return the gradient\n",
    "    return grad\n",
    "\n",
    "# Perform gradient descent to optimize the weights\n",
    "for i in range(max_iter):\n",
    "    # Calculate the current cost\n",
    "    cost = ridge_cost(np_x_train, np_log_y, weights, alpha)\n",
    "    # Calculate the current gradient\n",
    "    grad = ridge_gradient(np_x_train, np_log_y, weights, alpha)\n",
    "    # Update the weights\n",
    "    weights -= eta * grad\n",
    "    # Print the current cost every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}: cost={cost:.4f}')\n",
    "        \n",
    "# Make predictions on new data\n",
    "X_new = np.random.rand(10, 5)\n",
    "X_new = np.concatenate([X_new, np.ones((10, 1))], axis=1)\n",
    "y_pred = X_new.dot(weights[:-1]) + weights[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
