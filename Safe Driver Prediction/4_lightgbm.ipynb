{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능향상 기법\n",
    "# 1. 파생피처 추가\n",
    "# 2. 베이지안 최적화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 라인 모델로 Light BGM 모델을 사용할거임 \n",
    "# 책에서는 파이썬 래퍼 LightGMB 과 사이킷런 LightGBM 중 파이썬 래퍼 LightBGM 사용\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "# 캐글 ↓제출경로\n",
    "data_path = './input/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col='id')\n",
    "test  = pd.read_csv(data_path + 'test.csv' , index_col='id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')\n",
    "\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거 \n",
    "all_features = all_data.columns\n",
    "\n",
    "# 명목형 피처는 보통 웟-핫 인코딩 적용하는게 정석인듯 하다 \n",
    "# 명목형 피처 추출\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "# 원 핫 인코딩\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생 피처 추가 \n",
    "# 데이터 하나당 결측값 개수를 파생 피처로 추가\n",
    "\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처, calc 분류의 피처를 제외한 피처\n",
    "remaining_features = [\n",
    "    feature for feature in all_features\n",
    "    if ('cat' not in feature and 'calc' not in feature)\n",
    "]\n",
    "\n",
    "# num_missing 을 remaining_features 에 추가\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "                           ...                  \n",
       "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
       "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
       "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
       "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
       "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
       "Name: mix_ind, Length: 1488028, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 갑자기 뜬금없는 피처 엔지니어링 \n",
    "\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_featrue in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_featrue].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_featrue].astype(str) + '_'\n",
    "        \n",
    "all_data['mix_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1079327\n",
       " 2     309747\n",
       " 3      70172\n",
       " 4      28259\n",
       "-1        523\n",
       "Name: ps_ind_02_cat, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "for feature in cat_features + ['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "    \n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat_count',\n",
       " 'ps_ind_04_cat_count',\n",
       " 'ps_ind_05_cat_count',\n",
       " 'ps_car_01_cat_count',\n",
       " 'ps_car_02_cat_count',\n",
       " 'ps_car_03_cat_count',\n",
       " 'ps_car_04_cat_count',\n",
       " 'ps_car_05_cat_count',\n",
       " 'ps_car_06_cat_count',\n",
       " 'ps_car_07_cat_count',\n",
       " 'ps_car_08_cat_count',\n",
       " 'ps_car_09_cat_count',\n",
       " 'ps_car_10_cat_count',\n",
       " 'ps_car_11_cat_count',\n",
       " 'mix_ind_count']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             6\n",
       "1            36\n",
       "2            24\n",
       "3          2784\n",
       "4           258\n",
       "           ... \n",
       "1488023     107\n",
       "1488024      26\n",
       "1488025     258\n",
       "1488026      37\n",
       "1488027      67\n",
       "Name: mix_ind_count, Length: 1488028, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없다고 판단한 피처는 제거한 다음 모든 데이터 합치기 \n",
    "from scipy import sparse \n",
    "\n",
    "# 필요 없는 피처들\n",
    "drop_features = [\n",
    "    'ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
    "    'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14'\n",
    "]\n",
    "\n",
    "# remaining_features, cat_count_features 에서 drop_features 를 제거한 데이터 \n",
    "all_data_remaining = all_data[remaining_features + cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "# 데이터 합치기\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix],\n",
    "                               format='csr'\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_data_sprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 나누기\n",
    "num_train = len(train)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 베이지안 최적화 데이터셋\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼 파라미터 범위 설정 \n",
    "\n",
    "param_bounds = {\n",
    "    'num_leaves': (30,40), # 개별 트리가 가질수 있는 최대 말단 노드 개수\n",
    "    'lambda_l1': (0.7, 0.9), # L1 규제 조정값 \n",
    "    'lambda_l2': (0.9, 1), # L2 규제 조정값 \n",
    "    'feature_fraction': (0.6, 0.7), # 개별 트리를 훈련할때 사용하는 피처 샘플링 비율 \n",
    "    'bagging_fraction': (0.6, 0.9), # 개별 트리 훈련할때 사용할 데이터 샘플링 비율  \n",
    "    'min_child_samples': (6, 10), # 말단 노드가 되기 위해 필요한 최소 데이터 갯수\n",
    "    'min_child_weight': (10, 40) # 과대적합 방지를 위한 값 \n",
    "}\n",
    "\n",
    "# 값이 고정된 하이퍼 파라미터\n",
    "fixed_params = {\n",
    "    'objective': 'binary', # 이진분류라서 binary 설정\n",
    "    'learning_rate': 0.005, # 학습률은 0.005\n",
    "    'bagging_freq': 1, # 배깅 수행 빈도는 1 \n",
    "    'force_row_wise': True,  # 경고문구를 없애기 위해 True 설정\n",
    "    'random_state': 1991 # 설정 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실젯값과 예측값의 크기가 서로 같은지 확인 (값이 다르면 오류 발생 )\n",
    "    \n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace( 1/ n_samples, 1, n_samples) # 1/n_samples~ 1 까지 n_samples-1 개의 구간으로 나누겠다. \n",
    "    \n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)\n",
    "    \n",
    "    # 2) 예측 완벽할때 지니계수\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    G_true = np.sum(L_mid - L_true)\n",
    "    \n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label() # get_label 은 데이터셋의 타깃값 반환\n",
    "    return 'gini', eval_gini(labels, preds) , True # 반환값 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (베이지안 최적화용) 평가지표 계산 함수 작성 \n",
    "\n",
    "def eval_function(num_leaves , lambda_l1, lambda_l2, feature_fraction, \n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화 하려는 평가지표(지니계수) 계산 함수 '''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼 파라미터\n",
    "    params = {\n",
    "        'num_leaves': int(round(num_leaves)),\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight' : min_child_weight,\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "    \n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼 파라미터 : ',params)\n",
    "    \n",
    "    # LightGBM 모델 훈련 \n",
    "    \n",
    "    lgb_model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=bayes_dtrain,\n",
    "        num_boost_round=2500,\n",
    "        valid_sets=bayes_dvalid,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # 검증 데이터로 예측 수행 \n",
    "    preds = lgb_model.predict(X_valid)\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성 \n",
    "optimizer = BayesianOptimization(f=eval_function, # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds, # 하이퍼 파라미터 범위\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼 파라미터 :  {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28554700343777367\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2855   \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m0.8206   \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m7.695    \u001b[0m | \u001b[0m29.38    \u001b[0m | \u001b[0m34.38    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28378832768916534\n",
      "\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2838   \u001b[0m | \u001b[0m0.8675   \u001b[0m | \u001b[0m0.6964   \u001b[0m | \u001b[0m0.7767   \u001b[0m | \u001b[0m0.9792   \u001b[0m | \u001b[0m8.116    \u001b[0m | \u001b[0m27.04    \u001b[0m | \u001b[0m39.26    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28553924378329776\n",
      "\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2855   \u001b[0m | \u001b[0m0.6213   \u001b[0m | \u001b[0m0.6087   \u001b[0m | \u001b[0m0.704    \u001b[0m | \u001b[0m0.9833   \u001b[0m | \u001b[0m9.113    \u001b[0m | \u001b[0m36.1     \u001b[0m | \u001b[0m39.79    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2830327176874422\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.283    \u001b[0m | \u001b[0m0.8978   \u001b[0m | \u001b[0m0.6594   \u001b[0m | \u001b[0m0.8445   \u001b[0m | \u001b[0m0.9234   \u001b[0m | \u001b[0m8.619    \u001b[0m | \u001b[0m10.55    \u001b[0m | \u001b[0m30.09    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2851291345200033\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.2851   \u001b[0m | \u001b[0m0.7667   \u001b[0m | \u001b[0m0.6606   \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m0.9033   \u001b[0m | \u001b[0m8.769    \u001b[0m | \u001b[0m29.31    \u001b[0m | \u001b[0m36.6     \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 33, 'lambda_l1': 0.8652404547317643, 'lambda_l2': 0.9223944605589859, 'feature_fraction': 0.6525139308423109, 'bagging_fraction': 0.8990136629850742, 'min_child_samples': 10, 'min_child_weight': 34.93746220111729, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2841661124720632\n",
      "\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2842   \u001b[0m | \u001b[0m0.899    \u001b[0m | \u001b[0m0.6525   \u001b[0m | \u001b[0m0.8652   \u001b[0m | \u001b[0m0.9224   \u001b[0m | \u001b[0m9.828    \u001b[0m | \u001b[0m34.94    \u001b[0m | \u001b[0m33.04    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 38, 'lambda_l1': 0.7207248405861013, 'lambda_l2': 0.9908596534847374, 'feature_fraction': 0.6362085873266162, 'bagging_fraction': 0.6, 'min_child_samples': 6, 'min_child_weight': 33.9471793967531, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2851552116496103\n",
      "\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2852   \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.6362   \u001b[0m | \u001b[0m0.7207   \u001b[0m | \u001b[0m0.9909   \u001b[0m | \u001b[0m6.469    \u001b[0m | \u001b[0m33.95    \u001b[0m | \u001b[0m38.37    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 40, 'lambda_l1': 0.7672855841634896, 'lambda_l2': 0.9768876584971635, 'feature_fraction': 0.6519643254785416, 'bagging_fraction': 0.8892105163800896, 'min_child_samples': 6, 'min_child_weight': 39.17165367785253, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28515214660631016\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2852   \u001b[0m | \u001b[0m0.8892   \u001b[0m | \u001b[0m0.652    \u001b[0m | \u001b[0m0.7673   \u001b[0m | \u001b[0m0.9769   \u001b[0m | \u001b[0m6.378    \u001b[0m | \u001b[0m39.17    \u001b[0m | \u001b[0m39.98    \u001b[0m |\n",
      "하이퍼 파라미터 :  {'num_leaves': 31, 'lambda_l1': 0.7666126427497625, 'lambda_l2': 0.953427422198884, 'feature_fraction': 0.6535635442606814, 'bagging_fraction': 0.8406992841576415, 'min_child_samples': 10, 'min_child_weight': 26.792694730461516, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.284430276342444\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.2844   \u001b[0m | \u001b[0m0.8407   \u001b[0m | \u001b[0m0.6536   \u001b[0m | \u001b[0m0.7666   \u001b[0m | \u001b[0m0.9534   \u001b[0m | \u001b[0m9.675    \u001b[0m | \u001b[0m26.79    \u001b[0m | \u001b[0m31.26    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.7646440511781974,\n",
       " 'feature_fraction': 0.6715189366372419,\n",
       " 'lambda_l1': 0.8205526752143287,\n",
       " 'lambda_l2': 0.9544883182996897,\n",
       " 'min_child_samples': 7.694619197355619,\n",
       " 'min_child_weight': 29.376823391999682,\n",
       " 'num_leaves': 34.37587211262692}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_leaves 와 min_child_samples 는 원래 정수형 하이퍼 파라미터이므로 정수형으로 변환하여 다시 저장 \n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼 파라미터 추가 \n",
    "max_params.update(fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.7646440511781974,\n",
       " 'feature_fraction': 0.6715189366372419,\n",
       " 'lambda_l1': 0.8205526752143287,\n",
       " 'lambda_l2': 0.9544883182996897,\n",
       " 'min_child_samples': 8,\n",
       " 'min_child_weight': 29.376823391999682,\n",
       " 'num_leaves': 34,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 및 성능 검증  \n",
    "# 6,7 장에서 사용한 그리드 서치와 달리 베이지안 최적화는 최적 예측기 (최적 하이퍼 파라미터 값들로 훈련된 모델)을 제공해\n",
    "# 주지 않는다. 따라서 베이지안 최적화로 찾은 하이퍼 파라미터를 활용해 Light GBM 다시 훈련시켜야함 \n",
    "# LightGBM 훈련시 사용한 파라미터만 max_params로 바꿨을 뿐임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154282\tvalid_0's gini: 0.266871\n",
      "[200]\tvalid_0's binary_logloss: 0.153223\tvalid_0's gini: 0.273142\n",
      "[300]\tvalid_0's binary_logloss: 0.152631\tvalid_0's gini: 0.278209\n",
      "[400]\tvalid_0's binary_logloss: 0.152272\tvalid_0's gini: 0.281871\n",
      "[500]\tvalid_0's binary_logloss: 0.15203\tvalid_0's gini: 0.285298\n",
      "[600]\tvalid_0's binary_logloss: 0.151872\tvalid_0's gini: 0.287682\n",
      "[700]\tvalid_0's binary_logloss: 0.151758\tvalid_0's gini: 0.289711\n",
      "[800]\tvalid_0's binary_logloss: 0.151669\tvalid_0's gini: 0.291441\n",
      "[900]\tvalid_0's binary_logloss: 0.151605\tvalid_0's gini: 0.292794\n",
      "[1000]\tvalid_0's binary_logloss: 0.151562\tvalid_0's gini: 0.293703\n",
      "[1100]\tvalid_0's binary_logloss: 0.151525\tvalid_0's gini: 0.294598\n",
      "[1200]\tvalid_0's binary_logloss: 0.151492\tvalid_0's gini: 0.295385\n",
      "[1300]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.296024\n",
      "[1400]\tvalid_0's binary_logloss: 0.151449\tvalid_0's gini: 0.296409\n",
      "[1500]\tvalid_0's binary_logloss: 0.15143\tvalid_0's gini: 0.296846\n",
      "[1600]\tvalid_0's binary_logloss: 0.151422\tvalid_0's gini: 0.297044\n",
      "[1700]\tvalid_0's binary_logloss: 0.15141\tvalid_0's gini: 0.297302\n",
      "[1800]\tvalid_0's binary_logloss: 0.151397\tvalid_0's gini: 0.297643\n",
      "[1900]\tvalid_0's binary_logloss: 0.151392\tvalid_0's gini: 0.297858\n",
      "[2000]\tvalid_0's binary_logloss: 0.151383\tvalid_0's gini: 0.298106\n",
      "[2100]\tvalid_0's binary_logloss: 0.151374\tvalid_0's gini: 0.298305\n",
      "[2200]\tvalid_0's binary_logloss: 0.151371\tvalid_0's gini: 0.298381\n",
      "[2300]\tvalid_0's binary_logloss: 0.151373\tvalid_0's gini: 0.298277\n",
      "[2400]\tvalid_0's binary_logloss: 0.151371\tvalid_0's gini: 0.298374\n",
      "[2500]\tvalid_0's binary_logloss: 0.15137\tvalid_0's gini: 0.298379\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2256]\tvalid_0's binary_logloss: 0.151368\tvalid_0's gini: 0.298417\n",
      "폴드 1 지니계수 : 0.29841701876318205\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154382\tvalid_0's gini: 0.254841\n",
      "[200]\tvalid_0's binary_logloss: 0.15339\tvalid_0's gini: 0.261398\n",
      "[300]\tvalid_0's binary_logloss: 0.152844\tvalid_0's gini: 0.266455\n",
      "[400]\tvalid_0's binary_logloss: 0.15252\tvalid_0's gini: 0.27011\n",
      "[500]\tvalid_0's binary_logloss: 0.15231\tvalid_0's gini: 0.273222\n",
      "[600]\tvalid_0's binary_logloss: 0.152179\tvalid_0's gini: 0.275325\n",
      "[700]\tvalid_0's binary_logloss: 0.152073\tvalid_0's gini: 0.277343\n",
      "[800]\tvalid_0's binary_logloss: 0.151999\tvalid_0's gini: 0.278893\n",
      "[900]\tvalid_0's binary_logloss: 0.151947\tvalid_0's gini: 0.28002\n",
      "[1000]\tvalid_0's binary_logloss: 0.151897\tvalid_0's gini: 0.281289\n",
      "[1100]\tvalid_0's binary_logloss: 0.151864\tvalid_0's gini: 0.28205\n",
      "[1200]\tvalid_0's binary_logloss: 0.151841\tvalid_0's gini: 0.282626\n",
      "[1300]\tvalid_0's binary_logloss: 0.151811\tvalid_0's gini: 0.283536\n",
      "[1400]\tvalid_0's binary_logloss: 0.151794\tvalid_0's gini: 0.284005\n",
      "[1500]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.284581\n",
      "[1600]\tvalid_0's binary_logloss: 0.151763\tvalid_0's gini: 0.284781\n",
      "[1700]\tvalid_0's binary_logloss: 0.151751\tvalid_0's gini: 0.285031\n",
      "[1800]\tvalid_0's binary_logloss: 0.151743\tvalid_0's gini: 0.285268\n",
      "[1900]\tvalid_0's binary_logloss: 0.151738\tvalid_0's gini: 0.28538\n",
      "[2000]\tvalid_0's binary_logloss: 0.151736\tvalid_0's gini: 0.285434\n",
      "[2100]\tvalid_0's binary_logloss: 0.151736\tvalid_0's gini: 0.285396\n",
      "[2200]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.285394\n",
      "[2300]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.285373\n",
      "[2400]\tvalid_0's binary_logloss: 0.151738\tvalid_0's gini: 0.285461\n",
      "Early stopping, best iteration is:\n",
      "[2175]\tvalid_0's binary_logloss: 0.151733\tvalid_0's gini: 0.285539\n",
      "폴드 2 지니계수 : 0.2855389485361659\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154277\tvalid_0's gini: 0.25949\n",
      "[200]\tvalid_0's binary_logloss: 0.153219\tvalid_0's gini: 0.265955\n",
      "[300]\tvalid_0's binary_logloss: 0.152625\tvalid_0's gini: 0.271098\n",
      "[400]\tvalid_0's binary_logloss: 0.152265\tvalid_0's gini: 0.274707\n",
      "[500]\tvalid_0's binary_logloss: 0.152038\tvalid_0's gini: 0.277258\n",
      "[600]\tvalid_0's binary_logloss: 0.151884\tvalid_0's gini: 0.279389\n",
      "[700]\tvalid_0's binary_logloss: 0.151785\tvalid_0's gini: 0.28084\n",
      "[800]\tvalid_0's binary_logloss: 0.151715\tvalid_0's gini: 0.282084\n",
      "[900]\tvalid_0's binary_logloss: 0.151662\tvalid_0's gini: 0.283016\n",
      "[1000]\tvalid_0's binary_logloss: 0.151626\tvalid_0's gini: 0.283779\n",
      "[1100]\tvalid_0's binary_logloss: 0.151604\tvalid_0's gini: 0.284127\n",
      "[1200]\tvalid_0's binary_logloss: 0.15159\tvalid_0's gini: 0.284335\n",
      "[1300]\tvalid_0's binary_logloss: 0.151586\tvalid_0's gini: 0.284398\n",
      "[1400]\tvalid_0's binary_logloss: 0.151583\tvalid_0's gini: 0.284453\n",
      "[1500]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.284597\n",
      "[1600]\tvalid_0's binary_logloss: 0.15158\tvalid_0's gini: 0.284594\n",
      "[1700]\tvalid_0's binary_logloss: 0.151586\tvalid_0's gini: 0.284382\n",
      "[1800]\tvalid_0's binary_logloss: 0.151593\tvalid_0's gini: 0.284256\n",
      "Early stopping, best iteration is:\n",
      "[1549]\tvalid_0's binary_logloss: 0.151575\tvalid_0's gini: 0.284736\n",
      "폴드 3 지니계수 : 0.284736507191425\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154358\tvalid_0's gini: 0.253403\n",
      "[200]\tvalid_0's binary_logloss: 0.153351\tvalid_0's gini: 0.259717\n",
      "[300]\tvalid_0's binary_logloss: 0.152792\tvalid_0's gini: 0.26449\n",
      "[400]\tvalid_0's binary_logloss: 0.152467\tvalid_0's gini: 0.267864\n",
      "[500]\tvalid_0's binary_logloss: 0.152265\tvalid_0's gini: 0.270474\n",
      "[600]\tvalid_0's binary_logloss: 0.152134\tvalid_0's gini: 0.272319\n",
      "[700]\tvalid_0's binary_logloss: 0.152046\tvalid_0's gini: 0.27383\n",
      "[800]\tvalid_0's binary_logloss: 0.151979\tvalid_0's gini: 0.275146\n",
      "[900]\tvalid_0's binary_logloss: 0.15193\tvalid_0's gini: 0.276264\n",
      "[1000]\tvalid_0's binary_logloss: 0.151898\tvalid_0's gini: 0.277013\n",
      "[1100]\tvalid_0's binary_logloss: 0.151869\tvalid_0's gini: 0.27774\n",
      "[1200]\tvalid_0's binary_logloss: 0.151848\tvalid_0's gini: 0.27839\n",
      "[1300]\tvalid_0's binary_logloss: 0.151838\tvalid_0's gini: 0.27873\n",
      "[1400]\tvalid_0's binary_logloss: 0.15183\tvalid_0's gini: 0.278983\n",
      "[1500]\tvalid_0's binary_logloss: 0.15183\tvalid_0's gini: 0.27902\n",
      "[1600]\tvalid_0's binary_logloss: 0.151822\tvalid_0's gini: 0.279336\n",
      "[1700]\tvalid_0's binary_logloss: 0.151821\tvalid_0's gini: 0.279463\n",
      "[1800]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.279638\n",
      "[1900]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.279728\n",
      "[2000]\tvalid_0's binary_logloss: 0.151825\tvalid_0's gini: 0.279602\n",
      "[2100]\tvalid_0's binary_logloss: 0.151821\tvalid_0's gini: 0.279786\n",
      "Early stopping, best iteration is:\n",
      "[1836]\tvalid_0's binary_logloss: 0.151816\tvalid_0's gini: 0.279754\n",
      "폴드 4 지니계수 : 0.2797544531558188\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154425\tvalid_0's gini: 0.26366\n",
      "[200]\tvalid_0's binary_logloss: 0.153432\tvalid_0's gini: 0.269123\n",
      "[300]\tvalid_0's binary_logloss: 0.152887\tvalid_0's gini: 0.273399\n",
      "[400]\tvalid_0's binary_logloss: 0.152552\tvalid_0's gini: 0.277268\n",
      "[500]\tvalid_0's binary_logloss: 0.152334\tvalid_0's gini: 0.280597\n",
      "[600]\tvalid_0's binary_logloss: 0.152179\tvalid_0's gini: 0.283304\n",
      "[700]\tvalid_0's binary_logloss: 0.152065\tvalid_0's gini: 0.285733\n",
      "[800]\tvalid_0's binary_logloss: 0.151972\tvalid_0's gini: 0.287916\n",
      "[900]\tvalid_0's binary_logloss: 0.151907\tvalid_0's gini: 0.289625\n",
      "[1000]\tvalid_0's binary_logloss: 0.151863\tvalid_0's gini: 0.290801\n",
      "[1100]\tvalid_0's binary_logloss: 0.15182\tvalid_0's gini: 0.291919\n",
      "[1200]\tvalid_0's binary_logloss: 0.151788\tvalid_0's gini: 0.292811\n",
      "[1300]\tvalid_0's binary_logloss: 0.151769\tvalid_0's gini: 0.293446\n",
      "[1400]\tvalid_0's binary_logloss: 0.151751\tvalid_0's gini: 0.293827\n",
      "[1500]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.29418\n",
      "[1600]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.294552\n",
      "[1700]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294928\n",
      "[1800]\tvalid_0's binary_logloss: 0.151706\tvalid_0's gini: 0.295087\n",
      "[1900]\tvalid_0's binary_logloss: 0.151708\tvalid_0's gini: 0.295045\n",
      "[2000]\tvalid_0's binary_logloss: 0.151708\tvalid_0's gini: 0.295148\n",
      "[2100]\tvalid_0's binary_logloss: 0.151707\tvalid_0's gini: 0.295204\n",
      "Early stopping, best iteration is:\n",
      "[1877]\tvalid_0's binary_logloss: 0.151704\tvalid_0's gini: 0.295174\n",
      "폴드 5 지니계수 : 0.29517400467321525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담은 1차원 배열 \n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담은 1차원 배열 \n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정 \n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n",
    "    \n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n",
    "    \n",
    "    # LightGBM 모델 훈련 \n",
    "    lgb_model = lgb.train(params=max_params, # 최적 하이퍼 파라미터\n",
    "                          train_set=dtrain, # 훈련 데이터셋\n",
    "                          num_boost_round=2500, # 부스팅 반복 횟수\n",
    "                          valid_sets=dvalid, # 성능 평가용 검증 데이터셋\n",
    "                          feval=gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds=300, # 조기종료 조건\n",
    "                          verbose_eval=100) # 100 번쨰 마다 점수 출력\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 :  0.28863294839417913\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ',eval_gini(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('./output/submission_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Porto Seguro’s Safe Driver Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/24.4M [00:00<?, ?B/s]\n",
      "  0%|          | 8.00k/24.4M [00:00<13:52, 30.7kB/s]\n",
      "  0%|          | 112k/24.4M [00:00<01:06, 383kB/s]  \n",
      "  4%|▎         | 904k/24.4M [00:00<00:08, 2.86MB/s]\n",
      "  8%|▊         | 1.92M/24.4M [00:01<00:12, 1.94MB/s]\n",
      " 17%|█▋        | 4.10M/24.4M [00:01<00:04, 4.82MB/s]\n",
      " 24%|██▍       | 5.84M/24.4M [00:01<00:02, 7.03MB/s]\n",
      " 35%|███▌      | 8.60M/24.4M [00:01<00:01, 11.2MB/s]\n",
      " 42%|████▏     | 10.3M/24.4M [00:02<00:02, 5.50MB/s]\n",
      " 52%|█████▏    | 12.7M/24.4M [00:02<00:01, 7.67MB/s]\n",
      " 58%|█████▊    | 14.2M/24.4M [00:02<00:02, 4.99MB/s]\n",
      " 65%|██████▍   | 15.8M/24.4M [00:03<00:01, 6.23MB/s]\n",
      " 70%|███████   | 17.1M/24.4M [00:03<00:01, 6.18MB/s]\n",
      " 74%|███████▍  | 18.1M/24.4M [00:03<00:01, 6.05MB/s]\n",
      " 78%|███████▊  | 19.1M/24.4M [00:03<00:00, 6.09MB/s]\n",
      " 81%|████████▏ | 19.9M/24.4M [00:03<00:00, 5.93MB/s]\n",
      " 84%|████████▍ | 20.6M/24.4M [00:03<00:00, 6.04MB/s]\n",
      " 87%|████████▋ | 21.3M/24.4M [00:03<00:00, 6.06MB/s]\n",
      " 90%|█████████ | 22.0M/24.4M [00:04<00:00, 5.94MB/s]\n",
      " 93%|█████████▎| 22.6M/24.4M [00:04<00:00, 5.98MB/s]\n",
      " 95%|█████████▌| 23.2M/24.4M [00:04<00:00, 6.06MB/s]\n",
      " 98%|█████████▊| 23.8M/24.4M [00:04<00:00, 6.12MB/s]\n",
      "100%|██████████| 24.4M/24.4M [00:07<00:00, 3.57MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c porto-seguro-safe-driver-prediction -f ./output/submission_2.csv -m submit_in_local"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
