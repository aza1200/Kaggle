{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 라인 모델로 Light BGM 모델을 사용할거임 \n",
    "# 책에서는 파이썬 래퍼 LightGMB 과 사이킷런 LightGBM 중 파이썬 래퍼 LightBGM 사용\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "# 캐글 ↓제출경로\n",
    "data_path = './input/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col='id')\n",
    "test  = pd.read_csv(data_path + 'test.csv' , index_col='id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20832392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처는 보통 웟-핫 인코딩 적용하는게 정석인듯 하다 \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "#인코딩\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  필요없는 피처 제거 \n",
    "# 1. calc 분류 속하는 20개 피처\n",
    "# 2. ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin,\n",
    "# 3. ps_ind_14,\n",
    "# 4. ps_car_14\n",
    "\n",
    "# 제거이유 \n",
    "\n",
    "# 1 => 고윳값 별 타깃값 비율 차이가 없음. 타깃값 비율이 다르더라도 신뢰구간이 넓어 통계적 유효성이 떨어짐\n",
    "# 2 => 신뢰구간이 넓어 통계적 유효성이 떨어짐 \n",
    "# 3 => 타깃값 비율의 신뢰구간이 넓어 통계적 유효성이 떨어짐 \n",
    "# 4 => ps_car_12 와 ps_car_14 두개가 강한 상관관계를 가지는데 이중 하나를 제거하는것이 ㄱㅊ다고 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_01',\n",
       " 'ps_ind_03',\n",
       " 'ps_ind_06_bin',\n",
       " 'ps_ind_07_bin',\n",
       " 'ps_ind_08_bin',\n",
       " 'ps_ind_09_bin',\n",
       " 'ps_ind_15',\n",
       " 'ps_ind_16_bin',\n",
       " 'ps_ind_17_bin',\n",
       " 'ps_ind_18_bin',\n",
       " 'ps_reg_01',\n",
       " 'ps_reg_02',\n",
       " 'ps_reg_03',\n",
       " 'ps_car_11',\n",
       " 'ps_car_12',\n",
       " 'ps_car_13',\n",
       " 'ps_car_15']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추가로 제거할 피처 \n",
    "\n",
    "drop_features = [\n",
    "    'ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14'\n",
    "]\n",
    "\n",
    "# '1) 명목형 피처, 2) calc 분류의 피처 3) 추가 제거할 피처를 제외한 피처 \n",
    "# 제외하고 남김 \n",
    "\n",
    "# 아 명목형 피처는 원핫 인코딩에 사용할 예정이므로 cat 은 제거한다 \n",
    "# 괜히 헷갈렸군.........\n",
    "remaining_features = [\n",
    "    feature for feature in all_features\n",
    "    if ( 'cat' not in feature and\n",
    "        'calc' not in feature and\n",
    "        feature not in drop_features\n",
    "        )\n",
    "]\n",
    "\n",
    "remaining_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "# remaining_features 와 앞서 명목형 피처들 원핫 인코딩한 encoded_cat_matrix 를 합쳐줌 \n",
    "\n",
    "all_data_sprs =sparse.hstack([sparse.csr_matrix(all_data[remaining_features]),\n",
    "    encoded_cat_matrix],\n",
    "    format ='csr'\n",
    ")\n",
    "\n",
    "# csr_matrix() 는 전달받은 데이터는 CSR 형식으로 바꿔줌 \n",
    "# hstack 은 행렬을 수평방향으로 합침 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 나누기\n",
    "\n",
    "num_train = len(train)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 계산 함수 작성 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실젯값과 예측값의 크기가 서로 같은지 확인 (값이 다르면 오류 발생 )\n",
    "    \n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace( 1/ n_samples, 1, n_samples) # 1/n_samples~ 1 까지 n_samples-1 개의 구간으로 나누겠다. \n",
    "    \n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)\n",
    "    \n",
    "    # 2) 예측 완벽할때 지니계수\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    G_true = np.sum(L_mid - L_true)\n",
    "    \n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGMB 용 gini() 함수 \n",
    "\n",
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label() # \n",
    "    return 'gini', eval_gini(labels, preds), True # 반환값 \n",
    " \n",
    "# 반환값 리스트\n",
    "# 평가지표 이름, 평가점수 , 평가 점수가 높을수록 좋은지 여부 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF 방식으로 Light GBM 훈련 \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기\n",
    "# 시계열 데이터가 아니라면 섞어도 되는데 시계열 데이터는 절대로 섞으면 안된다. \n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "params = {\n",
    "    'objective' : 'binary',\n",
    "    'learning_rate' : 0.01,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열 \n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153354\tvalid_0's gini: 0.261651\n",
      "[200]\tvalid_0's binary_logloss: 0.152426\tvalid_0's gini: 0.275704\n",
      "[300]\tvalid_0's binary_logloss: 0.152023\tvalid_0's gini: 0.282277\n",
      "[400]\tvalid_0's binary_logloss: 0.1518\tvalid_0's gini: 0.286648\n",
      "[500]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.287944\n",
      "[600]\tvalid_0's binary_logloss: 0.151672\tvalid_0's gini: 0.288637\n",
      "[700]\tvalid_0's binary_logloss: 0.151659\tvalid_0's gini: 0.288939\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid_0's binary_logloss: 0.151659\tvalid_0's gini: 0.289034\n",
      "폴드 1 지니계수 : 0.2890336154188232 \n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1093\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153498\tvalid_0's gini: 0.249446\n",
      "[200]\tvalid_0's binary_logloss: 0.152708\tvalid_0's gini: 0.260777\n",
      "[300]\tvalid_0's binary_logloss: 0.152397\tvalid_0's gini: 0.267104\n",
      "[400]\tvalid_0's binary_logloss: 0.152234\tvalid_0's gini: 0.271522\n",
      "[500]\tvalid_0's binary_logloss: 0.152164\tvalid_0's gini: 0.273276\n",
      "[600]\tvalid_0's binary_logloss: 0.152136\tvalid_0's gini: 0.27426\n",
      "[700]\tvalid_0's binary_logloss: 0.152125\tvalid_0's gini: 0.274337\n",
      "[800]\tvalid_0's binary_logloss: 0.152116\tvalid_0's gini: 0.274634\n",
      "[900]\tvalid_0's binary_logloss: 0.152106\tvalid_0's gini: 0.274992\n",
      "[1000]\tvalid_0's binary_logloss: 0.152104\tvalid_0's gini: 0.275078\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[983]\tvalid_0's binary_logloss: 0.152101\tvalid_0's gini: 0.275121\n",
      "폴드 2 지니계수 : 0.27512085497432026 \n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153263\tvalid_0's gini: 0.261144\n",
      "[200]\tvalid_0's binary_logloss: 0.15234\tvalid_0's gini: 0.271571\n",
      "[300]\tvalid_0's binary_logloss: 0.151981\tvalid_0's gini: 0.276391\n",
      "[400]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.278667\n",
      "[500]\tvalid_0's binary_logloss: 0.151758\tvalid_0's gini: 0.279899\n",
      "[600]\tvalid_0's binary_logloss: 0.15174\tvalid_0's gini: 0.280373\n",
      "Early stopping, best iteration is:\n",
      "[560]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.280598\n",
      "폴드 3 지니계수 : 0.2805977528883293 \n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1096\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153399\tvalid_0's gini: 0.25056\n",
      "[200]\tvalid_0's binary_logloss: 0.152556\tvalid_0's gini: 0.262835\n",
      "[300]\tvalid_0's binary_logloss: 0.152258\tvalid_0's gini: 0.267038\n",
      "[400]\tvalid_0's binary_logloss: 0.152118\tvalid_0's gini: 0.269715\n",
      "[500]\tvalid_0's binary_logloss: 0.15208\tvalid_0's gini: 0.270536\n",
      "[600]\tvalid_0's binary_logloss: 0.152085\tvalid_0's gini: 0.270629\n",
      "Early stopping, best iteration is:\n",
      "[522]\tvalid_0's binary_logloss: 0.152074\tvalid_0's gini: 0.270749\n",
      "폴드 4 지니계수 : 0.2707490338932929 \n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153483\tvalid_0's gini: 0.262106\n",
      "[200]\tvalid_0's binary_logloss: 0.152646\tvalid_0's gini: 0.273406\n",
      "[300]\tvalid_0's binary_logloss: 0.152291\tvalid_0's gini: 0.279805\n",
      "[400]\tvalid_0's binary_logloss: 0.152093\tvalid_0's gini: 0.284645\n",
      "[500]\tvalid_0's binary_logloss: 0.152004\tvalid_0's gini: 0.28713\n",
      "[600]\tvalid_0's binary_logloss: 0.151982\tvalid_0's gini: 0.287668\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's binary_logloss: 0.15198\tvalid_0's gini: 0.287804\n",
      "폴드 5 지니계수 : 0.2878042213842625 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n",
    "    \n",
    "    # LightGMB 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # LightGMB 전용 검증 데이터셋\n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params,    # 훈련용 하이퍼 파라미터 \n",
    "                          train_set=dtrain, # 훈련 데이터셋\n",
    "                          num_boost_round=1000, # 부스팅 반복 횟수\n",
    "                          valid_sets=dvalid, # 성능 평가용 검증 데이터셋\n",
    "                          feval=gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds=100, # 조기종료 조건\n",
    "                          verbose_eval=100 # 100번쨰마다 점수 출력\n",
    "                          )\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수 \n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score} \\n')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터  0.2804995714877777\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 ', eval_gini(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('./output/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
