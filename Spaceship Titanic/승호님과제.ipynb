{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/train.csv')\n",
    "test_df = pd.read_csv('./input/test.csv')\n",
    "all_df = pd.concat([train_df, test_df]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['group'] = [int(s[:4]) for s in all_df.PassengerId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['last_name'] = [names.split(' ')[1] if isinstance(names, str) else names for names in all_df['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sim = ['HomePlanet','Cabin','Destination','last_name']\n",
    "\n",
    "for col in group_sim:\n",
    "    all_df[col] = all_df.groupby('group')[col].transform(lambda x: x.fillna(method='ffill'))\n",
    "    all_df[col] = all_df[col].transform(lambda x: x.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_col = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
    "\n",
    "for col in cost_col :\n",
    "    all_df[col] = all_df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['spend_fillna_0'] = all_df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['VIP_fill_na_False'] = all_df['VIP'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>CryoSleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>736.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10383.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>5176.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  spend_fillna_0  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0             0.0   \n",
       "1        109.0        9.0          25.0   549.0    44.0           736.0   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0         10383.0   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0          5176.0   \n",
       "4        303.0       70.0         151.0   565.0     2.0          1091.0   \n",
       "\n",
       "  CryoSleep  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','spend_fillna_0','CryoSleep']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, a in enumerate(all_df['CryoSleep']) :\n",
    "    if np.isnan(a) :\n",
    "        if all_df['spend_fillna_0'][idx] == 0:\n",
    "            all_df.loc[idx, 'CryoSleep'] = True\n",
    "        else :\n",
    "            all_df.loc[idx, 'CryoSleep'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = []\n",
    "num = []\n",
    "side = []\n",
    "\n",
    "for i in all_df.Cabin :\n",
    "    if str(i).find('/') >= 0 :\n",
    "        try :\n",
    "            d,n,s = str(i).split('/')\n",
    "            deck.append(d)\n",
    "            num.append(int(n))\n",
    "            side.append(s)\n",
    "        except :\n",
    "            print(i)\n",
    "            break\n",
    "    else: \n",
    "        print(i)\n",
    "        deck.append(np.nan)\n",
    "        num.append(np.nan)\n",
    "        side.append(np.nan)\n",
    "\n",
    "all_df['deck'] = deck\n",
    "all_df['num'] = num\n",
    "all_df['side'] = side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, HomePlanet, CryoSleep, Destination, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, Transported, group, last_name, spend_fillna_0, VIP_fill_na_False, deck, num, side]\n",
       "Index: []"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df[all_df['deck']=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['deck'].fillna(all_df['deck'].mode()[0], inplace=True)\n",
    "all_df['num'].fillna(all_df['num'].mode()[0], inplace=True)\n",
    "all_df['side'].fillna(all_df['side'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종적으로 버릴 변수 PassengerId, VIP, Name, Cabin\n",
    "\n",
    "col_drop = ['PassengerId', 'VIP', 'Name', 'Cabin']\n",
    "all_df.drop(columns=col_drop, inplace=True)\n",
    "all_df['Age'].fillna(np.median(all_df['Age']),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df['Transported']\n",
    "# df = df.drop('Transported', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet \n",
      " ['Europa' 'Earth' 'Mars']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "CryoSleep \n",
      " [False True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "Destination \n",
      " ['TRAPPIST-1e' 'PSO J318.5-22' '55 Cancri e']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "Age \n",
      " [39. 24. 58. 33. 16. 44. 26. 28. 35. 14. 34. 45. 32. 48. 31. 27.  0.  1.\n",
      " 49. 29. 10.  7. 21. 62. 15. 43. 47.  2. 20. 23. 30. 17. 55.  4. 19. 56.\n",
      " nan 25. 38. 36. 22. 18. 42. 37. 13.  8. 40.  3. 54.  9.  6. 64. 67. 61.\n",
      " 50. 41. 57. 11. 52. 51. 46. 60. 63. 59.  5. 79. 68. 74. 12. 53. 65. 71.\n",
      " 75. 70. 76. 78. 73. 66. 69. 72. 77.]\n",
      "유니크한 카테고리 수 :  81\n",
      "null 수              :  179 \n",
      "\n",
      "RoomService \n",
      " [   0.  109.   43. ... 1569. 8586.  745.]\n",
      "유니크한 카테고리 수 :  1273\n",
      "null 수              :  0 \n",
      "\n",
      "FoodCourt \n",
      " [   0.    9. 3576. ... 3208. 6819. 4688.]\n",
      "유니크한 카테고리 수 :  1507\n",
      "null 수              :  0 \n",
      "\n",
      "ShoppingMall \n",
      " [   0.   25.  371. ... 1085.  510. 1872.]\n",
      "유니크한 카테고리 수 :  1115\n",
      "null 수              :  0 \n",
      "\n",
      "Spa \n",
      " [   0.  549. 6715. ... 2868. 1107. 1643.]\n",
      "유니크한 카테고리 수 :  1327\n",
      "null 수              :  0 \n",
      "\n",
      "VRDeck \n",
      " [   0.   44.   49. ... 1164.  971. 3235.]\n",
      "유니크한 카테고리 수 :  1306\n",
      "null 수              :  0 \n",
      "\n",
      "Transported \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "group \n",
      " [   1    2    3 ... 9278 9279 9280]\n",
      "유니크한 카테고리 수 :  6217\n",
      "null 수              :  0 \n",
      "\n",
      "last_name \n",
      " ['Ofracculy' 'Vines' 'Susent' ... 'Fort' 'Workmanson' 'Sionerorly']\n",
      "유니크한 카테고리 수 :  2217\n",
      "null 수              :  0 \n",
      "\n",
      "spend_fillna_0 \n",
      " [    0.   736. 10383. ...  8803.  8536.  4637.]\n",
      "유니크한 카테고리 수 :  2336\n",
      "null 수              :  0 \n",
      "\n",
      "VIP_fill_na_False \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "deck \n",
      " ['B' 'F' 'A' 'G' 'E' 'D' 'C' 'T']\n",
      "유니크한 카테고리 수 :  8\n",
      "null 수              :  0 \n",
      "\n",
      "num \n",
      " [   0    1    2 ... 1892 1893 1894]\n",
      "유니크한 카테고리 수 :  1817\n",
      "null 수              :  0 \n",
      "\n",
      "side \n",
      " ['P' 'S']\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns :\n",
    "    print(col,\"\\n\", df[col].unique())\n",
    "    print('유니크한 카테고리 수 : ',len(df[col].unique()))\n",
    "    print('null 수              : ', df[col].isna().sum(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet \n",
      " ['Earth' 'Europa' 'Mars']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "CryoSleep \n",
      " [True False]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "Destination \n",
      " ['TRAPPIST-1e' '55 Cancri e' 'PSO J318.5-22']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "Age \n",
      " [27. 19. 31. 38. 20. 21. 23. 24. 45. 44. 46. 29. 40. 30. 14. 66. 36. 18.\n",
      " 26. 48.  6. 33. nan  1. 17. 34.  5. 22. 16.  7.  2.  0. 56. 39. 35.  8.\n",
      "  3.  9. 25. 42. 50. 41. 32. 49. 55. 60. 47. 58. 28. 12. 52. 15. 43. 54.\n",
      " 70. 37. 59. 61. 63. 53.  4. 10. 65. 13. 51. 79. 74. 67. 69. 57. 72. 75.\n",
      " 11. 73. 71. 64. 62. 78. 77. 68.]\n",
      "유니크한 카테고리 수 :  80\n",
      "null 수              :  91 \n",
      "\n",
      "RoomService \n",
      " [0.0000e+00 1.0000e+01 3.3900e+02 9.3200e+02 2.0000e+00 2.6000e+01\n",
      " 3.9000e+01 1.7840e+03 2.9300e+02 5.9800e+02 1.4000e+01 4.8300e+02\n",
      " 6.6000e+01 1.0000e+00 1.0370e+03 3.9660e+03 4.6000e+01 4.1300e+02\n",
      " 3.7800e+02 3.2000e+01 8.3200e+02 3.6200e+02 7.5600e+02 1.9700e+02\n",
      " 6.0000e+00 2.6100e+02 1.6690e+03 4.4000e+01 3.3000e+01 7.1700e+02\n",
      " 4.7620e+03 7.4100e+02 3.0000e+00 7.8600e+02 2.5560e+03 1.8600e+02\n",
      " 1.3400e+03 6.9600e+02 1.5420e+03 1.0800e+02 3.1860e+03 4.1000e+01\n",
      " 1.0540e+03 6.0000e+01 3.0100e+02 1.2300e+02 9.8000e+01 2.1200e+02\n",
      " 7.3000e+01 3.7600e+02 5.0000e+00 1.0310e+03 3.5600e+02 6.3800e+02\n",
      " 1.3950e+03 2.3000e+01 8.0000e+00 1.3000e+02 2.7300e+02 2.8800e+02\n",
      " 7.3100e+02 1.6780e+03 4.9080e+03 8.7000e+01 4.0000e+00 7.0800e+02\n",
      " 6.4200e+02 6.5800e+02 2.3900e+02 1.8900e+02 1.2900e+02 1.2800e+02\n",
      " 1.0300e+03 5.3000e+01 2.4600e+02 6.4800e+02 1.7290e+03 1.5000e+01\n",
      " 4.1050e+03 4.4100e+02 7.6000e+02 3.4240e+03 4.9220e+03 1.0630e+03\n",
      " 2.2210e+03 2.8600e+02 2.5430e+03 1.6440e+03 7.5500e+02 4.9900e+02\n",
      " 3.0000e+01 7.2000e+01 7.4800e+02 1.0450e+03 6.3200e+02 5.9600e+02\n",
      " 7.1000e+02 1.2790e+03 1.3000e+01 9.0000e+00 1.3440e+03 4.6600e+02\n",
      " 1.6610e+03 1.7000e+01 5.5900e+02 7.2000e+02 2.6600e+02 5.3000e+02\n",
      " 1.7050e+03 4.7200e+02 1.7800e+02 1.4900e+02 3.6820e+03 8.3000e+02\n",
      " 2.2270e+03 1.8200e+02 2.5770e+03 6.9800e+02 8.6900e+02 2.0240e+03\n",
      " 3.1600e+03 7.8000e+02 2.9900e+02 1.3990e+03 6.1000e+01 5.8900e+02\n",
      " 1.2460e+03 1.7600e+02 1.7930e+03 4.1100e+02 2.2000e+01 9.0100e+02\n",
      " 5.9000e+02 1.4100e+02 6.4000e+02 6.6600e+02 2.1640e+03 8.2600e+02\n",
      " 2.4000e+02 5.5300e+02 5.4000e+01 2.8900e+02 1.5000e+02 6.9900e+02\n",
      " 1.5400e+02 2.2330e+03 1.2160e+03 1.1990e+03 1.3200e+02 8.4700e+02\n",
      " 1.4770e+03 4.3000e+01 5.5500e+02 1.3710e+03 7.7500e+02 2.2000e+02\n",
      " 5.2700e+02 1.8000e+01 6.7800e+02 1.7100e+03 1.7890e+03 7.7000e+02\n",
      " 1.4590e+03 4.8000e+01 5.3900e+02 5.0100e+02 1.3630e+03 6.7300e+02\n",
      " 9.8700e+02 1.5300e+02 1.2400e+02 9.7000e+01 9.9000e+01 2.2460e+03\n",
      " 1.0060e+03 2.2010e+03 3.4000e+01 1.2420e+03 8.9000e+01 8.0700e+02\n",
      " 9.3600e+02 6.3000e+01 1.0180e+03 9.0000e+01 1.3300e+02 2.0120e+03\n",
      " 4.5000e+01 5.8000e+01 3.0000e+02 4.7300e+02 2.5650e+03 2.0000e+01\n",
      " 1.0420e+03 1.8300e+02 7.9900e+02 6.7900e+02 5.1500e+02 5.8300e+02\n",
      " 3.1600e+02 1.6000e+01 6.2800e+02 6.7400e+02 1.5560e+03 3.3030e+03\n",
      " 1.6920e+03 1.1060e+03 8.6000e+01 1.0930e+03 2.2400e+02 6.3300e+02\n",
      " 1.7200e+02 2.0100e+02 1.5070e+03 1.0220e+03 1.1000e+02 9.8000e+02\n",
      " 3.6700e+02 6.8400e+02 2.4700e+02 6.9000e+02 5.3200e+02 4.7700e+02\n",
      " 1.2960e+03 1.1500e+02 1.5700e+02 1.0100e+02 2.3400e+02 3.7900e+02\n",
      " 1.2820e+03 1.4020e+03 6.4300e+02 6.2300e+02 1.1820e+03 6.0300e+02\n",
      " 5.8200e+02 1.1560e+03 5.5700e+02 1.2860e+03 9.0800e+02 1.9000e+01\n",
      " 3.5290e+03 3.1100e+02 8.0900e+02 1.6510e+03 4.0700e+02 5.9900e+02\n",
      " 1.2850e+03 2.1000e+01 2.7000e+01 1.6240e+03 1.8440e+03 1.4300e+02\n",
      " 1.0200e+02 8.5000e+01 4.0000e+01 2.6200e+02 8.7800e+02 6.6400e+02\n",
      " 2.0800e+02 6.3700e+02 9.0400e+02 3.0800e+02 1.2260e+03 7.6700e+02\n",
      " 2.5800e+02 4.2500e+02 5.1000e+01 3.9900e+02 3.4400e+02 2.1000e+02\n",
      " 1.7750e+03 1.5200e+02 1.1000e+01 6.9300e+02 1.4230e+03 1.6700e+02\n",
      " 1.4200e+02 1.2000e+01 7.6000e+01 5.6700e+02 1.2590e+03 2.0400e+02\n",
      " 8.3900e+02 3.4000e+02 8.7700e+02 8.1700e+02 8.5300e+02 1.8720e+03\n",
      " 9.9500e+02 1.0300e+02 9.4000e+02 6.5100e+02 1.1050e+03 1.4480e+03\n",
      " 8.8400e+02 3.4300e+02 1.6900e+02 8.7400e+02 8.5800e+02 5.7400e+02\n",
      " 6.2000e+01 6.1100e+02 4.7000e+02 7.8500e+02 2.5000e+01 5.4900e+02\n",
      " 2.1380e+03 6.6700e+02 2.7800e+02 4.6000e+02 5.7200e+02 6.1700e+02\n",
      " 1.9380e+03 6.5000e+01 5.9500e+02 1.2700e+02 5.9000e+01 3.8100e+02\n",
      " 2.8300e+02 8.2400e+02 8.1000e+01 2.4690e+03 6.4400e+02 9.3800e+02\n",
      " 1.1567e+04 4.1900e+02 7.3300e+02 1.2360e+03 1.0000e+02 2.7300e+03\n",
      " 8.5500e+02 6.7100e+02 1.4160e+03 2.1940e+03 7.9800e+02 4.5400e+02\n",
      " 5.4500e+02 9.1500e+02 8.4200e+02 2.0300e+02 1.4400e+02 1.2080e+03\n",
      " 2.3660e+03 9.1000e+01 2.4360e+03 8.9000e+02 2.5210e+03 7.4300e+02\n",
      " 1.7220e+03 6.4600e+02 3.6800e+02 7.5000e+01 5.4600e+02 7.8300e+02\n",
      " 1.2140e+03 1.2280e+03 7.6800e+02 1.7400e+02 1.7260e+03 5.6000e+01\n",
      " 8.8500e+02 1.0700e+02 1.8580e+03 9.5100e+02 3.3370e+03 2.4000e+01\n",
      " 2.9800e+02 1.2600e+02 8.5400e+02 9.6600e+02 1.7680e+03 1.9300e+02\n",
      " 1.6800e+02 3.2000e+02 2.5450e+03 1.4540e+03 1.6200e+02 6.4380e+03\n",
      " 2.6470e+03 1.3380e+03 1.6410e+03 6.4000e+01 1.1660e+03 1.6390e+03\n",
      " 1.5450e+03 4.9300e+02 1.6930e+03 1.1100e+02 3.8590e+03 1.2600e+03\n",
      " 3.3000e+02 4.4600e+02 1.1270e+03 1.4810e+03 1.6960e+03 1.9540e+03\n",
      " 4.7000e+01 9.3000e+02 2.0900e+02 1.2530e+03 3.7000e+01 3.3700e+02\n",
      " 1.3600e+02 1.0230e+03 2.8000e+01 6.7000e+01 5.5000e+01 4.2000e+01\n",
      " 4.9600e+02 1.1940e+03 8.2000e+01 9.7700e+02 3.6350e+03 9.1400e+02\n",
      " 7.9300e+02 1.0430e+03 2.7000e+02 2.4400e+02 1.8530e+03 1.1140e+03\n",
      " 5.0400e+02 1.6040e+03 7.0000e+00 1.0140e+03 8.8300e+02 1.8400e+02\n",
      " 4.3300e+02 8.4000e+01 1.1100e+03 1.7570e+03 2.4390e+03 2.7510e+03\n",
      " 2.8100e+02 5.2900e+02 3.8800e+02 3.5500e+02 8.0100e+02 5.9000e+03\n",
      " 4.3800e+02 4.0690e+03 4.4300e+02 1.4790e+03 6.2000e+02 2.4660e+03\n",
      " 1.3800e+03 3.2100e+02 1.3400e+02 5.7800e+02 5.8620e+03 1.0390e+03\n",
      " 7.1500e+02 6.1400e+02 6.1000e+02 2.9200e+02 2.6800e+02 1.8000e+02\n",
      " 2.9000e+01 1.2470e+03 6.3400e+02 3.1000e+02 1.0740e+03 3.2400e+02\n",
      " 1.0410e+03 3.5100e+02 1.3700e+02 1.2510e+03 1.4750e+03 1.0860e+03\n",
      " 1.5600e+02 1.7640e+03 6.8800e+02 4.9800e+02 4.3000e+02 5.2500e+02\n",
      " 5.1700e+02 7.3600e+02 1.1280e+03 8.4600e+02 9.7500e+02 6.2700e+02\n",
      " 6.9000e+01 5.6600e+02 3.4900e+02 4.3200e+02 1.0900e+02 1.3200e+03\n",
      " 1.4000e+02 3.9500e+02 8.0800e+02 9.5300e+02 5.1800e+02 1.2200e+02\n",
      " 5.7000e+01 1.2940e+03 1.3940e+03 1.0400e+02 9.8100e+02 5.3700e+02\n",
      " 1.2180e+03 3.6750e+03 2.1950e+03 5.3330e+03 4.1600e+02 8.2700e+02\n",
      " 7.0400e+02 1.7340e+03 1.0710e+03 3.9000e+02 7.9400e+02 5.6800e+02\n",
      " 1.6500e+02 7.1000e+01 1.3650e+03 2.0700e+02 3.3410e+03 7.0000e+01\n",
      " 1.5380e+03 8.0000e+01 1.2990e+03 1.5960e+03 3.4500e+02 1.1170e+03\n",
      " 1.1040e+03 7.8000e+01 2.1300e+02 4.6500e+02 1.3370e+03 6.6300e+02\n",
      " 3.8000e+01 1.6270e+03 1.1710e+03 2.0190e+03 1.1080e+03 5.4200e+02\n",
      " 1.3300e+03 1.4490e+03 4.5800e+02 7.4200e+02 1.3470e+03 2.3100e+02\n",
      " 1.7100e+02 4.1950e+03 6.8000e+01 1.6730e+03 1.4310e+03 8.9300e+02\n",
      " 9.0500e+02 1.0820e+03 4.7810e+03 6.4100e+02 1.1810e+03 2.3350e+03\n",
      " 7.6600e+02 7.4400e+02 9.4000e+01 3.7200e+02 4.4400e+02 4.9000e+01\n",
      " 1.3010e+03 1.4000e+03 4.7800e+02 8.6700e+02 1.6010e+03 5.8800e+02\n",
      " 1.6600e+02 2.2100e+02 2.2870e+03 1.6300e+02 1.0040e+03 1.2370e+03\n",
      " 2.9600e+02 2.0830e+03 3.9400e+02 7.3000e+02 2.1090e+03 7.9000e+01\n",
      " 1.2210e+03 8.3800e+02 7.8700e+02 9.2100e+02 1.1300e+02 1.8640e+03\n",
      " 5.8400e+02 1.1630e+03 3.5130e+03 1.7000e+02 6.8600e+02 4.6700e+02\n",
      " 1.7230e+03 6.0500e+02 3.6900e+02 8.8900e+02 6.7600e+02 1.6640e+03\n",
      " 6.4500e+02 2.7800e+03 9.3400e+02 2.4900e+02 7.4070e+03 1.1020e+03\n",
      " 1.1390e+03 1.5170e+03 3.8200e+02 2.9100e+02 3.1000e+01 1.0600e+02\n",
      " 7.7000e+01 3.6000e+01 1.0490e+03 1.3430e+03 7.1100e+02 8.7000e+02\n",
      " 5.0000e+02 5.6000e+02 9.1100e+02 1.4620e+03 3.5040e+03 1.0090e+03\n",
      " 2.0710e+03 1.2430e+03 2.5700e+02 1.8390e+03 1.2490e+03 7.1800e+02\n",
      " 2.2800e+02 2.6420e+03 1.4550e+03 1.1900e+02 7.0500e+02 1.4630e+03\n",
      " 1.0960e+03 9.4600e+02 1.1780e+03 9.2800e+02 2.5200e+02 7.0100e+02\n",
      " 1.1620e+03 2.4550e+03 8.8100e+02 8.6400e+02 1.2100e+02 1.8500e+02\n",
      " 4.8400e+02 1.7300e+02 1.5480e+03 4.8700e+02 1.4780e+03 5.0700e+02\n",
      " 1.1770e+03 2.3790e+03 4.5100e+02 9.4900e+02 1.1550e+03 1.2730e+03\n",
      " 4.5500e+02 8.8000e+01 9.8800e+02 5.1200e+02 3.6500e+02 6.9500e+02\n",
      " 6.3100e+02 2.6760e+03 8.0400e+02 4.2070e+03 3.3300e+02 5.1000e+03\n",
      " 4.4900e+02 7.0300e+02 2.5960e+03 1.2650e+03 2.5000e+02 2.5380e+03\n",
      " 7.5700e+02 1.7520e+03 1.8940e+03 8.8800e+02 2.4380e+03 1.7180e+03\n",
      " 1.3100e+02 5.0000e+01 4.0900e+02 8.4400e+02 8.7900e+02 1.2980e+03\n",
      " 1.2190e+03 4.1000e+02 1.3500e+03 5.0900e+02 1.1400e+02 1.4150e+03\n",
      " 1.3500e+02 1.2300e+03 9.7600e+02 7.5800e+02 2.4500e+02 9.3300e+02\n",
      " 2.2900e+02 5.1400e+02 9.6000e+01 1.0950e+03 1.0610e+03 6.1200e+02\n",
      " 3.5000e+02 1.1890e+03 1.1970e+03 8.4800e+02 3.3400e+02 8.3700e+02\n",
      " 3.6600e+02 7.9100e+02 1.5510e+03 1.0870e+03 1.2890e+03 1.9000e+02\n",
      " 6.0600e+02 3.3100e+02 4.8200e+02 1.4870e+03 1.1180e+03 1.1410e+03\n",
      " 1.3820e+03 2.0440e+03 1.6600e+03 1.1700e+02 1.6260e+03 2.7580e+03\n",
      " 4.1400e+02 1.9500e+02 8.2100e+02 4.8900e+02 1.1730e+03 2.9400e+02\n",
      " 5.0200e+02 1.3140e+03 6.9400e+02 1.8570e+03 3.6570e+03 1.9050e+03\n",
      " 1.6100e+02 6.7200e+02 9.1800e+02 1.3240e+03 2.2500e+02 7.9600e+02\n",
      " 8.8600e+02 4.1500e+02 1.0500e+03 2.3330e+03 7.7700e+02 2.8000e+02\n",
      " 6.5900e+02 2.1100e+02 6.8100e+02 1.0440e+03 2.4340e+03 3.8600e+02\n",
      " 1.9490e+03 2.4590e+03 1.0250e+03 9.0600e+02 1.2540e+03 9.9800e+02\n",
      " 2.3640e+03 8.4500e+02 3.4710e+03 4.7100e+02 7.4000e+01 9.5600e+02\n",
      " 5.2000e+01 1.0010e+03 2.2300e+02 6.0100e+02 1.5240e+03 3.9560e+03\n",
      " 3.5000e+01 2.1460e+03 2.5720e+03 2.2490e+03 1.4850e+03 8.2800e+02\n",
      " 8.8000e+02 3.5900e+02 2.2760e+03 2.5850e+03 4.2100e+02 2.5600e+02\n",
      " 4.7600e+02 1.2560e+03 2.3600e+02 3.3200e+02 1.0080e+03 4.5300e+02\n",
      " 2.7890e+03 6.0400e+02 1.5390e+03 2.5900e+02 3.5950e+03 8.1600e+02\n",
      " 1.9530e+03 9.4500e+02 2.0540e+03 1.6940e+03 2.7900e+02 2.3040e+03\n",
      " 4.9100e+02 5.4100e+02 1.2330e+03 1.4500e+02 1.5810e+03 1.1500e+03\n",
      " 9.9300e+02 1.6460e+03 2.6900e+02 1.1760e+03 3.2200e+02 6.3900e+02\n",
      " 7.7300e+02 5.4540e+03 2.3510e+03 6.6900e+02 2.4160e+03 2.8550e+03\n",
      " 6.8500e+02 3.8700e+02 1.0160e+03 9.3000e+01 7.8400e+02 1.5760e+03\n",
      " 3.2300e+02 1.7700e+02 1.2780e+03 3.8300e+02 9.9400e+02 1.7900e+02\n",
      " 1.0330e+03 5.4700e+02 2.8350e+03 5.2300e+02 6.5200e+02 8.8200e+02\n",
      " 1.2720e+03 1.4700e+02]\n",
      "유니크한 카테고리 수 :  842\n",
      "null 수              :  0 \n",
      "\n",
      "FoodCourt \n",
      " [0.0000e+00 9.0000e+00 6.6520e+03 1.6150e+03 6.3900e+02 3.0000e+00\n",
      " 7.4000e+01 1.5610e+03 1.2000e+01 7.7080e+03 1.9250e+03 8.7800e+02\n",
      " 6.9770e+03 6.4520e+03 8.0000e+00 3.4950e+03 6.1900e+02 8.7000e+02\n",
      " 2.0000e+01 1.7360e+03 1.7000e+01 2.2000e+01 8.6900e+02 1.0000e+00\n",
      " 2.8720e+03 3.1500e+02 5.1400e+02 4.0200e+02 8.8530e+03 4.4000e+01\n",
      " 2.6000e+01 4.6300e+02 1.1000e+01 7.7200e+02 1.5000e+01 1.1600e+02\n",
      " 2.7500e+02 2.0000e+00 5.1600e+02 3.8330e+03 2.5273e+04 1.4420e+03\n",
      " 3.3400e+02 8.6000e+01 1.0000e+01 6.7800e+02 6.2700e+02 1.1549e+04\n",
      " 3.4000e+01 7.5500e+02 1.2520e+03 6.9000e+01 1.0300e+03 1.4680e+03\n",
      " 6.7000e+01 1.7040e+03 7.2200e+02 5.3800e+02 2.5100e+02 1.1920e+03\n",
      " 1.8300e+02 7.5000e+01 2.1160e+03 6.0000e+00 1.3470e+03 2.5220e+03\n",
      " 1.4700e+02 1.3780e+03 3.4500e+02 2.0810e+03 6.4420e+03 7.5200e+02\n",
      " 5.4700e+02 1.3300e+02 9.3520e+03 8.0400e+02 3.7900e+02 1.0320e+03\n",
      " 4.9200e+02 2.4000e+01 4.2800e+02 4.3000e+02 4.6000e+01 1.5280e+03\n",
      " 1.0480e+03 5.7180e+03 7.7000e+01 4.0000e+00 9.0000e+01 8.7500e+02\n",
      " 1.4150e+03 6.5000e+01 1.5020e+03 4.3700e+02 2.3520e+03 4.3000e+01\n",
      " 7.2500e+02 4.8000e+02 9.8000e+01 8.8500e+02 1.6300e+02 1.5300e+02\n",
      " 1.0290e+03 2.4900e+03 4.7330e+03 1.4000e+01 3.4100e+03 1.2880e+03\n",
      " 1.8570e+03 3.3260e+03 1.6410e+03 3.7200e+02 3.6360e+03 3.2400e+02\n",
      " 2.6730e+03 1.4800e+02 6.9900e+02 1.7100e+02 2.9350e+03 4.9580e+03\n",
      " 5.4000e+01 1.1240e+03 1.2770e+03 8.1790e+03 6.9150e+03 4.0500e+02\n",
      " 6.9500e+02 1.5400e+02 6.0000e+01 1.5500e+02 7.1000e+01 1.7500e+02\n",
      " 3.8200e+02 3.6300e+02 9.0800e+02 5.0000e+00 1.6100e+02 2.9830e+03\n",
      " 4.5800e+02 1.2860e+03 1.1100e+02 4.7000e+01 9.2000e+01 7.8400e+02\n",
      " 4.9000e+01 1.8800e+02 1.9200e+02 7.6000e+03 7.0800e+02 2.7000e+01\n",
      " 5.7500e+02 2.8300e+02 2.0360e+03 3.1400e+02 2.2500e+02 1.6000e+01\n",
      " 1.4470e+03 3.6060e+03 1.0390e+03 7.3120e+03 1.5370e+03 1.0900e+02\n",
      " 6.5230e+03 1.7200e+02 2.2350e+03 1.5600e+02 5.5000e+01 4.0990e+03\n",
      " 3.5330e+03 2.6500e+03 4.9800e+02 5.8300e+02 1.0100e+02 6.1000e+02\n",
      " 2.9700e+02 2.8700e+02 2.5900e+02 6.8600e+03 5.3000e+01 3.2800e+02\n",
      " 1.8820e+03 6.5300e+02 8.0600e+02 1.3900e+02 3.8000e+01 3.0600e+02\n",
      " 4.7400e+02 9.7700e+02 4.1920e+03 4.6100e+02 1.6800e+02 5.5600e+02\n",
      " 6.8100e+02 9.0000e+02 1.8220e+03 1.4500e+02 2.7990e+03 8.3200e+02\n",
      " 1.5890e+03 5.5000e+02 1.0500e+02 2.6000e+02 1.7400e+02 5.7100e+02\n",
      " 3.6000e+01 6.2800e+02 3.5000e+01 3.6910e+03 1.5980e+03 3.0000e+01\n",
      " 1.7000e+02 3.6370e+03 4.0000e+01 2.1200e+02 6.3000e+01 3.5770e+03\n",
      " 4.2200e+02 5.1130e+03 2.2090e+03 1.0711e+04 6.9100e+02 8.6400e+02\n",
      " 6.1580e+03 1.2760e+03 1.1050e+03 2.9020e+03 2.1600e+02 3.0850e+03\n",
      " 6.2850e+03 6.7300e+02 1.1410e+03 6.7100e+02 8.1200e+02 6.1300e+02\n",
      " 5.9200e+02 1.3700e+02 8.7000e+01 2.2000e+02 1.9800e+03 4.6700e+02\n",
      " 3.1900e+02 2.0700e+02 1.0800e+03 1.6960e+03 4.2050e+03 8.9100e+02\n",
      " 1.0600e+02 6.0700e+02 2.5880e+03 1.9100e+02 1.6560e+03 5.2000e+01\n",
      " 1.4270e+03 2.1300e+02 9.0100e+02 1.1700e+02 8.2400e+02 1.2600e+02\n",
      " 2.0520e+03 3.0500e+02 1.2170e+03 7.8000e+01 6.8800e+02 4.1100e+02\n",
      " 3.6700e+02 1.9400e+02 4.8290e+03 2.8520e+03 5.9500e+03 5.9310e+03\n",
      " 1.9740e+03 5.7000e+01 9.3900e+02 5.7220e+03 1.3110e+03 5.3000e+02\n",
      " 6.1800e+02 4.6180e+03 4.4500e+02 7.7000e+02 3.9900e+02 3.1000e+01\n",
      " 5.2200e+02 6.7400e+02 5.4500e+02 2.5760e+03 4.6500e+02 5.5500e+02\n",
      " 5.6720e+03 1.8000e+01 1.6060e+03 2.3330e+03 3.2000e+01 2.1000e+02\n",
      " 2.5000e+01 2.3220e+03 1.4600e+02 1.4200e+02 1.8900e+02 5.6190e+03\n",
      " 3.5700e+02 2.1000e+01 3.3600e+02 3.5610e+03 1.3000e+01 1.7300e+02\n",
      " 2.6340e+03 7.9900e+02 1.3070e+03 1.3800e+02 1.8200e+02 2.4770e+03\n",
      " 3.0300e+03 4.7500e+02 1.1984e+04 1.6120e+03 6.2900e+02 5.9000e+01\n",
      " 1.2220e+03 9.5600e+02 1.0100e+03 1.1850e+03 6.3600e+02 5.2400e+02\n",
      " 2.4600e+02 1.3400e+02 1.9600e+02 7.7400e+02 3.7300e+02 3.8210e+03\n",
      " 2.6300e+02 3.9700e+02 1.0754e+04 2.0740e+03 2.4500e+02 1.2960e+03\n",
      " 1.0340e+03 2.9500e+02 7.6500e+02 1.3000e+02 1.0200e+02 4.4200e+02\n",
      " 1.0400e+02 9.6620e+03 6.9700e+02 9.7240e+03 4.3130e+03 1.3520e+03\n",
      " 2.9000e+02 5.6200e+02 7.2000e+01 3.6730e+03 9.0600e+02 2.0200e+02\n",
      " 8.2200e+02 7.0000e+00 8.8000e+01 3.8000e+02 6.3000e+02 9.3400e+02\n",
      " 6.8000e+02 3.9500e+02 2.0370e+03 8.4400e+02 2.0610e+03 4.7380e+03\n",
      " 2.2030e+03 3.5100e+02 4.7600e+02 5.7300e+02 9.9800e+02 2.9950e+03\n",
      " 4.8800e+02 3.3000e+01 4.8270e+03 7.6800e+02 8.0700e+02 9.3000e+01\n",
      " 5.9400e+02 9.0200e+02 4.7010e+03 8.4000e+01 2.6100e+02 5.6600e+02\n",
      " 3.1500e+03 4.3980e+03 1.1293e+04 9.8100e+02 8.8000e+02 8.7400e+02\n",
      " 6.2900e+03 1.5200e+02 3.5400e+02 4.2000e+01 2.4550e+03 8.4000e+02\n",
      " 4.8500e+02 7.9800e+03 2.6960e+03 8.4200e+02 1.6240e+03 2.0809e+04\n",
      " 6.5640e+03 8.1800e+02 7.5080e+03 5.8800e+02 9.1300e+02 4.2000e+02\n",
      " 4.2300e+03 5.2900e+02 2.9000e+01 1.9770e+03 1.0010e+03 5.2600e+02\n",
      " 3.7480e+03 9.3800e+02 7.4000e+02 8.1000e+02 5.9900e+02 3.2850e+03\n",
      " 1.1143e+04 3.3760e+03 5.3100e+02 2.3840e+03 5.0300e+02 2.4040e+03\n",
      " 1.4000e+02 3.3540e+03 8.5600e+02 4.1400e+02 2.2270e+03 4.2230e+03\n",
      " 2.5600e+02 7.7480e+03 3.0900e+02 1.8790e+03 7.5950e+03 2.8020e+03\n",
      " 3.0000e+02 5.1380e+03 1.4120e+03 9.5960e+03 7.6600e+02 5.1720e+03\n",
      " 8.5300e+02 5.6800e+02 1.4760e+03 1.4400e+03 3.3060e+03 1.0590e+03\n",
      " 2.9400e+02 5.3200e+02 6.7330e+03 7.3400e+02 2.4000e+02 6.4800e+02\n",
      " 2.1500e+02 6.6690e+03 6.1200e+02 1.5360e+03 2.5830e+03 1.2200e+02\n",
      " 1.2490e+03 2.2300e+02 1.9000e+02 1.2500e+02 7.2710e+03 2.4000e+03\n",
      " 2.2340e+03 7.7800e+02 2.2600e+02 8.1700e+02 7.9600e+02 2.3000e+02\n",
      " 2.8800e+02 6.3200e+02 3.2270e+03 7.0000e+03 2.0229e+04 5.9030e+03\n",
      " 6.9010e+03 8.3000e+02 2.2880e+03 1.6954e+04 4.7340e+03 3.7500e+02\n",
      " 7.4900e+02 1.6400e+02 7.0100e+02 6.7700e+02 8.7600e+02 6.3150e+03\n",
      " 8.9600e+02 9.0580e+03 4.3600e+02 1.0530e+03 7.1300e+02 2.3700e+02\n",
      " 1.0163e+04 4.6600e+02 6.2200e+02 9.1590e+03 7.1200e+02 7.3700e+02\n",
      " 7.6200e+02 1.2550e+03 4.3900e+02 8.2000e+01 7.1700e+02 2.1100e+02\n",
      " 8.0200e+02 7.5960e+03 1.0840e+03 7.7500e+02 4.1000e+01 1.4860e+03\n",
      " 1.6071e+04 2.0650e+03 7.0260e+03 6.0000e+02 9.2600e+02 3.2190e+03\n",
      " 3.5750e+03 1.2600e+03 6.6700e+02 2.8170e+03 3.2160e+03 1.1963e+04\n",
      " 1.6280e+03 1.7700e+02 1.2700e+02 2.3600e+02 2.5150e+03 1.0200e+03\n",
      " 2.5060e+03 6.6020e+03 2.6990e+03 8.3500e+02 2.0500e+02 9.2400e+02\n",
      " 1.3760e+03 9.1900e+02 4.4300e+02 2.3450e+03 5.7900e+02 1.0170e+03\n",
      " 6.6070e+03 1.3290e+03 2.3130e+03 2.7300e+02 2.3870e+03 4.6400e+02\n",
      " 6.6600e+02 7.8900e+02 9.9330e+03 5.1500e+02 8.1000e+01 1.1500e+02\n",
      " 9.7000e+01 4.3800e+02 5.2090e+03 1.2620e+03 3.9200e+02 1.4780e+03\n",
      " 1.4600e+03 1.6860e+03 6.8000e+01 8.3000e+01 9.1000e+01 2.9880e+03\n",
      " 5.8240e+03 5.0800e+02 8.2000e+02 1.6963e+04 2.9140e+03 8.3700e+02\n",
      " 1.8000e+02 5.3700e+02 2.1750e+03 2.9100e+02 7.4920e+03 4.5000e+01\n",
      " 4.5700e+02 2.3860e+03 7.2900e+02 2.7200e+02 1.7900e+02 1.2130e+03\n",
      " 3.7000e+02 3.1600e+02 1.9720e+03 3.1100e+02 1.3610e+03 4.1320e+03\n",
      " 3.2090e+03 1.0460e+03 2.3010e+03 2.8950e+03 7.9700e+02 1.6340e+03\n",
      " 5.0000e+01 1.7970e+03 4.8530e+03 7.9000e+01 2.5700e+03 4.7100e+02\n",
      " 2.0160e+03 1.0060e+03 3.7400e+02 2.8820e+03 7.6000e+01 6.4200e+02\n",
      " 4.0480e+03 1.4530e+03 1.2200e+03 3.2470e+03 9.4100e+02 9.0040e+03\n",
      " 1.6660e+03 2.8200e+02 2.7030e+03 1.1660e+03 5.9500e+02 8.0500e+02\n",
      " 1.8500e+02 2.1800e+02 2.3100e+02 3.5430e+03 4.9000e+02 3.3800e+02\n",
      " 1.9540e+03 5.1200e+02 3.2200e+02 1.4100e+02 5.2100e+02 1.4250e+03\n",
      " 5.3600e+02 1.1630e+03 4.9500e+02 6.9620e+03 1.8700e+02 4.1570e+03\n",
      " 1.5390e+03 1.1530e+03 6.7900e+02 1.2350e+04 1.7540e+03 8.4500e+02\n",
      " 2.3300e+02 1.4300e+02 1.0900e+03 2.0760e+03 4.2400e+02 5.8000e+02\n",
      " 2.4400e+02 3.2320e+03 1.9000e+01 7.1600e+02 2.8540e+03 6.0200e+02\n",
      " 6.8700e+02 2.8900e+02 1.4390e+03 9.7880e+03 2.9300e+02 7.2700e+02\n",
      " 2.8750e+03 3.5000e+02 8.5000e+01 1.4000e+03 9.3220e+03 7.8600e+02\n",
      " 7.3880e+03 9.5400e+03 7.0200e+02 1.5000e+02 9.5200e+02 1.6250e+04\n",
      " 1.3540e+03 9.6000e+01 6.1600e+02 1.9730e+03 5.0900e+02 3.7890e+03\n",
      " 2.3000e+01 5.6490e+03 7.4500e+02 3.6900e+02 2.8500e+02 1.8260e+03\n",
      " 4.8000e+01 5.4400e+02 2.2900e+02 3.8660e+03 3.2000e+02 4.2830e+03\n",
      " 1.6190e+03 6.4900e+02 7.3000e+01 4.2900e+02 1.6010e+03 2.0620e+03\n",
      " 6.2300e+02 2.2700e+02 6.5620e+03 2.3250e+03 9.9600e+02 6.3300e+02\n",
      " 1.5920e+03 6.8900e+02 3.1460e+03 6.7000e+02 1.5010e+03 2.7000e+02\n",
      " 2.7900e+02 6.2000e+02 2.8470e+03 8.5800e+02 7.1900e+02 4.0000e+02\n",
      " 1.1900e+03 3.3900e+02 3.7700e+03 6.2500e+02 1.0050e+03 1.8300e+03\n",
      " 9.3360e+03 5.2830e+03 2.1050e+03 5.1960e+03 2.5240e+03 3.1420e+03\n",
      " 2.9250e+03 1.2400e+03 1.4730e+03 4.6000e+02 1.8600e+02 4.0300e+02\n",
      " 7.4200e+02 3.8700e+02 2.3670e+03 2.7680e+03 1.2010e+03 9.4600e+02\n",
      " 1.4550e+03 2.4140e+03 2.4800e+02 4.9100e+02 7.2600e+02 2.8000e+01\n",
      " 7.3300e+02 2.7830e+03 3.1310e+03 4.5310e+03 1.9590e+03 9.1100e+02\n",
      " 6.4500e+02 8.0000e+01 1.0830e+03 2.2800e+02 3.8400e+02 2.1100e+03\n",
      " 2.0300e+02 1.1000e+02 3.4850e+03 7.0900e+02 3.3610e+03 7.4400e+02\n",
      " 4.4020e+03 1.3790e+03 1.0302e+04 3.4530e+03 1.4590e+03 2.9600e+02\n",
      " 1.1110e+03 1.3810e+03 1.3620e+03 1.0000e+03 1.5800e+02 2.5720e+03\n",
      " 4.5830e+03 9.9100e+02 3.9400e+02 3.7000e+01 2.5700e+02 7.4300e+02\n",
      " 2.9900e+02 2.1410e+03 3.4700e+02 5.6300e+02 1.2970e+03 2.0310e+03\n",
      " 2.8200e+03 9.4500e+02 6.6000e+01 5.6000e+01 1.1070e+03 6.8300e+02\n",
      " 4.5500e+02 3.3300e+02 2.5000e+02 1.8070e+03 2.6810e+03 3.4620e+03\n",
      " 7.8100e+02 2.3110e+03 6.3100e+02 3.0260e+03 4.7900e+02 1.1020e+03\n",
      " 8.0100e+02 4.0560e+03 4.9130e+03 2.0700e+03 4.5300e+02 1.3100e+02\n",
      " 8.1570e+03 4.5400e+02 1.8390e+03 2.7160e+03 3.8600e+02 4.0930e+03\n",
      " 3.5500e+02 3.7260e+03 1.6330e+03 5.3300e+03 4.4100e+02 7.5300e+02\n",
      " 1.0910e+03 2.0320e+03 2.4050e+03 2.3397e+04 6.4300e+02 2.2790e+03\n",
      " 4.9330e+03 7.8000e+02 5.5800e+02 1.1800e+03 1.2100e+03 8.8400e+02\n",
      " 1.1020e+04 3.1560e+03 2.1890e+03 7.3600e+02 1.6830e+03 2.6400e+02\n",
      " 5.3900e+02 1.6900e+02 4.1600e+02 2.3770e+03 8.2100e+02 7.5100e+02\n",
      " 4.2480e+03 2.2390e+03 1.2570e+03 5.6420e+03 2.4190e+03 7.1000e+02\n",
      " 2.0750e+03 4.8300e+02 1.3080e+03 1.9450e+03 3.9800e+02 7.2500e+03\n",
      " 2.1610e+03 9.0500e+02 2.4620e+03 2.1760e+03 2.1150e+03 3.9730e+03\n",
      " 1.7080e+03 3.4100e+02 5.4600e+02 5.9800e+02 7.6210e+03 2.1820e+03\n",
      " 1.0651e+04 6.3800e+02 5.7780e+03 1.0700e+02 7.1500e+02 1.5170e+03\n",
      " 2.9180e+03 6.8820e+03 2.9200e+02 1.0580e+03 1.2110e+03 3.8060e+03\n",
      " 6.5800e+02 1.9980e+03 7.9800e+02 2.0450e+03 4.7200e+02 5.9800e+03\n",
      " 4.0380e+03 8.7300e+02 4.8130e+03 6.0300e+02 6.0100e+02 8.6500e+02\n",
      " 8.4700e+02 2.6800e+03]\n",
      "유니크한 카테고리 수 :  902\n",
      "null 수              :  0 \n",
      "\n",
      "ShoppingMall \n",
      " [0.000e+00 6.350e+02 2.630e+02 1.360e+02 2.530e+02 2.430e+02 8.000e+00\n",
      " 2.600e+01 1.800e+01 5.273e+03 1.200e+01 3.000e+01 8.700e+01 1.350e+02\n",
      " 1.235e+03 1.585e+03 9.970e+02 3.900e+01 8.200e+01 2.000e+00 5.710e+02\n",
      " 2.305e+03 1.000e+00 6.108e+03 2.300e+01 5.210e+02 1.480e+02 7.050e+02\n",
      " 1.175e+03 1.404e+03 2.060e+02 2.420e+02 5.650e+02 4.000e+00 8.098e+03\n",
      " 2.200e+01 7.800e+01 7.280e+02 2.839e+03 4.620e+02 1.380e+02 4.100e+01\n",
      " 7.022e+03 7.900e+01 1.299e+03 4.220e+02 1.320e+02 8.100e+01 1.026e+03\n",
      " 2.880e+02 7.000e+00 7.550e+02 2.590e+02 6.100e+02 4.391e+03 7.370e+02\n",
      " 1.247e+03 3.000e+00 9.320e+02 5.170e+02 6.700e+01 2.219e+03 1.000e+01\n",
      " 7.590e+02 4.870e+02 6.252e+03 4.410e+02 2.000e+01 1.970e+02 8.580e+02\n",
      " 2.736e+03 5.410e+02 6.870e+02 1.022e+03 3.710e+02 6.300e+01 1.310e+02\n",
      " 6.720e+02 2.427e+03 9.900e+01 1.023e+03 5.500e+01 6.730e+02 1.580e+02\n",
      " 2.840e+02 1.281e+03 1.435e+03 1.740e+02 1.100e+01 1.469e+03 1.057e+03\n",
      " 1.456e+03 2.220e+02 1.110e+03 8.810e+02 6.910e+02 1.608e+03 2.640e+02\n",
      " 5.400e+01 1.790e+03 4.600e+01 2.210e+02 1.020e+03 5.830e+02 1.604e+03\n",
      " 3.500e+02 6.590e+02 3.300e+01 1.817e+03 5.000e+00 7.480e+02 1.487e+03\n",
      " 8.920e+02 1.195e+03 1.750e+02 2.780e+02 4.700e+01 3.500e+01 3.770e+02\n",
      " 8.000e+01 5.290e+02 2.690e+02 2.990e+02 7.620e+02 6.930e+02 5.900e+01\n",
      " 2.350e+02 3.767e+03 2.851e+03 6.440e+02 3.000e+02 5.120e+02 1.162e+03\n",
      " 3.100e+01 2.800e+01 6.970e+02 1.140e+02 4.970e+02 2.572e+03 7.830e+02\n",
      " 5.200e+01 3.440e+02 5.100e+01 1.510e+02 1.719e+03 6.300e+02 9.780e+02\n",
      " 4.230e+02 6.560e+02 5.400e+02 3.800e+01 1.161e+03 3.460e+02 3.200e+01\n",
      " 8.300e+01 2.940e+02 1.500e+02 3.520e+02 2.329e+03 1.810e+02 1.400e+01\n",
      " 5.390e+02 1.863e+03 1.600e+01 4.800e+01 1.094e+03 8.190e+02 2.250e+02\n",
      " 1.547e+03 9.270e+02 2.190e+02 5.970e+02 3.250e+02 1.690e+02 6.080e+02\n",
      " 9.950e+02 2.600e+02 2.936e+03 7.770e+02 8.292e+03 7.500e+01 6.500e+01\n",
      " 2.438e+03 4.300e+02 9.200e+01 3.320e+02 8.420e+02 3.720e+02 8.880e+02\n",
      " 2.620e+03 4.500e+01 3.630e+02 8.300e+02 1.166e+03 1.274e+03 2.130e+02\n",
      " 7.410e+02 8.710e+02 9.260e+02 2.400e+01 9.800e+01 1.090e+03 7.740e+02\n",
      " 6.290e+02 1.650e+02 3.600e+01 1.277e+03 2.160e+02 3.010e+02 2.521e+03\n",
      " 4.000e+01 7.400e+01 9.940e+02 1.566e+03 7.040e+02 5.850e+02 3.780e+02\n",
      " 5.840e+02 1.121e+03 8.017e+03 4.060e+02 2.768e+03 2.100e+01 4.760e+02\n",
      " 7.380e+02 9.000e+00 6.050e+02 1.217e+03 1.300e+01 5.600e+01 2.020e+02\n",
      " 2.700e+01 7.470e+02 7.090e+02 2.345e+03 5.740e+02 7.960e+02 2.980e+02\n",
      " 4.530e+02 6.061e+03 2.970e+02 3.380e+02 4.200e+01 2.000e+02 6.890e+02\n",
      " 8.450e+02 2.620e+02 3.760e+02 1.126e+03 4.661e+03 2.338e+03 5.300e+01\n",
      " 6.660e+02 1.290e+02 3.170e+02 1.400e+02 9.060e+02 2.570e+02 6.340e+02\n",
      " 1.120e+02 7.240e+02 7.150e+02 4.300e+01 2.950e+02 6.920e+02 4.510e+02\n",
      " 2.900e+01 2.080e+02 5.730e+02 7.270e+02 6.860e+02 5.320e+02 4.940e+02\n",
      " 9.690e+02 1.270e+02 1.160e+02 8.740e+02 1.990e+02 2.070e+02 1.522e+03\n",
      " 4.380e+02 8.560e+02 8.590e+02 7.060e+02 7.700e+02 3.950e+02 2.960e+02\n",
      " 1.059e+03 8.480e+02 7.400e+02 1.900e+01 3.870e+02 3.110e+02 4.780e+02\n",
      " 1.610e+03 8.500e+02 1.104e+03 5.649e+03 1.230e+02 2.267e+03 6.990e+02\n",
      " 2.670e+02 7.020e+02 1.352e+03 3.280e+02 1.819e+03 1.950e+02 2.760e+03\n",
      " 8.490e+02 1.330e+02 6.880e+02 3.765e+03 1.184e+03 1.103e+03 6.000e+01\n",
      " 1.388e+03 8.610e+02 9.100e+01 6.000e+00 4.310e+02 2.638e+03 8.500e+01\n",
      " 2.380e+02 1.631e+03 2.271e+03 1.202e+03 1.730e+02 3.030e+02 8.890e+02\n",
      " 3.270e+02 1.240e+02 7.000e+01 1.500e+01 7.700e+01 8.160e+02 3.450e+02\n",
      " 1.110e+02 8.800e+01 5.490e+02 1.923e+03 1.880e+02 9.580e+02 1.780e+02\n",
      " 1.600e+02 1.342e+03 9.820e+02 1.030e+02 3.070e+02 1.680e+02 3.820e+02\n",
      " 2.740e+02 9.770e+02 3.160e+02 2.710e+02 1.001e+03 8.400e+01 1.008e+03\n",
      " 4.600e+02 1.560e+02 6.760e+02 3.700e+01 1.170e+02 1.490e+02 1.425e+03\n",
      " 7.120e+02 7.230e+02 6.850e+02 1.300e+02 1.420e+02 1.698e+03 1.320e+03\n",
      " 4.410e+03 3.560e+02 6.400e+01 1.630e+03 2.451e+03 2.183e+03 5.080e+02\n",
      " 9.360e+02 7.140e+02 9.300e+01 3.790e+02 2.718e+03 2.751e+03 1.072e+03\n",
      " 2.450e+02 2.230e+02 2.820e+02 8.060e+02 1.188e+03 6.900e+01 5.000e+01\n",
      " 7.600e+01 1.930e+02 9.490e+02 4.850e+02 9.170e+02 8.550e+02 2.660e+02\n",
      " 4.150e+02 6.260e+02 3.540e+02 1.700e+01 6.310e+02 8.690e+02 1.275e+03\n",
      " 6.630e+02 1.583e+03 2.330e+02 3.129e+03 3.200e+02 7.720e+02 5.690e+02\n",
      " 2.050e+02 2.081e+03 5.040e+02 8.870e+02 8.790e+02 2.490e+02 6.540e+02\n",
      " 8.251e+03 5.450e+02 2.089e+03 2.320e+02 1.746e+03 1.710e+02 1.340e+02\n",
      " 7.100e+01 6.110e+02 4.610e+02 1.353e+03 1.050e+02 9.700e+01 9.500e+01\n",
      " 6.240e+02 4.390e+02 2.262e+03 3.610e+02 1.177e+03 1.530e+02 7.870e+02\n",
      " 7.690e+02 6.190e+02 1.000e+02 5.420e+02 7.930e+02 9.480e+02 1.460e+02\n",
      " 6.060e+02 2.110e+02 5.790e+02 1.613e+03 4.450e+02 1.168e+03 4.130e+02\n",
      " 4.420e+02 1.444e+03 9.960e+02 9.130e+02 1.365e+03 8.180e+02 7.800e+02\n",
      " 7.680e+02 3.091e+03 4.210e+02 3.930e+02 1.214e+03 4.460e+03 1.062e+03\n",
      " 1.090e+02 1.741e+03 3.650e+02 5.960e+02 5.800e+01 6.420e+02 4.090e+02\n",
      " 6.100e+01 1.048e+03 2.121e+03 1.296e+03 6.200e+01 5.580e+02 2.920e+02\n",
      " 3.510e+02 1.025e+03 1.010e+03 1.430e+02 1.630e+02 1.317e+03 1.014e+03\n",
      " 4.550e+02 6.023e+03 1.423e+03 2.837e+03 7.160e+02 1.030e+03 7.030e+02\n",
      " 1.222e+03 9.450e+02 8.380e+02 1.041e+03 5.940e+02 6.830e+02 7.170e+02\n",
      " 2.488e+03 2.500e+01 1.599e+03 1.565e+03 7.250e+02 1.210e+02 1.618e+03\n",
      " 6.140e+02 3.509e+03 1.150e+02 3.180e+02 1.450e+02 1.987e+03 9.840e+02\n",
      " 1.789e+03 2.760e+02 1.079e+03 1.080e+02 3.970e+02 1.143e+03 2.170e+02\n",
      " 4.020e+02 1.220e+03 2.127e+03 6.320e+02 7.200e+01 6.510e+02 8.430e+02\n",
      " 1.156e+03 7.000e+02 6.130e+02 9.460e+02 4.570e+02 2.280e+02 8.440e+02\n",
      " 1.540e+02 3.074e+03 9.930e+02 8.070e+02 6.550e+02 1.351e+03 1.410e+02\n",
      " 2.646e+03 9.830e+02 4.700e+02 5.190e+02 2.730e+02 4.860e+02 1.066e+03\n",
      " 2.104e+03 9.280e+02 5.950e+02 8.640e+02 9.680e+02 3.400e+01 8.310e+02\n",
      " 1.442e+03 5.250e+02 1.900e+02 1.896e+03 1.211e+03 6.200e+02 6.170e+02\n",
      " 9.600e+01 8.020e+02 4.950e+02 4.471e+03 7.300e+01 8.660e+02 5.280e+02\n",
      " 3.830e+02 6.460e+02 7.610e+02 4.290e+02 2.332e+03 6.470e+02 1.117e+03\n",
      " 7.260e+02 8.780e+02 2.140e+02 2.461e+03 1.709e+03 2.153e+03 5.100e+02\n",
      " 1.040e+02 1.005e+03 2.580e+02 4.080e+02 5.330e+02 2.396e+03 1.153e+03\n",
      " 7.350e+02 4.680e+02 1.078e+03 1.440e+02 3.050e+02 4.900e+02 9.040e+02\n",
      " 5.090e+02 5.140e+02 5.700e+01 7.320e+02 1.390e+02 1.820e+02 8.980e+02\n",
      " 1.063e+03 3.290e+02 1.570e+02 6.600e+01 8.680e+02 1.610e+02 6.960e+02\n",
      " 2.270e+02 6.020e+02 4.200e+02 1.367e+03 1.825e+03 1.250e+02 7.990e+02\n",
      " 2.790e+02 1.720e+02 2.560e+02 2.216e+03 1.512e+03 6.070e+02 1.015e+03\n",
      " 2.561e+03 3.120e+02 1.070e+02 7.570e+02 7.750e+02 7.010e+02 2.090e+02\n",
      " 9.760e+02 5.720e+02 1.349e+03 6.520e+02 2.228e+03 1.393e+03 3.230e+02\n",
      " 1.245e+03 2.871e+03 1.171e+03 6.820e+02 9.400e+02 9.540e+02 1.950e+03\n",
      " 9.520e+02 1.446e+03 7.210e+02 1.660e+02 1.968e+03 1.187e+03 2.890e+02\n",
      " 9.430e+02 9.400e+01 9.750e+02 8.120e+02 2.619e+03 1.370e+02 5.470e+02\n",
      " 4.790e+02 2.035e+03 3.260e+02 7.110e+02 2.237e+03 4.100e+02 9.710e+02\n",
      " 8.390e+02 1.260e+03 3.880e+02 3.340e+02 2.510e+02 9.700e+02 5.600e+02\n",
      " 6.940e+02 1.328e+03 9.650e+02 7.290e+02 4.170e+02 3.550e+02 1.773e+03\n",
      " 4.740e+02 3.700e+02 8.400e+02 1.322e+03 4.400e+02 6.210e+02 2.257e+03\n",
      " 1.319e+03 6.150e+02 1.464e+03 2.680e+02 2.142e+03 1.693e+03 1.307e+03\n",
      " 9.530e+02 3.429e+03 9.660e+02 1.039e+03 1.520e+02 1.636e+03 1.130e+02\n",
      " 7.980e+02 7.130e+02 6.620e+02 1.311e+03 2.156e+03 5.430e+02 1.258e+03\n",
      " 3.851e+03]\n",
      "유니크한 카테고리 수 :  715\n",
      "null 수              :  0 \n",
      "\n",
      "Spa \n",
      " [0.0000e+00 2.8230e+03 1.8100e+02 1.1300e+02 2.3700e+02 7.0000e+00\n",
      " 1.4000e+01 1.6870e+03 9.0300e+02 4.5700e+02 5.6900e+02 3.1440e+03\n",
      " 3.0000e+00 1.6090e+03 4.0000e+00 1.4090e+03 1.8000e+01 1.1800e+02\n",
      " 2.1400e+02 2.6500e+02 8.6600e+02 5.9900e+02 1.0000e+00 3.6000e+01\n",
      " 2.9000e+01 7.2300e+02 1.2130e+03 1.5020e+03 2.1000e+01 7.5600e+02\n",
      " 4.6500e+02 4.5720e+03 1.5500e+02 1.2767e+04 2.0000e+00 1.5310e+03\n",
      " 1.3000e+01 1.8130e+03 7.5100e+02 8.5000e+01 6.0200e+02 8.5170e+03\n",
      " 4.6000e+01 1.5600e+03 6.2000e+01 9.1900e+02 1.9360e+03 3.6800e+02\n",
      " 6.1300e+02 1.0790e+03 4.4000e+01 9.7800e+02 4.9660e+03 1.0000e+01\n",
      " 4.2000e+01 3.0800e+02 4.8820e+03 3.4900e+03 4.9300e+02 7.0400e+02\n",
      " 3.9300e+02 5.0000e+01 6.0000e+00 7.8300e+02 3.3200e+02 7.3600e+02\n",
      " 8.8200e+02 1.0280e+03 6.2800e+02 7.6680e+03 1.5000e+01 3.8000e+01\n",
      " 1.6300e+02 4.8600e+02 3.1500e+02 2.3800e+02 1.6000e+01 1.1210e+03\n",
      " 4.4200e+02 3.9500e+02 2.4880e+03 4.4700e+02 1.1540e+03 1.0200e+02\n",
      " 8.0000e+00 6.1000e+01 1.0243e+04 5.3200e+02 3.3300e+02 5.0500e+02\n",
      " 1.7000e+02 7.5500e+02 5.0000e+00 8.7400e+02 6.6000e+01 3.3600e+02\n",
      " 4.0000e+01 5.3900e+02 1.6900e+02 7.8700e+02 1.1000e+01 4.3400e+02\n",
      " 1.1970e+03 1.9350e+03 1.4120e+03 4.7800e+02 3.7170e+03 1.3420e+03\n",
      " 3.8600e+02 6.2100e+02 1.6630e+03 7.4020e+03 8.3000e+01 1.6780e+03\n",
      " 2.3000e+01 3.8580e+03 2.3930e+03 2.2000e+01 2.2170e+03 2.0000e+01\n",
      " 2.4100e+02 2.6730e+03 3.6500e+02 3.2430e+03 3.7500e+02 7.3200e+02\n",
      " 7.9800e+02 1.6000e+02 1.1600e+02 8.0800e+02 3.7000e+02 6.9620e+03\n",
      " 5.6000e+01 6.2820e+03 2.5600e+02 5.2700e+02 4.5000e+01 9.0000e+00\n",
      " 1.5000e+02 6.0000e+01 1.8800e+02 4.1000e+01 7.7000e+01 5.6200e+02\n",
      " 3.6130e+03 4.8400e+02 1.6240e+03 4.5840e+03 1.1200e+02 1.5140e+03\n",
      " 9.1000e+01 4.6300e+02 4.7000e+01 4.4900e+02 1.4200e+02 1.2000e+01\n",
      " 2.2100e+02 7.5700e+02 1.1490e+03 1.8240e+03 4.2400e+02 9.4800e+02\n",
      " 1.6120e+03 9.1000e+02 6.0100e+02 1.3100e+02 6.3600e+02 2.0000e+02\n",
      " 1.7000e+01 1.4460e+03 1.7070e+03 4.3000e+02 1.2600e+02 3.3700e+02\n",
      " 1.9080e+03 9.0100e+02 2.5100e+02 1.9000e+01 1.9200e+02 2.0100e+02\n",
      " 1.5100e+02 3.5000e+01 3.1830e+03 7.5360e+03 4.2200e+02 4.8500e+02\n",
      " 4.9000e+02 7.7100e+02 5.6800e+02 7.6000e+01 7.4980e+03 5.2200e+02\n",
      " 1.6160e+03 7.5800e+02 6.9900e+02 9.2000e+02 6.7100e+02 4.5200e+02\n",
      " 5.3500e+02 1.8900e+02 6.7200e+02 8.9990e+03 1.2200e+02 2.9800e+02\n",
      " 6.8000e+01 8.8600e+02 1.6320e+03 1.0380e+03 2.8180e+03 4.0200e+02\n",
      " 8.6000e+01 4.7900e+02 4.9760e+03 9.1300e+02 2.1200e+02 1.3010e+03\n",
      " 2.7100e+02 3.9400e+02 7.0700e+02 7.2000e+01 1.2200e+03 8.2200e+02\n",
      " 6.0600e+02 4.7640e+03 3.1700e+02 3.3800e+02 6.1000e+02 2.1800e+02\n",
      " 9.3000e+01 1.2000e+02 3.5100e+02 1.3840e+03 3.5500e+02 1.6700e+02\n",
      " 2.4000e+02 3.7480e+03 7.4100e+02 1.6410e+03 2.4500e+02 1.4500e+02\n",
      " 8.2700e+02 1.8500e+02 6.3200e+02 4.1600e+03 9.7400e+02 6.3800e+02\n",
      " 6.0000e+02 1.3110e+03 4.3900e+02 1.2600e+03 5.3000e+01 3.9100e+02\n",
      " 3.8900e+02 3.2700e+02 9.9400e+02 1.7570e+03 2.8300e+02 6.7000e+01\n",
      " 4.2570e+03 1.1000e+02 5.9000e+02 3.9000e+01 6.3000e+01 1.4700e+02\n",
      " 8.4000e+01 5.9300e+02 2.0700e+02 1.3700e+02 1.2400e+02 9.5100e+02\n",
      " 2.0600e+03 2.3200e+02 6.6300e+02 9.8000e+02 9.2900e+02 2.7000e+01\n",
      " 1.0600e+02 5.2900e+02 1.2730e+03 8.9700e+02 4.0600e+02 5.3600e+02\n",
      " 1.1230e+03 6.8800e+02 5.9500e+02 3.7000e+01 7.8800e+02 3.7800e+02\n",
      " 2.7600e+02 1.1880e+03 1.5520e+03 3.7100e+02 3.0000e+01 9.1700e+02\n",
      " 1.6100e+02 2.4000e+01 2.4600e+02 2.6000e+02 2.5200e+02 5.2000e+01\n",
      " 5.6000e+02 8.6700e+02 1.5850e+03 3.5880e+03 4.4100e+02 5.7000e+01\n",
      " 7.0900e+02 5.8310e+03 3.3000e+01 1.2100e+02 1.3300e+02 1.9900e+02\n",
      " 5.9570e+03 3.1100e+02 6.3100e+02 6.3000e+02 3.4000e+01 4.1710e+03\n",
      " 4.0000e+02 8.5800e+03 1.7590e+03 3.4500e+02 9.8500e+02 1.6850e+03\n",
      " 9.6300e+02 8.0200e+02 5.1500e+02 1.7000e+03 1.9400e+03 1.1180e+03\n",
      " 4.9000e+01 1.1870e+03 2.1840e+03 2.3400e+02 6.6500e+02 6.5000e+01\n",
      " 2.4270e+03 4.4300e+02 4.5100e+02 1.2960e+03 2.9280e+03 9.5000e+01\n",
      " 4.4500e+02 2.0510e+03 1.8300e+02 8.9500e+02 1.7710e+03 3.8290e+03\n",
      " 1.5733e+04 1.5255e+04 5.5340e+03 7.5000e+01 1.7420e+03 3.4200e+02\n",
      " 4.0100e+02 3.8100e+02 6.5500e+02 5.9200e+02 1.6560e+03 2.4050e+03\n",
      " 2.5000e+02 1.0900e+03 2.9700e+02 5.1000e+01 1.7300e+02 5.1560e+03\n",
      " 2.0800e+02 7.9600e+02 7.8400e+02 5.2520e+03 3.7520e+03 3.6400e+02\n",
      " 7.7500e+02 1.2842e+04 1.9230e+03 2.8820e+03 2.0610e+03 3.7450e+03\n",
      " 3.5600e+03 1.0900e+02 7.1200e+02 9.6000e+01 1.7880e+03 6.3860e+03\n",
      " 2.8000e+01 1.7500e+02 7.1600e+02 3.7620e+03 6.3070e+03 5.0600e+02\n",
      " 9.6600e+02 4.4670e+03 2.1700e+02 4.7200e+02 8.8400e+02 6.8400e+02\n",
      " 2.2870e+03 9.2200e+02 4.1900e+02 7.3400e+02 1.0232e+04 4.0700e+02\n",
      " 6.6700e+02 3.7690e+03 2.5000e+01 2.0500e+02 9.4000e+01 3.7440e+03\n",
      " 7.4400e+02 3.0200e+02 3.0900e+02 2.2000e+02 5.4900e+02 1.3170e+03\n",
      " 1.0450e+03 2.1400e+03 2.6750e+03 8.7000e+01 6.8600e+02 1.9660e+03\n",
      " 7.2000e+02 5.9000e+01 7.7000e+02 4.3000e+01 5.8400e+02 5.4000e+01\n",
      " 7.1800e+02 6.2890e+03 1.4300e+02 7.9000e+01 8.5100e+02 7.6200e+02\n",
      " 7.5000e+02 5.4100e+02 9.0000e+01 5.3700e+02 9.9100e+02 1.6750e+03\n",
      " 4.1460e+03 2.8600e+02 2.1000e+02 3.5600e+02 8.1000e+01 7.4500e+02\n",
      " 2.6000e+01 1.7700e+02 5.1900e+02 1.7800e+02 5.0300e+02 1.5230e+03\n",
      " 2.3600e+02 6.4400e+02 1.4240e+03 6.2510e+03 1.3960e+03 7.8100e+02\n",
      " 2.2840e+03 3.7200e+02 4.6950e+03 7.0680e+03 1.2437e+04 2.3470e+03\n",
      " 8.0400e+02 9.2600e+02 4.4600e+02 1.6030e+03 1.8470e+03 4.6100e+02\n",
      " 3.4600e+02 9.5700e+02 9.4300e+02 6.1900e+02 1.3800e+02 3.0600e+02\n",
      " 1.0610e+03 3.0500e+02 7.9400e+02 1.1850e+03 2.6700e+02 1.3120e+03\n",
      " 5.9280e+03 5.3100e+02 6.5200e+02 1.3900e+03 1.0300e+02 1.2800e+02\n",
      " 4.6180e+03 2.8480e+03 1.5080e+03 3.9900e+02 2.2600e+02 5.5200e+02\n",
      " 8.9100e+02 2.7400e+02 4.1500e+02 3.4390e+03 1.0670e+03 3.1000e+01\n",
      " 9.8600e+02 1.4950e+03 1.6500e+02 8.9000e+01 3.4100e+02 5.4820e+03\n",
      " 4.0720e+03 5.2300e+02 1.0350e+03 2.5250e+03 8.0960e+03 1.2790e+03\n",
      " 1.5900e+02 8.2500e+02 2.7800e+02 8.1900e+02 1.7840e+03 7.0600e+02\n",
      " 2.3000e+02 1.9400e+02 2.5970e+03 1.8660e+03 1.3510e+03 2.6100e+02\n",
      " 4.3300e+02 2.8800e+02 1.9300e+02 4.3600e+02 9.8000e+01 1.2380e+03\n",
      " 5.2290e+03 6.8900e+02 1.3400e+02 4.2900e+02 4.2000e+02 9.9000e+01\n",
      " 6.8100e+02 6.1620e+03 1.0800e+03 4.9600e+02 6.5400e+02 8.8900e+02\n",
      " 7.4600e+02 2.0260e+03 1.1483e+04 5.7000e+02 1.3580e+03 3.5900e+02\n",
      " 1.3200e+03 2.1820e+03 1.1160e+03 1.9610e+03 6.2980e+03 3.8170e+03\n",
      " 4.1700e+02 6.9200e+02 8.8000e+01 3.0810e+03 3.6050e+03 2.0950e+03\n",
      " 3.4000e+02 2.4900e+03 1.8220e+03 1.2840e+03 6.6000e+02 1.1790e+03\n",
      " 7.9100e+02 2.3100e+02 2.6400e+02 1.5100e+03 1.0290e+03 6.9000e+02\n",
      " 6.9600e+02 1.7290e+03 6.2900e+02 6.9700e+02 3.2400e+02 6.1600e+02\n",
      " 4.7000e+02 1.5300e+02 2.5670e+03 7.0500e+02 1.1700e+02 4.9200e+02\n",
      " 1.1080e+03 3.8500e+02 2.6900e+03 6.6800e+02 3.0480e+03 2.1460e+03\n",
      " 2.8400e+02 1.9844e+04 5.8000e+02 8.7100e+02 5.5000e+01 6.2500e+02\n",
      " 6.3900e+02 1.5200e+02 2.8100e+02 2.6240e+03 1.7990e+03 1.0400e+02\n",
      " 5.3870e+03 4.0180e+03 7.9500e+02 8.9800e+02 2.5540e+03 2.2300e+02\n",
      " 4.8000e+01 1.4600e+03 6.9000e+01 8.8300e+02 4.9800e+02 2.7580e+03\n",
      " 1.0630e+03 1.1920e+03 1.3983e+04 6.9400e+02 7.3900e+02 7.6900e+02\n",
      " 2.5930e+03 6.5900e+02 1.4820e+03 4.0500e+02 2.7900e+02 1.3940e+03\n",
      " 1.0650e+03 8.4400e+02 2.5800e+02 6.8500e+02 2.9410e+03 4.8300e+02\n",
      " 1.6440e+03 2.0770e+03 1.2690e+04 1.4400e+02 7.1400e+02 5.2600e+02\n",
      " 7.4000e+02 7.2200e+02 7.6100e+02 2.4420e+03 3.7700e+02 1.0100e+02\n",
      " 6.6200e+02 4.6700e+02 3.8280e+03 1.1780e+03 3.2800e+02 2.4200e+03\n",
      " 3.2000e+02 1.2700e+02 3.6900e+02 1.6470e+03 5.5400e+02 5.5500e+02\n",
      " 9.3800e+02 1.2940e+03 1.5800e+02 6.4200e+02 1.1400e+02 1.4600e+02\n",
      " 6.9800e+02 4.9500e+02 4.2100e+02 3.4300e+03 5.8650e+03 9.6400e+02\n",
      " 1.0800e+02 6.7400e+02 1.7380e+03 3.7300e+02 8.3800e+02 4.9900e+02\n",
      " 1.4440e+03 6.0500e+02 2.0160e+03 6.4720e+03 1.9060e+03 5.0800e+02\n",
      " 5.1250e+03 4.5400e+02 6.6030e+03 3.4930e+03 3.4730e+03 1.6230e+03\n",
      " 5.5300e+02 6.8700e+02 9.4500e+02 6.4000e+01 9.8910e+03 1.4790e+03\n",
      " 4.1200e+02 1.4710e+03 7.6900e+03 9.7900e+02 2.9200e+02 5.8600e+02\n",
      " 3.4800e+02 1.1000e+03 2.0300e+02 1.1900e+02 2.2060e+03 7.4900e+02\n",
      " 3.8200e+02 1.4470e+03 1.0190e+03 5.6600e+02 6.0300e+02 1.8200e+02\n",
      " 1.7130e+03 5.9800e+02 3.2100e+02 1.3440e+03 8.9110e+03 5.7200e+02\n",
      " 7.0000e+01 2.5400e+02 6.4500e+02 9.7000e+01 3.6700e+02 1.4390e+03\n",
      " 1.2270e+03 9.0900e+02 5.7370e+03 7.8000e+02 7.7700e+02 6.5300e+02\n",
      " 1.2630e+03 3.5700e+02 2.4120e+03 1.3200e+02 9.0200e+02 1.6140e+03\n",
      " 7.5400e+02 1.0470e+03 3.0430e+03 2.9300e+02 4.4410e+03 8.1100e+02\n",
      " 7.6800e+02 1.5200e+03 1.8820e+03 1.0700e+02 5.8000e+01 1.6200e+02\n",
      " 2.9900e+02 1.5340e+03 2.6300e+02 2.3500e+02 3.5300e+02 6.1700e+02\n",
      " 6.5800e+02 1.2480e+03 4.7300e+02 9.2000e+01 7.3000e+01 1.8560e+03\n",
      " 2.5360e+03 2.1190e+03 7.0100e+02 1.0880e+03 1.1520e+03 4.7240e+03\n",
      " 3.6580e+03 1.6810e+03 1.7400e+02 7.2500e+02 1.5190e+03 2.5900e+02\n",
      " 7.3000e+02 7.8500e+02 2.4800e+02 2.9100e+02 1.5180e+03 1.0070e+03\n",
      " 8.9600e+02 1.1100e+02 8.1800e+02 8.7300e+02 2.6200e+02 1.4730e+03\n",
      " 9.8700e+02 3.2370e+03 4.8000e+02 6.7700e+02 1.0660e+03 1.9990e+03\n",
      " 1.8270e+03 3.7100e+03 2.5500e+02 4.9230e+03 1.7200e+02 2.6060e+03\n",
      " 3.0130e+03 5.5000e+02 1.3690e+03 7.8900e+02 4.7020e+03 3.1040e+03\n",
      " 1.4900e+02 7.2800e+02 7.1000e+01 4.3310e+03 1.1470e+03 6.5550e+03\n",
      " 4.9700e+02 5.1880e+03 3.9930e+03 1.0130e+03 2.5820e+03 9.0600e+02\n",
      " 3.1200e+02 3.5270e+03 9.6700e+02 3.7830e+03 3.7600e+02 5.4200e+02\n",
      " 1.6270e+03 1.4252e+04 3.1960e+03 7.2600e+02 2.2120e+03 1.2250e+03\n",
      " 1.3660e+03 1.0990e+03 6.8120e+03 2.1530e+03 1.0230e+03 1.8720e+03\n",
      " 7.8000e+01 1.7160e+03 1.3770e+03 1.8120e+03 1.6800e+02]\n",
      "유니크한 카테고리 수 :  833\n",
      "null 수              :  0 \n",
      "\n",
      "VRDeck \n",
      " [0.0000e+00 5.8500e+02 6.0000e+01 1.0100e+03 2.2400e+02 9.2000e+01\n",
      " 6.9000e+02 3.4300e+02 6.3000e+01 1.2350e+03 1.5500e+02 4.2200e+02\n",
      " 2.3000e+01 3.6700e+02 1.5000e+01 4.8400e+02 5.5600e+02 6.8600e+02\n",
      " 1.5760e+03 2.4580e+03 9.7000e+01 5.0900e+02 3.5230e+03 2.2300e+02\n",
      " 1.3600e+02 1.1720e+03 4.5000e+02 5.8000e+01 1.6300e+02 1.8000e+01\n",
      " 1.1930e+03 2.4390e+03 4.1000e+01 1.7140e+03 6.4390e+03 7.8000e+01\n",
      " 1.0000e+02 1.2560e+03 9.4000e+01 1.0000e+00 2.2000e+01 4.0550e+03\n",
      " 2.8100e+02 5.2400e+02 6.0800e+02 1.2400e+02 9.0000e+00 5.2200e+02\n",
      " 7.9600e+02 7.0000e+00 5.8320e+03 1.3730e+03 1.9750e+03 3.0000e+00\n",
      " 5.2880e+03 5.5300e+02 1.4000e+01 1.4440e+03 5.0000e+00 8.1000e+01\n",
      " 8.3500e+02 6.8900e+02 4.6100e+02 2.7560e+03 1.6000e+01 4.6000e+01\n",
      " 2.1000e+01 6.5900e+02 1.2980e+03 4.4600e+02 2.4710e+03 4.6410e+03\n",
      " 1.1300e+03 3.2000e+01 4.2400e+02 1.0250e+03 1.7000e+01 1.3850e+03\n",
      " 2.5600e+02 5.7000e+01 3.1000e+01 3.0400e+02 4.3700e+02 4.6790e+03\n",
      " 2.5830e+03 2.3100e+02 1.7100e+02 2.7600e+02 6.0000e+00 7.8800e+02\n",
      " 1.7200e+02 5.2000e+01 1.4400e+02 2.5050e+03 4.4190e+03 1.4560e+03\n",
      " 3.5060e+03 1.5940e+04 5.1000e+01 1.0000e+01 5.0600e+02 2.0000e+00\n",
      " 3.0000e+01 2.7700e+02 4.0000e+00 4.6200e+02 9.3000e+01 8.3300e+02\n",
      " 5.9150e+03 6.2200e+02 6.5120e+03 5.2180e+03 3.4000e+01 4.1500e+02\n",
      " 4.8200e+02 6.5300e+02 6.3200e+02 9.0800e+02 1.5600e+02 2.2570e+03\n",
      " 1.6500e+02 1.4050e+03 7.4000e+01 9.0000e+01 5.3000e+01 5.0800e+02\n",
      " 4.4900e+02 1.9000e+01 1.2863e+04 1.1000e+01 5.1300e+02 6.1190e+03\n",
      " 4.9000e+01 2.4000e+01 6.1510e+03 1.3800e+02 2.1580e+03 2.1500e+02\n",
      " 7.5300e+02 9.4200e+02 2.1700e+03 3.0700e+02 1.5820e+03 3.6100e+02\n",
      " 1.2000e+02 5.8150e+03 1.7420e+03 1.0310e+03 2.8820e+03 2.7500e+02\n",
      " 7.9000e+01 2.7000e+01 6.5600e+02 5.3800e+02 2.6000e+01 3.6000e+01\n",
      " 3.3110e+03 3.4100e+02 8.0000e+00 4.4800e+02 7.5570e+03 1.5900e+02\n",
      " 6.1100e+02 2.8000e+02 1.2600e+03 5.0000e+01 1.6100e+02 1.0300e+02\n",
      " 3.5100e+02 4.1900e+02 6.6100e+02 5.7000e+02 5.5000e+01 9.5000e+01\n",
      " 1.0600e+02 7.3820e+03 7.4700e+03 1.2000e+01 4.7000e+01 5.8240e+03\n",
      " 2.1880e+03 5.9000e+01 2.7350e+03 1.4200e+02 2.0000e+01 8.9000e+01\n",
      " 3.8770e+03 5.7800e+02 4.4200e+02 1.3500e+02 1.4268e+04 8.0030e+03\n",
      " 5.9100e+02 6.9200e+02 1.0800e+02 4.0000e+01 1.1240e+03 2.7320e+03\n",
      " 7.2100e+02 2.9700e+02 3.6200e+02 3.2700e+02 8.7900e+02 3.0800e+02\n",
      " 1.8300e+02 1.4000e+02 1.8100e+02 1.1410e+03 2.8900e+02 1.6620e+03\n",
      " 6.0830e+03 5.5500e+03 2.6000e+02 6.5200e+02 2.1490e+03 1.3300e+02\n",
      " 1.9610e+03 5.3000e+02 1.2226e+04 4.3000e+02 1.0500e+02 8.4000e+01\n",
      " 3.8700e+02 1.5100e+02 1.0680e+03 1.1700e+02 8.7000e+01 7.0460e+03\n",
      " 4.4110e+03 3.0200e+02 3.5000e+02 7.1000e+01 1.1250e+03 8.6000e+01\n",
      " 8.8000e+02 1.5200e+02 6.5400e+02 1.4080e+03 1.0550e+03 3.1550e+03\n",
      " 9.5400e+02 4.9200e+02 8.7200e+02 6.1700e+02 8.7800e+02 3.9000e+01\n",
      " 8.8200e+02 7.7900e+02 2.6010e+03 1.9170e+03 1.3100e+02 8.6500e+02\n",
      " 8.9600e+02 1.1632e+04 3.5500e+02 2.5200e+02 2.7200e+02 3.2090e+03\n",
      " 2.6900e+02 6.6600e+02 1.1280e+03 3.0300e+02 1.1100e+02 7.1610e+03\n",
      " 6.5700e+02 6.4300e+02 1.5930e+03 1.5125e+04 8.3000e+01 2.2700e+02\n",
      " 2.3580e+03 2.0060e+03 6.5000e+01 1.7650e+03 3.7700e+02 2.0300e+02\n",
      " 6.8500e+02 6.0310e+03 1.9086e+04 6.3100e+02 4.5000e+01 2.8270e+03\n",
      " 7.2000e+01 5.6000e+01 9.8200e+02 1.8200e+02 4.9600e+02 8.7300e+02\n",
      " 1.3250e+03 2.4060e+03 1.1800e+02 3.2170e+03 3.2230e+03 4.7800e+02\n",
      " 3.2000e+02 2.3300e+02 5.8700e+02 3.1500e+02 3.6900e+02 6.7000e+01\n",
      " 1.0650e+03 7.0300e+02 3.7900e+02 1.3170e+03 1.2010e+03 6.6800e+02\n",
      " 7.5000e+01 1.2810e+03 2.5000e+01 9.1000e+01 3.8080e+03 5.7200e+02\n",
      " 3.1400e+02 6.2000e+02 2.0700e+02 7.6600e+02 1.2920e+03 3.5900e+03\n",
      " 2.6500e+02 9.6000e+02 8.2000e+01 1.3000e+01 3.5400e+02 2.1660e+03\n",
      " 6.4200e+02 3.1780e+03 1.5700e+02 5.5200e+02 5.1400e+02 7.6000e+01\n",
      " 4.3200e+02 4.4840e+03 1.3870e+03 3.2900e+02 1.9190e+03 1.9400e+02\n",
      " 2.0800e+02 4.7300e+02 4.0300e+02 1.1330e+03 2.1800e+02 5.5100e+02\n",
      " 1.2390e+03 1.3700e+02 4.7360e+03 7.1200e+02 8.0800e+02 4.4000e+01\n",
      " 8.0300e+02 7.3900e+02 3.6560e+03 3.7620e+03 1.2700e+02 7.0900e+02\n",
      " 1.5690e+03 1.4700e+03 1.5630e+03 5.4700e+02 1.1520e+03 2.8620e+03\n",
      " 2.4300e+02 1.1333e+04 3.9600e+02 1.1130e+03 6.0300e+02 1.7900e+02\n",
      " 1.1110e+03 1.7400e+02 2.4130e+03 6.4500e+02 6.2000e+01 5.1800e+02\n",
      " 6.0760e+03 7.6900e+02 7.4200e+02 7.7000e+01 5.5800e+02 8.7700e+02\n",
      " 2.8600e+02 3.3000e+01 5.5130e+03 2.3600e+02 3.8000e+01 2.1860e+03\n",
      " 2.9220e+03 3.6000e+02 2.2430e+03 2.7900e+02 4.3000e+01 6.7700e+02\n",
      " 6.8000e+01 4.6300e+02 2.5100e+02 2.3340e+03 1.2060e+03 6.7800e+02\n",
      " 1.1700e+03 2.6040e+03 1.0400e+02 3.1040e+03 2.7220e+03 2.1700e+02\n",
      " 9.5900e+02 1.0040e+03 1.2970e+03 1.3970e+03 2.0190e+03 4.0900e+02\n",
      " 8.9700e+02 5.3400e+02 6.4900e+02 6.4950e+03 1.0780e+03 4.2000e+01\n",
      " 2.1950e+03 1.2300e+02 2.9440e+03 6.6300e+02 1.5680e+03 8.2550e+03\n",
      " 3.7200e+02 2.8210e+03 1.3200e+02 3.7000e+01 2.0600e+02 8.0000e+01\n",
      " 3.2950e+03 3.4200e+02 3.0810e+03 2.7100e+02 5.8830e+03 2.5500e+02\n",
      " 6.3660e+03 7.7000e+02 4.0000e+02 4.5900e+02 3.2500e+02 4.5300e+02\n",
      " 7.4800e+02 1.2500e+02 3.5210e+03 6.8100e+02 1.3900e+03 8.7100e+02\n",
      " 3.5700e+02 5.0500e+02 1.5440e+03 7.6300e+02 7.2600e+02 7.1700e+02\n",
      " 5.5000e+02 7.1500e+02 1.8850e+03 3.6410e+03 3.1290e+03 1.2200e+02\n",
      " 1.2800e+02 4.1110e+03 1.0200e+03 1.4200e+03 5.9200e+02 6.3800e+02\n",
      " 1.1090e+03 4.3400e+02 5.3900e+02 1.4210e+03 1.5350e+03 3.4500e+02\n",
      " 5.2800e+02 1.1225e+04 1.9800e+02 8.4430e+03 2.9910e+03 3.8200e+02\n",
      " 1.6380e+03 1.6490e+03 4.5200e+02 1.7790e+03 1.9100e+02 7.5500e+02\n",
      " 3.6300e+02 7.7490e+03 8.1800e+02 4.0930e+03 5.4000e+01 1.1450e+03\n",
      " 2.8500e+02 4.4300e+02 2.0500e+02 7.9850e+03 1.7840e+03 3.1520e+03\n",
      " 6.1000e+01 7.0000e+01 7.7800e+02 2.1750e+03 7.5600e+02 3.0620e+03\n",
      " 2.1040e+03 1.4300e+02 7.6500e+02 3.6800e+02 2.9000e+01 9.3300e+02\n",
      " 3.8100e+02 5.8600e+02 1.5080e+03 5.2200e+03 2.5900e+02 4.1020e+03\n",
      " 8.8600e+02 2.0920e+03 1.2100e+03 1.3400e+02 7.3120e+03 5.0700e+02\n",
      " 4.3800e+02 1.1150e+03 4.2500e+02 3.1000e+02 3.2800e+02 3.3000e+02\n",
      " 7.2800e+02 3.4900e+02 5.0300e+02 8.8300e+02 1.7000e+02 1.9300e+02\n",
      " 7.8500e+02 3.0850e+03 4.8800e+02 1.0750e+03 1.3660e+03 4.7900e+02\n",
      " 2.7770e+03 6.2400e+02 2.1600e+02 6.4000e+01 6.3000e+02 3.1700e+02\n",
      " 8.1400e+02 8.0900e+02 3.4000e+02 9.0900e+02 2.2730e+03 8.1700e+02\n",
      " 6.0400e+02 6.3400e+02 5.5400e+02 1.9940e+03 1.5040e+03 1.3040e+03\n",
      " 5.6100e+02 2.1300e+02 1.2730e+03 4.8000e+01 3.7300e+02 1.5400e+02\n",
      " 1.9860e+03 7.3500e+02 9.7700e+02 5.1600e+02 3.8600e+02 4.4350e+03\n",
      " 4.8500e+02 4.3300e+02 4.3260e+03 2.8870e+03 7.2400e+02 5.2700e+02\n",
      " 1.4700e+02 5.8100e+02 2.4000e+02 1.6240e+03 8.6300e+02 3.2340e+03\n",
      " 5.6400e+02 5.6110e+03 1.6514e+04 1.5150e+03 1.0587e+04 3.2720e+03\n",
      " 1.3940e+03 1.1900e+03 2.1200e+02 7.6430e+03 1.5190e+03 3.2400e+02\n",
      " 7.9200e+02 7.2300e+02 6.7000e+02 1.4587e+04 7.3000e+02 8.2800e+02\n",
      " 9.0000e+02 6.8200e+02 2.6800e+02 1.1940e+03 1.0200e+02 8.5000e+01\n",
      " 8.0200e+02 6.6000e+01 3.9940e+03 1.0810e+03 1.0980e+03 2.2272e+04\n",
      " 1.0100e+02 7.2200e+02 1.4100e+02 2.8400e+02 1.8800e+02 9.1300e+02\n",
      " 7.0700e+02 9.1400e+02 1.8440e+03 3.1850e+03 2.1680e+03 7.3600e+02\n",
      " 1.2900e+02 1.4120e+03 1.6600e+02 7.0000e+02 9.9700e+02 1.0900e+02\n",
      " 4.8900e+02 2.2500e+02 1.8670e+04 9.6260e+03 1.7540e+03 4.5320e+03\n",
      " 1.4500e+02 1.1500e+02 3.6500e+02 1.7300e+02 7.7300e+02 2.4400e+02\n",
      " 9.6130e+03 6.2600e+02 1.6700e+02 2.0000e+03 4.7200e+02 1.0050e+03\n",
      " 9.1900e+02 5.8790e+03 5.1000e+02 4.0700e+02 2.6550e+03 2.0790e+03\n",
      " 4.8300e+02 8.8000e+01 2.2600e+02 1.5520e+03 5.5900e+02 8.7600e+02\n",
      " 1.3340e+03 3.2100e+02 1.2380e+03 8.5200e+02 7.1900e+02 8.9200e+02\n",
      " 7.2290e+03 6.5800e+02 1.9660e+03 3.1900e+02 3.3100e+02 1.8680e+03\n",
      " 1.0260e+03 1.6170e+03 1.7210e+03 5.6800e+02 2.2000e+02 6.2900e+02\n",
      " 4.8600e+02 7.3300e+02 1.1200e+03 3.2280e+03 3.1530e+03 1.1650e+03\n",
      " 7.8300e+02 4.0090e+03 2.3420e+03 1.1060e+03 7.1300e+02 2.2100e+02\n",
      " 3.0500e+02 5.1900e+02 9.0100e+02 4.5400e+02 3.5800e+02 3.9300e+02\n",
      " 1.2410e+03 8.4900e+02 3.8600e+03 1.1300e+02 3.7400e+02 7.8900e+02\n",
      " 7.5800e+02 1.0720e+03 2.4200e+02 2.6980e+03 1.3390e+03 1.6060e+03\n",
      " 6.9460e+03 8.5300e+02 7.9000e+02 2.0670e+03 1.6120e+03 6.9000e+01\n",
      " 9.9000e+01 4.1100e+02 9.7400e+02 1.6200e+02 7.1600e+02 2.2190e+03\n",
      " 4.5500e+02 6.2500e+02 8.4400e+02 1.9050e+03 1.3760e+03 1.9500e+02\n",
      " 2.5670e+03 2.6990e+03 5.8650e+03 1.9700e+02 1.7570e+03 1.5110e+03\n",
      " 3.3300e+02 7.0500e+02 5.2760e+03 6.6400e+02 4.4570e+03 1.2160e+03\n",
      " 1.5000e+02 2.8900e+03 4.2810e+03 1.5180e+03 1.1900e+02 2.6400e+02\n",
      " 7.9400e+02 3.9100e+02 7.7200e+02 5.1820e+03 1.4600e+02 1.6160e+03\n",
      " 2.4900e+02 1.3020e+03 7.5400e+02 1.0700e+03 1.0360e+03 8.0500e+02\n",
      " 7.3400e+02 1.9220e+03 2.5620e+03 1.2260e+03 2.7000e+02 4.5740e+03\n",
      " 1.5620e+03 3.6490e+03 1.4850e+03 2.7240e+03 3.1600e+02 3.9700e+02\n",
      " 1.2430e+03 1.7600e+02 1.8130e+03 1.2113e+04 2.2720e+03 3.5000e+01\n",
      " 5.4300e+02 4.8100e+02 1.7500e+02 1.8900e+02 9.0600e+02 6.0020e+03\n",
      " 1.1440e+03 3.0000e+03 3.3800e+02 9.2800e+02 9.9800e+02 2.1460e+03\n",
      " 2.6280e+03 4.6170e+03 1.9950e+03 3.1300e+02 9.6300e+02 1.8410e+03\n",
      " 8.6700e+02 1.7200e+03 2.6500e+03 3.1080e+03 3.0930e+03 3.3400e+02\n",
      " 1.4834e+04 4.9400e+02 6.1800e+02 5.2300e+02]\n",
      "유니크한 카테고리 수 :  796\n",
      "null 수              :  0 \n",
      "\n",
      "group \n",
      " [  13   18   19 ... 9271 9273 9277]\n",
      "유니크한 카테고리 수 :  3063\n",
      "null 수              :  0 \n",
      "\n",
      "last_name \n",
      " ['Carsoning' 'Peckers' 'Unhearfus' ... 'Roses' 'Pumpeshaft' 'Cure']\n",
      "유니크한 카테고리 수 :  1725\n",
      "null 수              :  0 \n",
      "\n",
      "spend_fillna_0 \n",
      " [   0. 2832. 7418. ... 5478. 1818. 3898.]\n",
      "유니크한 카테고리 수 :  1437\n",
      "null 수              :  0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIP_fill_na_False \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "deck \n",
      " ['G' 'F' 'C' 'B' 'D' 'E' 'A' 'T']\n",
      "유니크한 카테고리 수 :  8\n",
      "null 수              :  0 \n",
      "\n",
      "num \n",
      " [   3    4    0 ... 1890 1794 1503]\n",
      "유니크한 카테고리 수 :  1505\n",
      "null 수              :  0 \n",
      "\n",
      "side \n",
      " ['S' 'P']\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df2.columns :\n",
    "    print(col,\"\\n\", df2[col].unique())\n",
    "    print('유니크한 카테고리 수 : ',len(df2[col].unique()))\n",
    "    print('null 수              : ', df2[col].isna().sum(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder_col = ['HomePlanet', 'CryoSleep', 'Destination','last_name','VIP_fill_na_False', 'deck','side']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_HomePlanet = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['HomePlanet'] = encoder_HomePlanet.fit_transform(df['HomePlanet'])\n",
    "df2['HomePlanet'] = encoder_HomePlanet.transform(df2['HomePlanet'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_CryoSleep = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['CryoSleep'] = encoder_CryoSleep.fit_transform(df['CryoSleep'])\n",
    "df2['CryoSleep'] = encoder_CryoSleep.transform(df2['CryoSleep'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_Destination = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['Destination'] = encoder_Destination.fit_transform(df['Destination'])\n",
    "df2['Destination'] = encoder_Destination.transform(df2['Destination'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_last_name = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['last_name'] = encoder_last_name.fit_transform(df['last_name'])\n",
    "df2['last_name'] = df2['last_name'].apply(lambda x: encoder_last_name.transform([x])[0] if x in encoder_last_name.classes_ else 0)\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_deck = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['deck'] = encoder_deck.fit_transform(df['deck'])\n",
    "df2['deck'] = encoder_deck.transform(df2['deck'])\n",
    "\n",
    "encoder_side = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['side'] = encoder_side.fit_transform(df['side'])\n",
    "df2['side'] = encoder_side.transform(df2['side'])\n",
    "\n",
    "\n",
    "df['VIP_fill_na_False'] = df['VIP_fill_na_False'].astype(int)\n",
    "df2['VIP_fill_na_False'] = df2['VIP_fill_na_False'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HomePlanet</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088953</td>\n",
       "      <td>0.039393</td>\n",
       "      <td>0.134480</td>\n",
       "      <td>0.212442</td>\n",
       "      <td>0.068821</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.116881</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.148622</td>\n",
       "      <td>0.124166</td>\n",
       "      <td>-0.410312</td>\n",
       "      <td>-0.198557</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CryoSleep</th>\n",
       "      <td>0.088953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094802</td>\n",
       "      <td>-0.078192</td>\n",
       "      <td>-0.250173</td>\n",
       "      <td>-0.211061</td>\n",
       "      <td>-0.212977</td>\n",
       "      <td>-0.203249</td>\n",
       "      <td>-0.197524</td>\n",
       "      <td>0.467230</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>-0.386081</td>\n",
       "      <td>-0.081318</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>-0.036606</td>\n",
       "      <td>0.018982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destination</th>\n",
       "      <td>0.039393</td>\n",
       "      <td>-0.094802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005453</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>-0.110375</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>-0.056524</td>\n",
       "      <td>-0.072374</td>\n",
       "      <td>-0.110135</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>-0.098442</td>\n",
       "      <td>-0.044469</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>-0.011132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.134480</td>\n",
       "      <td>-0.078192</td>\n",
       "      <td>-0.005453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068785</td>\n",
       "      <td>0.128464</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.100213</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.186530</td>\n",
       "      <td>0.091822</td>\n",
       "      <td>-0.247145</td>\n",
       "      <td>-0.133925</td>\n",
       "      <td>0.011096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoomService</th>\n",
       "      <td>0.212442</td>\n",
       "      <td>-0.250173</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>0.068785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015126</td>\n",
       "      <td>0.052337</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>-0.018624</td>\n",
       "      <td>-0.241124</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.016133</td>\n",
       "      <td>0.234374</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>-0.009727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FoodCourt</th>\n",
       "      <td>0.068821</td>\n",
       "      <td>-0.211061</td>\n",
       "      <td>-0.110375</td>\n",
       "      <td>0.128464</td>\n",
       "      <td>-0.015126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.224572</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.125499</td>\n",
       "      <td>-0.317544</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.016528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShoppingMall</th>\n",
       "      <td>0.101507</td>\n",
       "      <td>-0.212977</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>0.052337</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.017796</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.220529</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>-0.030913</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.018390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spa</th>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.203249</td>\n",
       "      <td>-0.056524</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147658</td>\n",
       "      <td>-0.218545</td>\n",
       "      <td>-0.005198</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>-0.222740</td>\n",
       "      <td>-0.129796</td>\n",
       "      <td>0.002587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRDeck</th>\n",
       "      <td>0.040193</td>\n",
       "      <td>-0.197524</td>\n",
       "      <td>-0.072374</td>\n",
       "      <td>0.100213</td>\n",
       "      <td>-0.018624</td>\n",
       "      <td>0.224572</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.147658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.204874</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.585684</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>-0.251901</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.009258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transported</th>\n",
       "      <td>0.116881</td>\n",
       "      <td>0.467230</td>\n",
       "      <td>-0.110135</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>-0.241124</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>-0.218545</td>\n",
       "      <td>-0.204874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>-0.199514</td>\n",
       "      <td>-0.037261</td>\n",
       "      <td>-0.117590</td>\n",
       "      <td>-0.046512</td>\n",
       "      <td>0.103359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <td>-0.004601</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.017796</td>\n",
       "      <td>-0.005198</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014035</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>0.680925</td>\n",
       "      <td>-0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_name</th>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>-0.016133</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>-0.014035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-0.022861</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>0.024559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <td>0.148622</td>\n",
       "      <td>-0.386081</td>\n",
       "      <td>-0.098442</td>\n",
       "      <td>0.186530</td>\n",
       "      <td>0.234374</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.220529</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.585684</td>\n",
       "      <td>-0.199514</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162987</td>\n",
       "      <td>-0.385887</td>\n",
       "      <td>-0.211337</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <td>0.124166</td>\n",
       "      <td>-0.081318</td>\n",
       "      <td>-0.044469</td>\n",
       "      <td>0.091822</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>0.125499</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>-0.037261</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>0.162987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.181667</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.008279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck</th>\n",
       "      <td>-0.410312</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>-0.247145</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.317544</td>\n",
       "      <td>-0.030913</td>\n",
       "      <td>-0.222740</td>\n",
       "      <td>-0.251901</td>\n",
       "      <td>-0.117590</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>-0.022861</td>\n",
       "      <td>-0.385887</td>\n",
       "      <td>-0.181667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531690</td>\n",
       "      <td>-0.034944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <td>-0.198557</td>\n",
       "      <td>-0.036606</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>-0.133925</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.129796</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.046512</td>\n",
       "      <td>0.680925</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>-0.211337</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>0.531690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side</th>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>-0.011132</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>-0.009258</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>0.024559</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.034944</td>\n",
       "      <td>-0.039158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   HomePlanet  CryoSleep  Destination       Age  RoomService  \\\n",
       "HomePlanet           1.000000   0.088953     0.039393  0.134480     0.212442   \n",
       "CryoSleep            0.088953   1.000000    -0.094802 -0.078192    -0.250173   \n",
       "Destination          0.039393  -0.094802     1.000000 -0.005453     0.046382   \n",
       "Age                  0.134480  -0.078192    -0.005453  1.000000     0.068785   \n",
       "RoomService          0.212442  -0.250173     0.046382  0.068785     1.000000   \n",
       "FoodCourt            0.068821  -0.211061    -0.110375  0.128464    -0.015126   \n",
       "ShoppingMall         0.101507  -0.212977     0.025525  0.032949     0.052337   \n",
       "Spa                  0.053451  -0.203249    -0.056524  0.122865     0.009244   \n",
       "VRDeck               0.040193  -0.197524    -0.072374  0.100213    -0.018624   \n",
       "Transported          0.116881   0.467230    -0.110135 -0.075026    -0.241124   \n",
       "group               -0.004601  -0.004304    -0.003366 -0.009439     0.000375   \n",
       "last_name            0.004435   0.014009    -0.015588  0.004729    -0.016133   \n",
       "spend_fillna_0       0.148622  -0.386081    -0.098442  0.186530     0.234374   \n",
       "VIP_fill_na_False    0.124166  -0.081318    -0.044469  0.091822     0.056566   \n",
       "deck                -0.410312   0.016910     0.187318 -0.247145    -0.030345   \n",
       "num                 -0.198557  -0.036606     0.102247 -0.133925    -0.013001   \n",
       "side                 0.001229   0.018982    -0.011132  0.011096    -0.009727   \n",
       "\n",
       "                   FoodCourt  ShoppingMall       Spa    VRDeck  Transported  \\\n",
       "HomePlanet          0.068821      0.101507  0.053451  0.040193     0.116881   \n",
       "CryoSleep          -0.211061     -0.212977 -0.203249 -0.197524     0.467230   \n",
       "Destination        -0.110375      0.025525 -0.056524 -0.072374    -0.110135   \n",
       "Age                 0.128464      0.032949  0.122865  0.100213    -0.075026   \n",
       "RoomService        -0.015126      0.052337  0.009244 -0.018624    -0.241124   \n",
       "FoodCourt           1.000000     -0.013717  0.221468  0.224572     0.045583   \n",
       "ShoppingMall       -0.013717      1.000000  0.014542 -0.007849     0.009391   \n",
       "Spa                 0.221468      0.014542  1.000000  0.147658    -0.218545   \n",
       "VRDeck              0.224572     -0.007849  0.147658  1.000000    -0.204874   \n",
       "Transported         0.045583      0.009391 -0.218545 -0.204874     1.000000   \n",
       "group              -0.009200      0.017796 -0.005198  0.015945     0.021491   \n",
       "last_name           0.012275      0.001924  0.001100  0.007743     0.015370   \n",
       "spend_fillna_0      0.742608      0.220529  0.592656  0.585684    -0.199514   \n",
       "VIP_fill_na_False   0.125499      0.018412  0.060991  0.123061    -0.037261   \n",
       "deck               -0.317544     -0.030913 -0.222740 -0.251901    -0.117590   \n",
       "num                -0.179688      0.002405 -0.129796 -0.134377    -0.046512   \n",
       "side                0.016528     -0.018390  0.002587 -0.009258     0.103359   \n",
       "\n",
       "                      group  last_name  spend_fillna_0  VIP_fill_na_False  \\\n",
       "HomePlanet        -0.004601   0.004435        0.148622           0.124166   \n",
       "CryoSleep         -0.004304   0.014009       -0.386081          -0.081318   \n",
       "Destination       -0.003366  -0.015588       -0.098442          -0.044469   \n",
       "Age               -0.009439   0.004729        0.186530           0.091822   \n",
       "RoomService        0.000375  -0.016133        0.234374           0.056566   \n",
       "FoodCourt         -0.009200   0.012275        0.742608           0.125499   \n",
       "ShoppingMall       0.017796   0.001924        0.220529           0.018412   \n",
       "Spa               -0.005198   0.001100        0.592656           0.060991   \n",
       "VRDeck             0.015945   0.007743        0.585684           0.123061   \n",
       "Transported        0.021491   0.015370       -0.199514          -0.037261   \n",
       "group              1.000000  -0.014035        0.003012           0.013608   \n",
       "last_name         -0.014035   1.000000        0.007172          -0.000562   \n",
       "spend_fillna_0     0.003012   0.007172        1.000000           0.162987   \n",
       "VIP_fill_na_False  0.013608  -0.000562        0.162987           1.000000   \n",
       "deck              -0.000370  -0.022861       -0.385887          -0.181667   \n",
       "num                0.680925  -0.022568       -0.211337          -0.098585   \n",
       "side              -0.006824   0.024559        0.000487          -0.008279   \n",
       "\n",
       "                       deck       num      side  \n",
       "HomePlanet        -0.410312 -0.198557  0.001229  \n",
       "CryoSleep          0.016910 -0.036606  0.018982  \n",
       "Destination        0.187318  0.102247 -0.011132  \n",
       "Age               -0.247145 -0.133925  0.011096  \n",
       "RoomService       -0.030345 -0.013001 -0.009727  \n",
       "FoodCourt         -0.317544 -0.179688  0.016528  \n",
       "ShoppingMall      -0.030913  0.002405 -0.018390  \n",
       "Spa               -0.222740 -0.129796  0.002587  \n",
       "VRDeck            -0.251901 -0.134377 -0.009258  \n",
       "Transported       -0.117590 -0.046512  0.103359  \n",
       "group             -0.000370  0.680925 -0.006824  \n",
       "last_name         -0.022861 -0.022568  0.024559  \n",
       "spend_fillna_0    -0.385887 -0.211337  0.000487  \n",
       "VIP_fill_na_False -0.181667 -0.098585 -0.008279  \n",
       "deck               1.000000  0.531690 -0.034944  \n",
       "num                0.531690  1.000000 -0.039158  \n",
       "side              -0.034944 -0.039158  1.000000  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1505</td>\n",
       "      <td>2832.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>7418.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>926</td>\n",
       "      <td>645.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9266</td>\n",
       "      <td>1531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>9269</td>\n",
       "      <td>1791</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9271</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>9273</td>\n",
       "      <td>457</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9277</td>\n",
       "      <td>1190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4277 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomePlanet  CryoSleep  Destination   Age  RoomService  FoodCourt  \\\n",
       "0              0          1            2  27.0          0.0        0.0   \n",
       "1              0          0            2  19.0          0.0        9.0   \n",
       "2              1          1            0  31.0          0.0        0.0   \n",
       "3              1          0            2  38.0          0.0     6652.0   \n",
       "4              0          0            2  20.0         10.0        0.0   \n",
       "...          ...        ...          ...   ...          ...        ...   \n",
       "4272           0          1            2  34.0          0.0        0.0   \n",
       "4273           0          0            2  42.0          0.0      847.0   \n",
       "4274           2          1            0   NaN          0.0        0.0   \n",
       "4275           1          0            1   NaN          0.0     2680.0   \n",
       "4276           0          1            1  43.0          0.0        0.0   \n",
       "\n",
       "      ShoppingMall     Spa  VRDeck  group  last_name  spend_fillna_0  \\\n",
       "0              0.0     0.0     0.0     13        344             0.0   \n",
       "1              0.0  2823.0     0.0     18       1505          2832.0   \n",
       "2              0.0     0.0     0.0     19       2070             0.0   \n",
       "3              0.0   181.0   585.0     21        324          7418.0   \n",
       "4            635.0     0.0     0.0     23        926           645.0   \n",
       "...            ...     ...     ...    ...        ...             ...   \n",
       "4272           0.0     0.0     0.0   9266       1531             0.0   \n",
       "4273          17.0    10.0   144.0   9269       1791          1018.0   \n",
       "4274           0.0     0.0     0.0   9271          0             0.0   \n",
       "4275           0.0     0.0   523.0   9273        457          3203.0   \n",
       "4276           0.0     0.0     0.0   9277       1190             0.0   \n",
       "\n",
       "      VIP_fill_na_False  deck   num  side  \n",
       "0                     0     6     3     1  \n",
       "1                     0     5     4     1  \n",
       "2                     0     2     0     1  \n",
       "3                     0     2     1     1  \n",
       "4                     0     5     5     1  \n",
       "...                 ...   ...   ...   ...  \n",
       "4272                  0     6  1496     1  \n",
       "4273                  0     3   296     0  \n",
       "4274                  0     3   296     0  \n",
       "4275                  0     3   297     0  \n",
       "4276                  0     6  1498     1  \n",
       "\n",
       "[4277 rows x 16 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Transported'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transported'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-13b52043be8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Transported'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Transported'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3807\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3808\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transported'"
     ]
    }
   ],
   "source": [
    "# col = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
    "#        'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'group', 'last_name',\n",
    "#        'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "\n",
    "y = df['Transported']\n",
    "df = df.drop('Transported', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
    "#        'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'group', 'last_name',\n",
    "#        'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "col = ['HomePlanet', 'CryoSleep', 'Destination', 'RoomService',\n",
    "       'FoodCourt',  'Spa', 'VRDeck', 'group', 'last_name',\n",
    "       'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "df_ = df.loc[:,col]\n",
    "df2_ = df2.loc[:,col]\n",
    "\n",
    "X_train = np.array(df_)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_test = np.array(df2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2109</td>\n",
       "      <td>736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>10383.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>5176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1778</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9276</td>\n",
       "      <td>1416</td>\n",
       "      <td>8536.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9278</td>\n",
       "      <td>1341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9279</td>\n",
       "      <td>470</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>9280</td>\n",
       "      <td>996</td>\n",
       "      <td>4637.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9280</td>\n",
       "      <td>996</td>\n",
       "      <td>4826.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomePlanet  CryoSleep  Destination  RoomService  FoodCourt  \\\n",
       "0              1          0            2          0.0        0.0   \n",
       "1              0          0            2        109.0        9.0   \n",
       "2              1          0            2         43.0     3576.0   \n",
       "3              1          0            2          0.0     1283.0   \n",
       "4              0          0            2        303.0       70.0   \n",
       "...          ...        ...          ...          ...        ...   \n",
       "8688           1          0            0          0.0     6819.0   \n",
       "8689           0          1            1          0.0        0.0   \n",
       "8690           0          0            2          0.0        0.0   \n",
       "8691           1          0            0          0.0     1049.0   \n",
       "8692           1          0            2        126.0     4688.0   \n",
       "\n",
       "      ShoppingMall     Spa  VRDeck  group  last_name  spend_fillna_0  \\\n",
       "0              0.0     0.0     0.0      1       1431             0.0   \n",
       "1             25.0   549.0    44.0      2       2109           736.0   \n",
       "2              0.0  6715.0    49.0      3       1990         10383.0   \n",
       "3            371.0  3329.0   193.0      3       1990          5176.0   \n",
       "4            151.0   565.0     2.0      4       1778          1091.0   \n",
       "...            ...     ...     ...    ...        ...             ...   \n",
       "8688           0.0  1643.0    74.0   9276       1416          8536.0   \n",
       "8689           0.0     0.0     0.0   9278       1341             0.0   \n",
       "8690        1872.0     1.0     0.0   9279        470          1873.0   \n",
       "8691           0.0   353.0  3235.0   9280        996          4637.0   \n",
       "8692           0.0     0.0    12.0   9280        996          4826.0   \n",
       "\n",
       "      VIP_fill_na_False  deck   num  side  \n",
       "0                     0     1     0     0  \n",
       "1                     0     5     0     1  \n",
       "2                     1     0     0     1  \n",
       "3                     0     0     0     1  \n",
       "4                     0     5     1     1  \n",
       "...                 ...   ...   ...   ...  \n",
       "8688                  1     0    98     0  \n",
       "8689                  0     6  1499     1  \n",
       "8690                  0     6  1500     1  \n",
       "8691                  0     4   608     1  \n",
       "8692                  0     4   608     1  \n",
       "\n",
       "[8693 rows x 15 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4277, 15)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    " \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler() \n",
    "\n",
    "# scaler.fit(X_train) \n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=12) # 주성분을 몇개로 할지 결정\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "# print(np.cumsum(pca.explained_variance_ratio_)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,train_size=0.8,random_state=32, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_train_pca,y_train,train_size=0.2,random_state=32, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6954, 15)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 왜 AGE를 빼야 학습이 진행되는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "435/435 [==============================] - 4s 5ms/step - loss: 0.7033 - binary_accuracy: 0.7010 - val_loss: 0.6461 - val_binary_accuracy: 0.6987\n",
      "Epoch 2/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.6083 - binary_accuracy: 0.7479 - val_loss: 0.5707 - val_binary_accuracy: 0.7941\n",
      "Epoch 3/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5827 - binary_accuracy: 0.7514 - val_loss: 0.5511 - val_binary_accuracy: 0.7769\n",
      "Epoch 4/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5740 - binary_accuracy: 0.7506 - val_loss: 0.5445 - val_binary_accuracy: 0.7855\n",
      "Epoch 5/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5626 - binary_accuracy: 0.7587 - val_loss: 0.5706 - val_binary_accuracy: 0.7918\n",
      "Epoch 6/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5625 - binary_accuracy: 0.7617 - val_loss: 0.5740 - val_binary_accuracy: 0.7855\n",
      "Epoch 7/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5611 - binary_accuracy: 0.7613 - val_loss: 0.5631 - val_binary_accuracy: 0.7424\n",
      "Epoch 8/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5689 - binary_accuracy: 0.7547 - val_loss: 0.5368 - val_binary_accuracy: 0.7884\n",
      "Epoch 9/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5674 - binary_accuracy: 0.7509 - val_loss: 0.5786 - val_binary_accuracy: 0.7872\n",
      "Epoch 10/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5703 - binary_accuracy: 0.7495 - val_loss: 0.6278 - val_binary_accuracy: 0.5227\n",
      "Epoch 11/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5532 - binary_accuracy: 0.7603 - val_loss: 0.5718 - val_binary_accuracy: 0.7269\n",
      "Epoch 12/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5586 - binary_accuracy: 0.7570 - val_loss: 0.5335 - val_binary_accuracy: 0.7930\n",
      "Epoch 13/100\n",
      "435/435 [==============================] - 2s 4ms/step - loss: 0.5591 - binary_accuracy: 0.7584 - val_loss: 0.5779 - val_binary_accuracy: 0.7884\n",
      "Epoch 14/100\n",
      " 59/435 [===>..........................] - ETA: 1s - loss: 0.5663 - binary_accuracy: 0.7521"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-307-6a5f51f5ee09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m history = model.fit(X_train, y_train, \n\u001b[0m\u001b[0;32m     33\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(12, input_shape=(X_train.shape[1],), \n",
    "                       activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'\n",
    "                      ))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 6, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?? 근데 왜 난 여지껏 트레인 비율을 0.2로 한거임?\n",
    "\n",
    "# layer 3층 64,64,1, 배치사이즈 16, 각 레이어에 l1규제 (0.001), 레이어 사이 배치정규화, dropout(0.3) 적용. -> val 성능 79%\n",
    "# layer 4층 8,64,64,1, 배치사이즈 16, 각 레이어에 l1규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 79.47%\n",
    "# 쓸데없는 변수는 버려야 하나?\n",
    "# 변수간의 상관성이 아마 매우 클텐데\n",
    "# layer 4층 8,12,12,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 79.30% 오히려 train loss 매우 흔들림\n",
    "# layer 4층 4,96,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 77.70% train loss, val loss 매우 흔들림\n",
    "# layer 4층 4,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 배치정규화 삭제, dropout(0.2) 적용. -> val 성능 78.84%\n",
    "# layer 4층 4,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 배치정규화 삭제, 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.17% 초반 학습이 안정적\n",
    "# layer 4층 8,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.12% \n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.25% \n",
    "\n",
    "# 확실히 input shape를 늘려야 성능이 올라가네. 근데 왜냐 이게.. 차원 축소하면 상관관계 사라지지 않나?\n",
    "\n",
    "# pca적용 -> 8개 차원, 75%  분산 설명\n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 74.32% \n",
    "\n",
    "# pca적용 -> 12개 차원, 95%  분산 설명\n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 77.61% 반복시에 성능 상승(79.31)이 보이긴 했으나, 최종적으론 79-77 사이를 오감\n",
    "\n",
    "\n",
    "# 1D cnn 적용 -> 커널 5, 채널 32,32,1, dropout(0.2) -> 78.02 -> 이게 근데 사실 되는게 이상하긴해 ㅋㅋ, val loss 많이 벌어짐\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "543/544 [============================>.] - ETA: 0s - loss: 0.8597 - binary_accuracy: 0.7102WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 2s 2ms/step - loss: 0.8597 - binary_accuracy: 0.7100\n",
      "Epoch 2/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.7339 - binary_accuracy: 0.7560WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.7348 - binary_accuracy: 0.7541\n",
      "Epoch 3/50\n",
      "544/544 [==============================] - ETA: 0s - loss: 0.6770 - binary_accuracy: 0.7736WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.6770 - binary_accuracy: 0.7736\n",
      "Epoch 4/50\n",
      "542/544 [============================>.] - ETA: 0s - loss: 0.6333 - binary_accuracy: 0.7825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.6330 - binary_accuracy: 0.7827\n",
      "Epoch 5/50\n",
      "544/544 [==============================] - ETA: 0s - loss: 0.6034 - binary_accuracy: 0.7861WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.6034 - binary_accuracy: 0.7861\n",
      "Epoch 6/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.5723 - binary_accuracy: 0.7854WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5718 - binary_accuracy: 0.7856\n",
      "Epoch 7/50\n",
      "541/544 [============================>.] - ETA: 0s - loss: 0.5510 - binary_accuracy: 0.7869WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5516 - binary_accuracy: 0.7864\n",
      "Epoch 8/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.5399 - binary_accuracy: 0.7852WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5377 - binary_accuracy: 0.7861\n",
      "Epoch 9/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.5226 - binary_accuracy: 0.7937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5217 - binary_accuracy: 0.7939\n",
      "Epoch 10/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.5143 - binary_accuracy: 0.7871WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5166 - binary_accuracy: 0.7855\n",
      "Epoch 11/50\n",
      "535/544 [============================>.] - ETA: 0s - loss: 0.5080 - binary_accuracy: 0.7898WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5071 - binary_accuracy: 0.7910\n",
      "Epoch 12/50\n",
      "523/544 [===========================>..] - ETA: 0s - loss: 0.5018 - binary_accuracy: 0.7916WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5015 - binary_accuracy: 0.7912\n",
      "Epoch 13/50\n",
      "532/544 [============================>.] - ETA: 0s - loss: 0.4966 - binary_accuracy: 0.7918WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4961 - binary_accuracy: 0.7920\n",
      "Epoch 14/50\n",
      "542/544 [============================>.] - ETA: 0s - loss: 0.4923 - binary_accuracy: 0.7897WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4925 - binary_accuracy: 0.7897\n",
      "Epoch 15/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4902 - binary_accuracy: 0.7919WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4899 - binary_accuracy: 0.7919\n",
      "Epoch 16/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.4853 - binary_accuracy: 0.7938WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4843 - binary_accuracy: 0.7947\n",
      "Epoch 17/50\n",
      "534/544 [============================>.] - ETA: 0s - loss: 0.4853 - binary_accuracy: 0.7934WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4874 - binary_accuracy: 0.7924\n",
      "Epoch 18/50\n",
      "537/544 [============================>.] - ETA: 0s - loss: 0.4884 - binary_accuracy: 0.7892WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4877 - binary_accuracy: 0.7899\n",
      "Epoch 19/50\n",
      "522/544 [===========================>..] - ETA: 0s - loss: 0.4796 - binary_accuracy: 0.7939WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4806 - binary_accuracy: 0.7937\n",
      "Epoch 20/50\n",
      "523/544 [===========================>..] - ETA: 0s - loss: 0.4832 - binary_accuracy: 0.7905WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4820 - binary_accuracy: 0.7910\n",
      "Epoch 21/50\n",
      "530/544 [============================>.] - ETA: 0s - loss: 0.4811 - binary_accuracy: 0.7965WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4822 - binary_accuracy: 0.7965\n",
      "Epoch 22/50\n",
      "527/544 [============================>.] - ETA: 0s - loss: 0.4791 - binary_accuracy: 0.7978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4799 - binary_accuracy: 0.7968\n",
      "Epoch 23/50\n",
      "530/544 [============================>.] - ETA: 0s - loss: 0.4807 - binary_accuracy: 0.7933WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4801 - binary_accuracy: 0.7932\n",
      "Epoch 24/50\n",
      "533/544 [============================>.] - ETA: 0s - loss: 0.4806 - binary_accuracy: 0.7915WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4813 - binary_accuracy: 0.7908\n",
      "Epoch 25/50\n",
      "539/544 [============================>.] - ETA: 0s - loss: 0.4753 - binary_accuracy: 0.7959WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4749 - binary_accuracy: 0.7960\n",
      "Epoch 26/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.4769 - binary_accuracy: 0.7966WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4749 - binary_accuracy: 0.7977\n",
      "Epoch 27/50\n",
      "527/544 [============================>.] - ETA: 0s - loss: 0.4745 - binary_accuracy: 0.7986WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4749 - binary_accuracy: 0.7979\n",
      "Epoch 28/50\n",
      "532/544 [============================>.] - ETA: 0s - loss: 0.4725 - binary_accuracy: 0.7950WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4734 - binary_accuracy: 0.7942\n",
      "Epoch 29/50\n",
      "537/544 [============================>.] - ETA: 0s - loss: 0.4768 - binary_accuracy: 0.7941WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4764 - binary_accuracy: 0.7943\n",
      "Epoch 30/50\n",
      "523/544 [===========================>..] - ETA: 0s - loss: 0.4716 - binary_accuracy: 0.7970WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4730 - binary_accuracy: 0.7959\n",
      "Epoch 31/50\n",
      "533/544 [============================>.] - ETA: 0s - loss: 0.4708 - binary_accuracy: 0.7960WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4710 - binary_accuracy: 0.7957\n",
      "Epoch 32/50\n",
      "527/544 [============================>.] - ETA: 0s - loss: 0.4735 - binary_accuracy: 0.7949WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4738 - binary_accuracy: 0.7945\n",
      "Epoch 33/50\n",
      "533/544 [============================>.] - ETA: 0s - loss: 0.4695 - binary_accuracy: 0.7997WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4705 - binary_accuracy: 0.7986\n",
      "Epoch 34/50\n",
      "537/544 [============================>.] - ETA: 0s - loss: 0.4732 - binary_accuracy: 0.7918WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4740 - binary_accuracy: 0.7916\n",
      "Epoch 35/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.4716 - binary_accuracy: 0.7936WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4695 - binary_accuracy: 0.7949\n",
      "Epoch 36/50\n",
      "536/544 [============================>.] - ETA: 0s - loss: 0.4700 - binary_accuracy: 0.7948WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4698 - binary_accuracy: 0.7950\n",
      "Epoch 37/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.4677 - binary_accuracy: 0.7934WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4676 - binary_accuracy: 0.7936\n",
      "Epoch 38/50\n",
      "541/544 [============================>.] - ETA: 0s - loss: 0.4681 - binary_accuracy: 0.7988WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4681 - binary_accuracy: 0.7990\n",
      "Epoch 39/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.4722 - binary_accuracy: 0.7903WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4714 - binary_accuracy: 0.7904\n",
      "Epoch 40/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.4647 - binary_accuracy: 0.7972WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4654 - binary_accuracy: 0.7967\n",
      "Epoch 41/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.4661 - binary_accuracy: 0.7944WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4665 - binary_accuracy: 0.7945\n",
      "Epoch 42/50\n",
      "534/544 [============================>.] - ETA: 0s - loss: 0.4694 - binary_accuracy: 0.7903WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4699 - binary_accuracy: 0.7904\n",
      "Epoch 43/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4694 - binary_accuracy: 0.7937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4710 - binary_accuracy: 0.7927\n",
      "Epoch 44/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4690 - binary_accuracy: 0.7968WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4704 - binary_accuracy: 0.7960\n",
      "Epoch 45/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.4663 - binary_accuracy: 0.7971WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4671 - binary_accuracy: 0.7965\n",
      "Epoch 46/50\n",
      "534/544 [============================>.] - ETA: 0s - loss: 0.4659 - binary_accuracy: 0.7933WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4658 - binary_accuracy: 0.7934\n",
      "Epoch 47/50\n",
      "527/544 [============================>.] - ETA: 0s - loss: 0.4654 - binary_accuracy: 0.7951WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4646 - binary_accuracy: 0.7956\n",
      "Epoch 48/50\n",
      "534/544 [============================>.] - ETA: 0s - loss: 0.4674 - binary_accuracy: 0.7941WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4660 - binary_accuracy: 0.7948\n",
      "Epoch 49/50\n",
      "540/544 [============================>.] - ETA: 0s - loss: 0.4646 - binary_accuracy: 0.7964WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4646 - binary_accuracy: 0.7966\n",
      "Epoch 50/50\n",
      "538/544 [============================>.] - ETA: 0s - loss: 0.4627 - binary_accuracy: 0.7969WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4622 - binary_accuracy: 0.7975\n"
     ]
    }
   ],
   "source": [
    "## 실험 최고 모델\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(30, input_shape=(X_train.shape[1],), \n",
    "                       activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'\n",
    "                      ))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 6, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=16,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-e982bf3ba480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkVUlEQVR4nO3de3hddZ3v8fd371xLrm2SXpL0QpveKNCWTAsUtMKDFJ0DOHp8WnTE56jMHAd1vI0446in6hn1OeN4ZmRUdHh0VKyIivWIIsNNkFtT2gK9p6GXpC1Jk+bSNvd8zx97teyGtNltdrqTtT+v59lP9vqttZLvgvSzV9Zv/X7L3B0REQmvSKoLEBGR0aWgFxEJOQW9iEjIKehFREJOQS8iEnIZqS5gsJKSEp85c2aqyxARGVc2btx4xN1Lh1o35oJ+5syZ1NTUpLoMEZFxxcz2nWmdLt2IiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnKhCfr2rl6++V+72HKgNdWliIiMKaEJenf45n/tZsPellSXIiIypoQm6AtyMsjJjPBae1eqSxERGVMSCnozW2VmO82s1szuGmL9dDN73Mw2mdlLZva2oH2mmXWa2ebg9Z1kH0BcDZTl5/Bae/do/QgRkXFp2LluzCwK3A3cANQDG8xsvbtvi9vsc8D97v5tM1sIPATMDNbtcffFSa36DCYXZNPYoTN6EZF4iZzRLwNq3b3O3XuAdcAtg7ZxoCB4XwgcTF6JiSsryKFRZ/QiIqdJJOjLgQNxy/VBW7wvAu81s3piZ/MfiVs3K7ik86SZXTvUDzCzO8ysxsxqmpqaEq9+kLL8bBo7FPQiIvGS1Rm7BviBu1cAbwN+ZGYR4BAw3d2XAJ8A7jOzgsE7u/s97l7t7tWlpUNOp5yQyQU5HOvu41h333l/DxGRsEkk6BuAyrjliqAt3geA+wHc/VkgByhx9253bw7aNwJ7gLkjLfpMyvKzAWjUnTciIqckEvQbgCozm2VmWcBqYP2gbfYD1wOY2QJiQd9kZqVBZy5mdjFQBdQlq/jBJhfkAOjyjYhInGHvunH3PjO7E3gYiAL3uvtWM1sL1Lj7euCTwPfM7OPEOmbf7+5uZm8C1ppZLzAA/LW7j9qIpskFsTN63UsvIvK6hB4l6O4PEetkjW/7fNz7bcCKIfb7BfCLEdaYsNL82Bl9k87oRUROCc3IWNDoWBGRoYQq6DU6VkTkjUIV9KDRsSIig4Uu6DU6VkTkdOELeo2OFRE5TeiCXqNjRUROF7qg1+hYEZHThS7oNTpWROR0IQx6jY4VEYkXuqDX6FgRkdOFLug1OlZE5HShC3qNjhUROV3ogh40OlZEJF4og16jY0VEXhfOoNfoWBGRU0IZ9BodKyLyuoSC3sxWmdlOM6s1s7uGWD/dzB43s01m9pKZvS1u3WeD/Xaa2Y3JLP5MNDpWROR1wwZ98MzXu4GbgIXAGjNbOGizzwH3u/sSYs+U/fdg34XB8iXAKuDfTz5DdjRpdKyIyOsSOaNfBtS6e5279wDrgFsGbeNAQfC+EDgYvL8FWOfu3e7+KlAbfL9RpdGxIiKvSyToy4EDccv1QVu8LwLvNbN6Ys+W/cg57IuZ3WFmNWZW09TUlGDpZ6bRsSIir0tWZ+wa4AfuXgG8DfiRmSX8vd39Hnevdvfq0tLSERej0bEiIq/LSGCbBqAybrkiaIv3AWLX4HH3Z80sByhJcN+k0+hYEZHXJXLWvQGoMrNZZpZFrHN1/aBt9gPXA5jZAiAHaAq2W21m2WY2C6gCXkhW8Wej0bEiIjHDntG7e5+Z3Qk8DESBe919q5mtBWrcfT3wSeB7ZvZxYh2z73d3B7aa2f3ANqAP+Bt37x+tg4lXVpDD9oPtF+JHiYiMaYlcusHdHyLWyRrf9vm499uAFWfY9yvAV0ZQ43kpy8/mSXXGioiEc2QsaHSsiMhJoQ16jY4VEYkJbdBrdKyISEyIg16jY0VEIMRBr9GxIiIxoQ16jY4VEYkJbdBrdKyISExogx40OlZEBEIe9Hp2rIhI2IM+P1vX6EUk7YU66CcX5HC8p1+jY0UkrYU66DU6VkQk5EGv0bEiIqEPeo2OFREJddCfHB2rO29EJJ2FOuhPjo7VvfQiks4SCnozW2VmO82s1szuGmL9v5jZ5uC1y8xa49b1x60b/AjCUaXRsSIiCTxhysyiwN3ADUA9sMHM1gdPlQLA3T8et/1HgCVx36LT3RcnreJzpNGxIpLuEjmjXwbUunudu/cA64BbzrL9GuCnySguGTQ6VkTSXSJBXw4ciFuuD9rewMxmALOAx+Kac8ysxsyeM7Nbz7DfHcE2NU1NTYlVniCNjhWRdJfsztjVwAPu3h/XNsPdq4HbgG+a2ezBO7n7Pe5e7e7VpaWlSS1Io2NFJN0lEvQNQGXcckXQNpTVDLps4+4Nwdc64AlOv34/6jQ6VkTSXSJBvwGoMrNZZpZFLMzfcPeMmc0HioFn49qKzSw7eF8CrAC2Dd53NGl0rIiku2HvunH3PjO7E3gYiAL3uvtWM1sL1Lj7ydBfDaxzd4/bfQHwXTMbIPah8tX4u3UuBI2OFZF0N2zQA7j7Q8BDg9o+P2j5i0Ps9wxw6QjqGzGNjhWRdBfqkbGg0bEiIqEPejNjWlEu+5pPpLoUEZGUCH3QA1xWXsjmA62c3n0gIpIe0iLol0wvprGjm0NtunwjIuknLYJ+cWURAJsPtKa0DhGRVEiLoF8wtYCsjIiCXkTSUloEfVZGhEumFbBp/9FUlyIicsGlRdADLKks5uWGNnr7B1JdiojIBZU2Qb94ehFdvQPsPNyR6lJERC6otAn6JUGH7CZdpxeRNJM2QV9RnEtJXhab97emuhQRkQsqbYLezFhcWcTmA+qQFZH0kjZBD7H76fc0HaetszfVpYiIXDBpFvTFAGzRdXoRSSNpFfSXVRZiphGyIpJe0iroC3IymVOap6AXkbSSUNCb2Soz22lmtWZ21xDr/8XMNgevXWbWGrfudjPbHbxuT2Lt52VxZRGb9h/VTJYikjaGDXoziwJ3AzcBC4E1ZrYwfht3/7i7L3b3xcC/Ab8M9p0IfAFYDiwDvmBmxUk9gnO0eHoRR0/0sr9F89OLSHpI5Ix+GVDr7nXu3gOsA245y/ZrgJ8G728EHnH3Fnc/CjwCrBpJwSO1JOiQ1eUbEUkXiQR9OXAgbrk+aHsDM5sBzAIeO5d9zewOM6sxs5qmpqZE6j5vcyfnkZsZZZMGTolImkh2Z+xq4AF37z+Xndz9Hnevdvfq0tLSJJd0uoxohEsrCjUVgoikjUSCvgGojFuuCNqGsprXL9uc674XzJLKIrYfbKe775w+j0RExqVEgn4DUGVms8wsi1iYrx+8kZnNB4qBZ+OaHwbeambFQSfsW4O2lFoyvYie/gG2HWxPdSkiIqNu2KB39z7gTmIBvR243923mtlaM7s5btPVwDqPu2/R3VuALxH7sNgArA3aUurkCFldpxeRdJCRyEbu/hDw0KC2zw9a/uIZ9r0XuPc86xsVUwpzmFKQoztvRCQtpNXI2HixmSxbU12GiMioS9ugXzK9iP0tJ2g+1p3qUkRERlXaBv3i4IlTOqsXkbBL26C/tKKQaMQU9CISemkb9BOyMpg3OV933ohI6KVt0ANUzyxm476jdPVq4JSIhFdaB/2b55bS2dtPzV49R1ZEwiutg/6q2ZPIikZ4YmdjqksRERk1aR30E7IyWH7xRJ7YNbozZoqIpFJaBz3ELt/UNh6j/qgeRCIi4ZT2Qb9yXmxa5Cd1Vi8iIZX2QT+7NI/yolye2KmgF5FwSvugNzPePK+UZ2qP0NM3kOpyRESSLu2DHmDl3FKO9/RTsy/lMyiLiCSdgh64ek4JmVHjSV2+EZEQSijozWyVme00s1ozu+sM27zbzLaZ2VYzuy+uvd/MNgevNzyZaizIy86gesZEdciKSCgN++ARM4sCdwM3APXABjNb7+7b4rapAj4LrHD3o2ZWFvctOt19cXLLTr6V80r5p9/t4FBbJ1MLc1NdjohI0iRyRr8MqHX3OnfvAdYBtwza5kPA3e5+FMDdx91Q05XzYp9NunwjImGTSNCXAwfiluuDtnhzgblm9icze87MVsWtyzGzmqD91pGVO3rmTs5jSkGOLt+ISOgk9MzYBL9PFbASqAD+aGaXunsrMMPdG8zsYuAxM3vZ3ffE72xmdwB3AEyfPj1JJZ0bM2PlvFJ++9IhevsHyIyqn1pEwiGRNGsAKuOWK4K2ePXAenfvdfdXgV3Egh93bwi+1gFPAEsG/wB3v8fdq929urS09JwPIllWziulo7uPF/dpNksRCY9Egn4DUGVms8wsC1gNDL575kFiZ/OYWQmxSzl1ZlZsZtlx7SuAbYxRV88pISNiunwjIqEybNC7ex9wJ/AwsB243923mtlaM7s52OxhoNnMtgGPA59292ZgAVBjZluC9q/G360z1hTkZLJ0RrGmQxCRUEnoGr27PwQ8NKjt83HvHfhE8Irf5hng0pGXeeGsnFfK13+/k8b2LsoKclJdjojIiKnHcZCVc4PbLHX5RkRCQkE/yIKp+ZTlZ+thJCISGgr6QcyMN88t5aldTfT2azZLERn/FPRDuOnSKbR39fGHra+luhQRkRFT0A/hzXPLqJyYyw+f3ZvqUkRERkxBP4RoxPjLK2fwwqstbD/UnupyRERGREF/Bu+uriQ7I8J/Prsv1aWIiIyIgv4MiiZkcevich7c1EDbid5UlyMict4U9Gfxl1fNoLO3n59vPDD8xiIiY5SC/iwWlRdSPaOYHz+3j4EBT3U5IiLnRUE/jPddPZO9zSf4424NoBKR8UlBP4xVl0yhND9bnbIiMm4p6IeRlRHhtmXTeXxnI/uaj6e6HBGRc6agT8Bty6cTNePHz+msXkTGHwV9AiYX5LBq0RR+tuEAnT39qS5HROScKOgTdPvVM2nv6uPXmwc/RVFEZGxLKOjNbJWZ7TSzWjO76wzbvNvMtpnZVjO7L679djPbHbxuT1bhF1r1jGIWTC3gh8/uI/acFRGR8WHYoDezKHA3cBOwEFhjZgsHbVMFfBZY4e6XAH8btE8EvgAsB5YBXzCz4mQewIViZtx+1Qy2H2qnRg8PF5FxJJEz+mVArbvXuXsPsA64ZdA2HwLudvejAO7eGLTfCDzi7i3BukeAVckp/cK7efE08rMz+OkL+1NdiohIwhIJ+nIgfg6A+qAt3lxgrpn9ycyeM7NV57AvZnaHmdWYWU1T09gdmDQhK4ObF0/jty8d0vw3IjJuJKszNgOoAlYCa4DvmVlRoju7+z3uXu3u1aWlpUkqaXSsWTad7r4BHlSnrIiME4kEfQNQGbdcEbTFqwfWu3uvu78K7CIW/InsO64sKi/k0vJCfvrCfnXKisi4kEjQbwCqzGyWmWUBq4H1g7Z5kNjZPGZWQuxSTh3wMPBWMysOOmHfGrSNa6uXVbLjcAebD7SmuhQRkWENG/Tu3gfcSSygtwP3u/tWM1trZjcHmz0MNJvZNuBx4NPu3uzuLcCXiH1YbADWBm3j2s2XT2NCVpR1L2j6YhEZ+2ysXX6orq72mpqaVJcxrM888BLrtxzkhX+4nvyczFSXIyJpzsw2unv1UOs0MvY8rVk+nc7eftZvOZjqUkREzkpBf54uryhk/pR83VMvImOegv48mRlrlk3nlYZ2XmloS3U5IiJnpKAfgVuXlJOdEdFZvYiMaQr6ESjMzeTtl03l15sPcry7L9XliIgMSUE/Qrctm86x7j5++9KhVJciIjIkBf0IXTGjmDlledynyzciMkYp6EfIzFj9Z5VsPtDK9kPtqS5HROQNFPRJ8M6lFeRkRvjXR3enuhQRkTdQ0CdB8UVZfOS6Kn73ymEe39E4/A4iIheQgj5JPnTtxcwpy+Pz61/RA8RFZExR0CdJVkaEL9+6iAMtnXzrcV3CEZGxQ0GfRFdePIl3Lq3gnj/Wsfu1jlSXIyICKOiT7u/fNp8JWRn8w4Ov6MEkIjImKOiTbFJeNp+9aT4vvNrCL14c1w/TEpGQUNCPgndXV1I9o5j//dB2jh7vSXU5IpLmEgp6M1tlZjvNrNbM7hpi/fvNrMnMNgevD8at649rH/wIwlCKRIwvv2MR7Z29fO33O1JdjoikuYzhNjCzKHA3cAOxh4BvMLP17r5t0KY/c/c7h/gWne6+eMSVjjPzpxTwgWtn8d0n63jXFRVUz5yY6pJEJE0lcka/DKh19zp37wHWAbeMblnh8LHrqygvyuVj6zZzuK0r1eWISJpKJOjLgfinYNcHbYO908xeMrMHzKwyrj3HzGrM7Dkzu3WoH2BmdwTb1DQ1NSVc/Fg3ISuD7/7lFbR19nL7vS/Q1tmb6pJEJA0lqzP2N8BMd78MeAT4Ydy6GcEDa28Dvmlmswfv7O73uHu1u1eXlpYmqaSxYVF5Id957xXUHTnGh/6zhq5ejZoVkQsrkaBvAOLP0CuCtlPcvdndu4PF7wNXxK1rCL7WAU8AS0ZQ77h0TVUJ//zuxbzwagt/u24z/QO6v15ELpxEgn4DUGVms8wsC1gNnHb3jJlNjVu8GdgetBebWXbwvgRYAQzuxE0LN18+jX/884X8futhvrh+qwZTicgFM+xdN+7eZ2Z3Ag8DUeBed99qZmuBGndfD3zUzG4G+oAW4P3B7guA75rZALEPla8OcbdO2vjANbNo7Ojiu0/WUZafzUeur0p1SSKSBmysnVlWV1d7TU1NqssYNQMDzqd+voVfbmrgq39xKauXTU91SSISAma2MegPfYNhz+gluSIR42vvuowjx3v43IOvMKcsT/fYi8io0hQIKZAZjfBva5ZQUZzLh3/yIo0dusdeREaPgj5FCnMz+fZ7r6C9q5c779tEb/9AqksSkZBS0KfQgqkF/NNfXMoLr7bwdc2JIyKjREGfYu9YUsH7rprB9556lYdePpTqckQkhBT0Y8Dn3r6QJdOL+PTPt1DbeCzV5YhIyCjox4CsjAj//p6l5GRG+esfb+R4d1+qSxKREFHQjxFTC3P5tzVLqGs6xqd+voX2Lk2AJiLJoaAfQ66eU8JdN83nd68cZvlXHuXvHtjClgOtmi5BREZEA6bGmDveNJsrL57Efc/v59ebD3J/TT2XTCvgPctncPPiaeRl63+ZiJwbTYEwhrV39fLrTQ385Pn97DjcQV52Bv/w9gWs/rNKzCzV5YnIGHK2KRAU9OOAu7PpQCvf+MMunq49wl8sKefL71jEhCyd3YtIzNmCXtfoxwEzY+n0Yn74P5bxiRvm8qvNDdzyrT+x+7WOVJcmIuOAgn4ciUaMj15fxY8/sJyjJ3q4+Vt/4leb6lNdloiMcQr6cWjFnBJ++9FrubSikI//bAuf/eXLekShiJyRgn6cmlyQw30fXM7/XDmbn76wn2u+9hj/+uhuWo73pLo0ERljEgp6M1tlZjvNrNbM7hpi/fvNrMnMNgevD8atu93Mdgev25NZfLrLiEb4zKr53P9XV3FpeSHfeGQXV3/1Uf7xwVfYe+R4qssTkTFi2LtuzCwK7AJuAOqJPUN2TfwjAc3s/UC1u985aN+JQA1QDTiwEbjC3Y+e6efprpvzt+u1Dr7/VB0PbjpI78AANy6cwgevncUVM4p1O6ZIyI30rptlQK2717l7D7AOuCXBn30j8Ii7twTh/giwKsF95RzNnZzP1991OU9/5i18eOVsnq1r5l3feZb/9q2neWBjva7ji6SpRIK+HDgQt1wftA32TjN7ycweMLPKc9nXzO4wsxozq2lqakqwdDmTsoIcPn3jfJ797HV85R2L6O4d4FM/38LVX32M//PwTg636YlWIukkWZ2xvwFmuvtlxM7af3guO7v7Pe5e7e7VpaWlSSpJJmRl8J7lM/jDx9/ETz64nKXTi7n7iVpWfO0x/upHNfy85gBNHd2pLlNERlkiQysbgMq45Yqg7RR3b45b/D7w9bh9Vw7a94lzLVJGxsxYMaeEFXNK2N98gh89t5f1Ww7y8NbXMIPLKoq4bl4Z180v45JpBUQiup4vEiaJdMZmEOuMvZ5YcG8AbnP3rXHbTHX3Q8H7dwCfcfcrg87YjcDSYNMXiXXGtpzp56kz9sJwd7YebOfxHY08uqORLfWtuENJXhaLK4u4tLyIyyoKubSikJK87FSXKyLDOFtn7LBn9O7eZ2Z3Ag8DUeBed99qZmuBGndfD3zUzG4G+oAW4P3Bvi1m9iViHw4Aa88W8nLhmBmLygtZVF7IR66v4sixbp7c2cTTtUd4qb6VR3c0cvIcYFphDpdVFHHrkmncsHAKUZ3xi4wrmtRMhtTR1cvWg+28XN/Gyw1tbNjbwqG2LmaVXMQHrpnFu66oICczOuS+3X39bN7fyoGjnXT39dPTN0BP3wDdwdeiCZmsWTadizTlskjSaPZKGbG+/gF+v/Uw9/yxjpfq25h4URbvu2oG77tqJhOyomw+0Mpzdc08V9fMpv2tdPcNDPl9ohGjf8Apzc/m0zfO411LK9QnIJIECnpJGnfn+Vdb+N4f63h0RyPZGREc6OkbwAwumVbAlbMmceXFk5g7OZ/szAjZGRGyMiJkRSNkRCO8uP8oa3+zjc0HWllUXsA/vn0hyy+elOpDExnXFPQyKna/1sFPnt9PRsS4avYkqmdOpDA3M6F93Z31Ww7ytd/t4GBbFzctmsKnb5xHZjTCwdZODrV1cbCtk4OtnRxu66a7rx936B9wBtxxhwF3SvKyWTC1gAVT81kwtYCK4tyERgG7O8e6+2js6KaxvZvj3X2smFNCbtbQl6NExjoFvYxZnT39fO+pOr79xB46hxi5W5ibydTCHHIyo0QjRsQgYkbEDDM41NbF3ubjpzqO83MyWDClgGlFOfQNOP0DTt+A09c/QN+A09XbfyrcB/+8krxs/uYts1mzbPoZ+x9ExioFvYx5h9u6WL+lIQj2XKYV5TC1MDehDtvj3X3sONzB9kPtp15Nx7rJjETIiBrRSITMqBGNGNkZEUrzcyjLz469CrIpy8+ht3+Abz+xh+dfbWFqYQ4fua6K/15dQWZUE7zK+KCgF0mAu/PMnmb++Q87eXF/K5UTc/nodVUsnVE8xLan3uEem7HvZFtuZpQphTlkZQz9ITEw4Gw/3M4Lr7bwfF0LWw+1MbcsnzfNLeXaqhJmlVykSejknCnoRc6Bu/PEria+8YddvNzQdl7fwwymFORQXpRLRXEuFcUTyM2K8uK+o2zY20J7Vx8AFcW5LJpWyPbD7exrPgFAeVEub5pbwrVVpSyuLGJqYc5Zg/9ETx+bD7Syce9Rjp7oZfnFE7lq9iQKcs7cX/LqkeM8vqORmn0tLCov5JbF5ZQX5Z7XscrYoKAXOQ/uzrN7mmk6NvR8QCfD14gFuxHrNzje3UdDayf1RzupP3qC+qOxzuX+AefikotYfvFEls2ayLJZk04L133Nx3lq9xH+uKuJZ/c009Ed+zC4KCvKnLI85pTlM6csj6qyPDp7+9m47ygb9x1l26F2+gdi/46zMyJ09w0QjRiXVxRybVXsr4R5U/Kp2XuUJ3Y28sSuplMfKlMKcjjcHpvk7sqLJ/KOJeXcdOnUs35IJENDayeb9h/l8ooiKidOSPr37+rtp+V4D60nesnKMHKzMpiQGSU3K0p2RiSUfzEp6EVSrK9/gBO9/QkHaG//AC/Vt7LtUAd7Go+xu7GD2sZjvNb++odObmaUxZVFVM8sZumMYpZWFpObFWXT/qM8tfsIT9Ue4eX6Vgbi/onnZEa4enYJK+eVsnJuGdMnTWB/8wke3NzArzY18OqR42RlRLhhwWRml+XR2z9Ab98Avf0D9PQ7vf0D5GVnMH9KPvOnFjB3ch4TsobvR2k90cOze5r5054j/Km2mVeDB+NEI8Yti6fx4ZWzmVOWf07/TfsHnFca2ni69gibD7Ry5Fg3zcd6aDnew7HgQ3IoEYOLsjKYNyWfa6pKuGZOCZdXFo37/hgFvUhItHX2sqfpGFnRCPOn5JMxTDi1nejlmT1H2HG4g6Uzilk+a+IZ7yhyd7bUt/HgpgZ+s+Ugzcd7yIwamdHIqVdW1Gjt7OVET+yOJTOYOeki5k/Jp3LiBHr6Bujs6aezN3j19NN8vIcdh9txj/11svziSayYU8LiykIeevkw9z2/n66+fm5cOIUPv2U2l1UUnfF4DrSc4OnaIzy1u4ln9jTTeqIXgDlleUwpyGFSXhYTL8qiJC+biRdlUZSbSe+A09nTx4mefk70xGrq6Opl04FWXm5oO1XXlUFdl1UUMqUwh7L8M/ezjEUKehE5JydzYahLHAMDzoGjJ9h+qIOdhzvYcbidHYc7aGjtJCcjwoSsDHKzouRkRsnNjJCXk8kV04tZMWfSkGfOzce6+cEze/nBM3vp6Orj2qoSlk4vPnWGfuRYd/B6/Ux9SkEO11SVcG1VbFbW85147+RfGk/XHuFPtUfYG1zSOqkkL5uphTlMLshhSmE2pXk5lOZnn/bKy86gsb2Lg21dHGztDF5dNHZ0UTwhi2lFuZQX5TCtKPfUK9HxJudCQS8iY15HVy8/fm4///F0HUeO9VA8IZOSvGwm5cXO0EvyspkxaQLXVpUwuzRvVK6z1x89QW3jMQ63dXG4vesNX0/+BXE2EYPJBbFbeFs7eznU2kVP/+lTgpTkZXPJtAIumVbAwmkFXDKtkBkTJ4xoOhAFvYiMGydHP4/Fa+Y9fQM0H++mqeP1V0dXH2UF2afO1ifnZ592SW1gwDlyvJuDrbEz/vqjJ9h5+BhbD7ZR23iMvqATJS87g5XzSvnWbUvP9OPPakTTFIuIXEjRiBFlbN4Vk5URYWphLlMLE78VNRIxyvJj1/wXVxadtq67r5/dr8VCf+vBdvJzRieSFfQiIimSnRE99VyI0TT2/jYSEZGkSijozWyVme00s1ozu+ss273TzNzMqoPlmWbWaWabg9d3klW4iIgkZthLN2YWBe4GbgDqgQ1mtt7dtw3aLh/4GPD8oG+xx90XJ6dcERE5V4mc0S8Dat29zt17gHXALUNs9yXga0BXEusTEZERSiToy4EDccv1QdspZrYUqHT33w6x/ywz22RmT5rZtUP9ADO7w8xqzKymqakp0dpFRCQBI+6MNbMI8A3gk0OsPgRMd/clwCeA+8ysYPBG7n6Pu1e7e3VpaelISxIRkTiJBH0DUBm3XBG0nZQPLAKeMLO9wJXAejOrdvdud28GcPeNwB5gbjIKFxGRxCQS9BuAKjObZWZZwGpg/cmV7t7m7iXuPtPdZwLPATe7e42ZlQaduZjZxUAVUJf0oxARkTMa9q4bd+8zszuBh4EocK+7bzWztUCNu68/y+5vAtaaWS8wAPy1u7ec7edt3LjxiJntS/wQ3qAEODKC/ccrHXd60XGnl0SOe8aZVoy5uW5GysxqzjTfQ5jpuNOLjju9jPS4NTJWRCTkFPQiIiEXxqC/J9UFpIiOO73ouNPLiI47dNfoRUTkdGE8oxcRkTgKehGRkAtN0Cc6lXIYmNm9ZtZoZq/EtU00s0fMbHfwtTiVNSabmVWa2eNmts3MtprZx4L2sB93jpm9YGZbguP+X0H7LDN7Pvh9/1kwmDF0zCwazJX1/4LldDnuvWb2cjC9e03Qdt6/66EI+riplG8CFgJrzGxhaqsaVT8AVg1quwt41N2rgEeD5TDpAz7p7guJTbPxN8H/47AfdzdwnbtfDiwGVpnZlcRmiv0Xd58DHAU+kLoSR9XHgO1xy+ly3ABvcffFcffPn/fveiiCnsSnUg4Fd/8jMHiE8S3AD4P3PwRuvZA1jTZ3P+TuLwbvO4j94y8n/Mft7n4sWMwMXg5cBzwQtIfuuAHMrAJ4O/D9YNlIg+M+i/P+XQ9L0A87lXIamOzuh4L3h4HJqSxmNJnZTGAJsYfchP64g8sXm4FG4BFikwO2untfsElYf9+/CfwdselTACaRHscNsQ/zP5jZRjO7I2g77991PRw8hNzdzSyU982aWR7wC+Bv3b09dpIXE9bjdvd+YLGZFQG/AuantqLRZ2Z/DjS6+0YzW5niclLhGndvMLMy4BEz2xG/8lx/18NyRj/cVMrp4DUzmwoQfG1McT1JZ2aZxEL+J+7+y6A59Md9kru3Ao8DVwFFZnbyRC2Mv+8rgJuDqc/XEbtk838J/3ED4O4NwddGYh/uyxjB73pYgv6sUymnifXA7cH724Ffp7CWpAuuz/4HsN3dvxG3KuzHXRqcyWNmucSe3bydWOC/K9gsdMft7p9194pg6vPVwGPu/h5CftwAZnZR8AxuzOwi4K3AK4zgdz00I2PN7G3ErumdnEr5K6mtaPSY2U+BlcSmLn0N+ALwIHA/MB3YB7x7uCmhxxMzuwZ4CniZ16/Z/j2x6/RhPu7LiHW8RYmdmN3v7muD5zusAyYCm4D3unt36iodPcGlm0+5+5+nw3EHx/irYDEDuM/dv2JmkzjP3/XQBL2IiAwtLJduRETkDBT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQ+/8UvsFrUpFf0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5408057 ],\n",
       "       [0.03620268],\n",
       "       [0.96230537],\n",
       "       ...,\n",
       "       [0.8691489 ],\n",
       "       [0.76534176],\n",
       "       [0.66572136]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test 데이터에 대한 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과 출력\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\sample_submission.csv\")\n",
    "preds = [p[0] >= 0.5 for p in y_pred]\n",
    "submission['Transported'] = preds\n",
    "submission.to_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\second_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "109/109 [==============================] - 3s 12ms/step - loss: nan - binary_accuracy: 0.5144 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 2/100\n",
      "109/109 [==============================] - 1s 12ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 3/100\n",
      "109/109 [==============================] - 2s 14ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 4/100\n",
      "109/109 [==============================] - 2s 14ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 5/100\n",
      "101/109 [==========================>...] - ETA: 0s - loss: nan - binary_accuracy: 0.4938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-128356bc7a1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, \n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1443\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1445\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1446\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1757\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(32, kernel_size=5, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 8, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], 1), y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼 파라미터 k,d : ( 10 10 )\n",
      "정확도 :  0.7728253055355859\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 15 )\n",
      "정확도 :  0.7739755571531273\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 20 )\n",
      "정확도 :  0.7725377426312006\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 25 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 30 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 35 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 40 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 45 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 50 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 10 )\n",
      "정확도 :  0.7787203450754853\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 15 )\n",
      "정확도 :  0.7800143781452192\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 20 )\n",
      "정확도 :  0.784040258806614\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 25 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 30 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 35 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 40 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 45 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 50 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 10 )\n",
      "정확도 :  0.779726815240834\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 20 )\n",
      "정확도 :  0.7807332854061826\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 25 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 30 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 35 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 40 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 45 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 50 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 10 )\n",
      "정확도 :  0.7846153846153846\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 15 )\n",
      "정확도 :  0.7864845434938893\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 20 )\n",
      "정확도 :  0.7851905104241553\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 25 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 30 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 35 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 40 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 45 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 50 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 10 )\n",
      "정확도 :  0.7838964773544213\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 15 )\n",
      "정확도 :  0.7889288281811646\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 20 )\n",
      "정확도 :  0.7869158878504673\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 25 )\n",
      "정확도 :  0.785765636232926\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 30 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 35 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 40 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 45 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 50 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 10 )\n",
      "정확도 :  0.7851905104241553\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 15 )\n",
      "정확도 :  0.7866283249460819\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 20 )\n",
      "정확도 :  0.7866283249460819\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 25 )\n",
      "정확도 :  0.7860531991373113\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 30 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 35 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 40 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 45 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 50 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 10 )\n",
      "정확도 :  0.7841840402588066\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 15 )\n",
      "정확도 :  0.7867721063982747\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 20 )\n",
      "정확도 :  0.7874910136592379\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 25 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 30 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 35 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 40 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 45 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 50 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 10 )\n",
      "정확도 :  0.7859094176851186\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 20 )\n",
      "정확도 :  0.787922358015816\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 25 )\n",
      "정확도 :  0.78490294751977\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 30 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 35 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 40 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 45 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 50 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 10 )\n",
      "정확도 :  0.78490294751977\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 15 )\n",
      "정확도 :  0.7828900071890726\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 20 )\n",
      "정확도 :  0.7869158878504673\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 25 )\n",
      "정확도 :  0.784040258806614\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 30 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 35 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 40 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 45 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 50 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 10 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 20 )\n",
      "정확도 :  0.7876347951114306\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 25 )\n",
      "정확도 :  0.7850467289719626\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 30 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 35 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 40 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 45 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 50 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수\n",
    "\n",
    "\n",
    "for n in range(10,101,10):\n",
    "    for j in range(10,51,5):\n",
    "        clf = RandomForestClassifier(n_estimators=n, max_depth=j,random_state=0)\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        predict1 = clf.predict(X_val)\n",
    "        print('하이퍼 파라미터 k,d : (',n,j,')')\n",
    "        print('정확도 : ',accuracy_score(y_val,predict1))\n",
    "        print('=======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost(Extreme Gradient Boosting)는 GBM(Gradient Boosting Machine) 기반의 앙상블 학습 알고리즘 중 하나로, 다양한 데이터 분류 및 회귀 문제에 대해 높은 성능을 보이고 있습니다.\n",
    "\n",
    "XGBoost의 주요 특징은 다음과 같습니다.\n",
    "\n",
    "* Regularization: L1, L2 regularization 및 max depth 등을 통해 overfitting 방지\n",
    "* Tree Pruning: 불필요한 가지 제거를 통한 효율적인 모델 생성\n",
    "* Handling Missing Values: 결측치 처리 기능 제공\n",
    "* Cross-Validation: 교차 검증 기능 제공\n",
    "* Parallel Processing: 멀티코어 CPU를 이용한 병렬처리\n",
    "\n",
    "하이퍼파라미터 세팅은 다음과 같은 것들이 있습니다.\n",
    "\n",
    "* n_estimators: 생성할 tree의 개수\n",
    "* max_depth: 트리의 깊이\n",
    "* learning_rate: 각 트리마다 가중치를 얼마나 적용할지를 결정하는 파라미터\n",
    "* gamma: loss function의 값이 얼마나 감소해야 leaf node를 생성할 것인지 결정하는 파라미터\n",
    "* subsample: 데이터를 샘플링하는 비율\n",
    "* colsample_bytree: 각 tree 생성 시, feature를 샘플링하는 비율\n",
    "* reg_alpha: L1 regularization 계수\n",
    "* reg_lambda: L2 regularization 계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.22.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78058950395399"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 모델 선언 예시\n",
    "model = XGBClassifier(n_estimators=1000, learning_rate=0.02, max_depth=25, random_state = 32)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val) \n",
    "\n",
    "# 예측 라벨과 실제 라벨 사이의 정확도 측정\n",
    "accuracy_score(y_pred, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1738, 15)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.88369\tval-auc:0.87393\n",
      "[50]\ttrain-auc:0.91092\tval-auc:0.88745\n",
      "[100]\ttrain-auc:0.91695\tval-auc:0.89137\n",
      "[150]\ttrain-auc:0.92092\tval-auc:0.89274\n",
      "[200]\ttrain-auc:0.92506\tval-auc:0.89451\n",
      "[250]\ttrain-auc:0.92930\tval-auc:0.89665\n",
      "[300]\ttrain-auc:0.93314\tval-auc:0.89799\n",
      "[350]\ttrain-auc:0.93642\tval-auc:0.89894\n",
      "[400]\ttrain-auc:0.93924\tval-auc:0.89989\n",
      "[450]\ttrain-auc:0.94169\tval-auc:0.90064\n",
      "[500]\ttrain-auc:0.94382\tval-auc:0.90109\n",
      "[550]\ttrain-auc:0.94619\tval-auc:0.90154\n",
      "[600]\ttrain-auc:0.94820\tval-auc:0.90189\n",
      "[650]\ttrain-auc:0.95028\tval-auc:0.90210\n",
      "[700]\ttrain-auc:0.95218\tval-auc:0.90238\n",
      "[750]\ttrain-auc:0.95398\tval-auc:0.90272\n",
      "[800]\ttrain-auc:0.95588\tval-auc:0.90312\n",
      "[850]\ttrain-auc:0.95758\tval-auc:0.90331\n",
      "[900]\ttrain-auc:0.95911\tval-auc:0.90357\n",
      "[950]\ttrain-auc:0.96098\tval-auc:0.90400\n",
      "[1000]\ttrain-auc:0.96279\tval-auc:0.90424\n",
      "[1050]\ttrain-auc:0.96433\tval-auc:0.90445\n",
      "[1100]\ttrain-auc:0.96592\tval-auc:0.90477\n",
      "[1150]\ttrain-auc:0.96734\tval-auc:0.90491\n",
      "[1200]\ttrain-auc:0.96874\tval-auc:0.90487\n",
      "[1250]\ttrain-auc:0.97006\tval-auc:0.90500\n",
      "[1300]\ttrain-auc:0.97133\tval-auc:0.90491\n",
      "[1350]\ttrain-auc:0.97271\tval-auc:0.90512\n",
      "[1400]\ttrain-auc:0.97397\tval-auc:0.90539\n",
      "[1450]\ttrain-auc:0.97505\tval-auc:0.90540\n",
      "[1500]\ttrain-auc:0.97610\tval-auc:0.90553\n",
      "[1550]\ttrain-auc:0.97715\tval-auc:0.90545\n",
      "[1600]\ttrain-auc:0.97819\tval-auc:0.90558\n",
      "[1650]\ttrain-auc:0.97915\tval-auc:0.90539\n",
      "[1700]\ttrain-auc:0.98010\tval-auc:0.90542\n",
      "[1750]\ttrain-auc:0.98105\tval-auc:0.90543\n",
      "[1800]\ttrain-auc:0.98184\tval-auc:0.90538\n",
      "[1850]\ttrain-auc:0.98269\tval-auc:0.90535\n",
      "[1900]\ttrain-auc:0.98343\tval-auc:0.90523\n",
      "[1950]\ttrain-auc:0.98403\tval-auc:0.90521\n",
      "[1999]\ttrain-auc:0.98465\tval-auc:0.90532\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# X_train, y_train, X_val, y_val 데이터가 있다고 가정합니다.\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'early_stopping_rounds': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 모델 예측\n",
    "y_pred = model.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 출력\n",
    "y_preds= [p >= 0.5 for p in y_pred]\n",
    "\n",
    "submission = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\sample_submission.csv\")\n",
    "\n",
    "submission['Transported'] = y_preds\n",
    "submission.to_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\xgboost2_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017215164"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6955"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb.plot_importance(model, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# 여기서 (f+'숫자') 조합은, 원래 df의 column의 index와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "important_column_index = [int(f[1:]) for f, _ in importance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 중요 변수 :  CryoSleep\n",
      "1 번째 중요 변수 :  HomePlanet\n",
      "2 번째 중요 변수 :  spend_fillna_0\n",
      "3 번째 중요 변수 :  deck\n",
      "4 번째 중요 변수 :  Spa\n",
      "5 번째 중요 변수 :  FoodCourt\n",
      "6 번째 중요 변수 :  side\n",
      "7 번째 중요 변수 :  VRDeck\n",
      "8 번째 중요 변수 :  ShoppingMall\n",
      "9 번째 중요 변수 :  RoomService\n",
      "10 번째 중요 변수 :  Destination\n",
      "11 번째 중요 변수 :  num\n",
      "12 번째 중요 변수 :  group\n",
      "13 번째 중요 변수 :  last_name\n",
      "14 번째 중요 변수 :  VIP_fill_na_False\n"
     ]
    }
   ],
   "source": [
    "for i,ind in enumerate(important_column_index) :\n",
    "    print(i,'번째 중요 변수 : ',df.columns[ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 왜 내가 제일 중요하다 생각했던 group이랑 VIP 가 중요도가 이렇게 낮지?\n",
    "\n",
    "My Q) \n",
    "\n",
    "위는 XGboost 방식으로 계산한 변수 중요도야. 그런데 조금 궁금한게, VIP_fill_na_False 변수가 True일 경우, 사례는 적지만  target이 true인 경향이 60%로 매우 높았고, VIP_fill_na_False 변수가 False일 경우, 사례는 매우 많지만 target이 true인 경향이 49%로, VIP_fill_na 변수가 타겟변수와 유의미한 상관관계를 가질 것으로 예측했고, 실제 corr로 계산한 피어슨 상관계수도 타 변수에 비해 어느정도 높았는데 왜 예측에서 쓰이지 않은걸까? 네가 생각하는 이유를 설명해줘\n",
    "\n",
    "<br>\n",
    "\n",
    "Chat gpt A) \n",
    "\n",
    "VIP_fill_na_False 변수가 타겟 변수와 상관관계가 높다고 해서 반드시 모델에 반영되는 것은 아닙니다. 변수 중요도는 해당 변수가 모델 예측에 얼마나 중요한 역할을 하는지를 나타내는 지표일 뿐, 실제로 모델에서 사용되는 변수와 중요도가 높은 변수가 일치하지 않을 수도 있습니다.\n",
    "\n",
    "또한, VIP_fill_na_False 변수가 True일 경우 target이 true인 경향이 높다고 하더라도, 이 변수 자체가 다른 변수와 중복되거나 정보의 양이 적어서 모델에서 더이상 유의미한 변수가 아닐 수도 있습니다. 따라서 변수 중요도만으로는 모델 예측 결과를 충분히 설명할 수 없습니다.\n",
    "\n",
    "만약 VIP_fill_na_False 변수가 모델 예측에 미치는 영향을 확인하고 싶다면, 이 변수를 제외한 다른 변수들과 함께 모델을 학습하고 변수 중요도를 계산해보는 것이 좋습니다. 이를 통해 VIP_fill_na_False 변수의 유의미성을 다시한번 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
