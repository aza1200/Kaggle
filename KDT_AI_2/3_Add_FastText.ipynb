{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.9\n",
    "# !pip install torchtext==0.10.1\n",
    "# Libraries\n",
    "from konlpy.tag import Okt  \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from gensim.models import FastText\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "# Preliminaries\n",
    "from torchtext.legacy.data import TabularDataset ,Field, LabelField ,BucketIterator, Example, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Models\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import ConcatDataset\n",
    "from konlpy.tag import Okt  \n",
    "# Training\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import logging\n",
    "from gensim.models import FastText\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"./middle/after_tokenized_train.xlsx\")[[\"tokenized_text\",\"label\"]]\n",
    "test_data = pd.read_excel(\"./middle/after_tokenized_test.xlsx\")[[\"tokenized_text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt=Okt()  \n",
    "with open('./middle/all_speech_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_morphs = okt.morphs(text)\n",
    "counted_morphs = Counter(all_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65863/65863 [00:02<00:00, 24812.15it/s]\n",
      "100%|██████████| 13491/13491 [00:00<00:00, 29979.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clean_train_text = [ eval(text) for text in tqdm(train_data['tokenized_text'])]\n",
    "clean_test_text = [ eval(text) for text in tqdm(test_data['tokenized_text'])]\n",
    "clean_full_text = clean_train_text + clean_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_model = FastText(vector_size=100, window=3, min_count=1, sentences=clean_full_text, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이명숙', 0.9372692108154297),\n",
       " ('이명박정부', 0.9092457890510559),\n",
       " ('이명희', 0.8945683836936951),\n",
       " ('명박', 0.8714800477027893),\n",
       " ('이명', 0.8540940880775452),\n",
       " ('배근혜', 0.8535810112953186),\n",
       " ('닥근혜', 0.851435661315918),\n",
       " ('근혜', 0.840703547000885),\n",
       " ('문근혜', 0.840450644493103),\n",
       " ('최순실', 0.8337816596031189)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_model.wv.most_similar(\"이명박\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(counted_morphs.keys())\n",
    "token_to_index = {token: i+1 for i, token in enumerate(tokens)}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "index_to_token[0] = '<unknown>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_index_list(token_list):\n",
    "    token_list = eval(token_list)\n",
    "    ret = []\n",
    "    for token in token_list:\n",
    "        ret.append(token_to_index[token])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"indexed_token\"] = train_data[\"tokenized_text\"].map(token_to_index_list)\n",
    "test_data[\"indexed_token\"] = test_data[\"tokenized_text\"].map(token_to_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens_list_test = [tokens for tokens in test_data['indexed_token']]\n",
    "indexed_tokens_list_train = [tokens for tokens in train_data['indexed_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>label</th>\n",
       "      <th>indexed_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['유소영', '비호감', '성형', '아줌마']</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['나오지마라', '썅']</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['식상하고', '지긋지긋했는데', '잘', '끝나네', '오예', '소리', '벗...</td>\n",
       "      <td>6</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['성희롱', '당할', '얼굴', '아닌데', 'ㅋㅋ']</td>\n",
       "      <td>5</td>\n",
       "      <td>[16, 17, 18, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['끝', '까지', '해보자', '쪽바리', '원숭이', '자유', '한국', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[22, 23, 24, 25, 26, 27, 28, 29, 30]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_text  label  \\\n",
       "0                        ['유소영', '비호감', '성형', '아줌마']      1   \n",
       "1                                     ['나오지마라', '썅']      3   \n",
       "2  ['식상하고', '지긋지긋했는데', '잘', '끝나네', '오예', '소리', '벗...      6   \n",
       "3                   ['성희롱', '당할', '얼굴', '아닌데', 'ㅋㅋ']      5   \n",
       "4  ['끝', '까지', '해보자', '쪽바리', '원숭이', '자유', '한국', '...      0   \n",
       "\n",
       "                          indexed_token  \n",
       "0                          [1, 2, 3, 4]  \n",
       "1                                [5, 6]  \n",
       "2     [7, 8, 9, 10, 11, 12, 13, 14, 15]  \n",
       "3                  [16, 17, 18, 20, 21]  \n",
       "4  [22, 23, 24, 25, 26, 27, 28, 29, 30]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_sequences(sequences, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(seq) for seq in sequences])\n",
    "    padded_seqs = torch.zeros((len(sequences), max_len)).long()\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = min(len(seq), max_len)\n",
    "        padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_list = pad_sequences(indexed_tokens_list_train,121)\n",
    "test_input_list = pad_sequences(indexed_tokens_list_test,121)\n",
    "train_answer_list = [int(label) for label in train_data['label']]\n",
    "train_answer_list = torch.tensor(train_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65863, 121])\n",
      "torch.Size([13491, 121])\n",
      "65863\n"
     ]
    }
   ],
   "source": [
    "print(train_input_list.shape)\n",
    "print(test_input_list.shape)\n",
    "print(len(train_answer_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65863, 121, 100)\n"
     ]
    }
   ],
   "source": [
    "# tokens = list(counted_morphs.keys())\n",
    "# token_to_index = {token: i+1 for i, token in enumerate(tokens)}\n",
    "# index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "# index_to_token[0] = '<unkown>'\n",
    "\n",
    "final_train_input = []\n",
    "\n",
    "for tmp_input_list in train_input_list:\n",
    "    ret = []\n",
    "    for tmp in tmp_input_list:\n",
    "        tmp_to_word = index_to_token[int(tmp)]\n",
    "        if tmp_to_word == \"<unknown>\":\n",
    "            arr = np.zeros(100)\n",
    "        arr = fast_text_model.wv[tmp_to_word]\n",
    "        arr = arr.tolist()\n",
    "        ret.append(arr)\n",
    "    final_train_input.append(ret)\n",
    "    \n",
    "final_train_input = np.array(final_train_input)\n",
    "\n",
    "print(final_train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13491, 121, 100)\n"
     ]
    }
   ],
   "source": [
    "final_test_input = []\n",
    "\n",
    "for tmp_input_list in test_input_list:\n",
    "    ret = []\n",
    "    for tmp in tmp_input_list:\n",
    "        tmp_to_word = index_to_token[int(tmp)]\n",
    "        if tmp_to_word == \"<unknown>\":\n",
    "            arr = np.zeros(100)\n",
    "        arr = fast_text_model.wv[tmp_to_word]\n",
    "        arr = arr.tolist()\n",
    "        ret.append(arr)\n",
    "    final_test_input.append(ret)\n",
    "    \n",
    "final_test_input = np.array(final_test_input)\n",
    "\n",
    "print(final_test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_input = torch.Tensor(final_test_input)\n",
    "final_train_input = torch.Tensor(final_train_input)\n",
    "\n",
    "torch.save(final_train_input, './processed_data/fasttext_train_input.pt')\n",
    "torch.save(final_test_input,  './processed_data/fast_text_test_input.pt')\n",
    "torch.save(train_answer_list, './processed_data/train_label.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
