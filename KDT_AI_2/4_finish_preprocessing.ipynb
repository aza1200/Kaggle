{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.9\n",
    "# !pip install torchtext==0.10.1\n",
    "# Libraries\n",
    "from konlpy.tag import Okt  \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "# Preliminaries\n",
    "from torchtext.legacy.data import TabularDataset ,Field, LabelField ,BucketIterator, Example, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Models\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import ConcatDataset\n",
    "from konlpy.tag import Okt  \n",
    "# Training\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"./middle/after_tokenized_train.xlsx\")[[\"tokenized_text\",\"label\"]]\n",
    "test_data = pd.read_excel(\"./middle/after_tokenized_test.xlsx\")[[\"tokenized_text\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt=Okt()  \n",
    "with open('./middle/all_speech_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_morphs = okt.morphs(text)\n",
    "counted_morphs = Counter(all_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(counted_morphs.keys())\n",
    "token_to_index = {token: i+1 for i, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_index_list(token_list):\n",
    "    token_list = eval(token_list)\n",
    "    ret = []\n",
    "    for token in token_list:\n",
    "        ret.append(token_to_index[token])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"indexed_token\"] = train_data[\"tokenized_text\"].map(token_to_index_list)\n",
    "test_data[\"indexed_token\"] = test_data[\"tokenized_text\"].map(token_to_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>label</th>\n",
       "      <th>indexed_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['유소영', '비호감', '성형', '아줌마']</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['나오지마라', '썅']</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['식상하고', '지긋지긋했는데', '잘', '끝나네', '오예', '소리', '벗...</td>\n",
       "      <td>6</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['성희롱', '당할', '얼굴', '아닌데', 'ㅋㅋ']</td>\n",
       "      <td>5</td>\n",
       "      <td>[16, 17, 18, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['끝', '까지', '해보자', '쪽바리', '원숭이', '자유', '한국', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[22, 23, 24, 25, 26, 27, 28, 29, 30]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized_text  label  \\\n",
       "0                        ['유소영', '비호감', '성형', '아줌마']      1   \n",
       "1                                     ['나오지마라', '썅']      3   \n",
       "2  ['식상하고', '지긋지긋했는데', '잘', '끝나네', '오예', '소리', '벗...      6   \n",
       "3                   ['성희롱', '당할', '얼굴', '아닌데', 'ㅋㅋ']      5   \n",
       "4  ['끝', '까지', '해보자', '쪽바리', '원숭이', '자유', '한국', '...      0   \n",
       "\n",
       "                          indexed_token  \n",
       "0                          [1, 2, 3, 4]  \n",
       "1                                [5, 6]  \n",
       "2     [7, 8, 9, 10, 11, 12, 13, 14, 15]  \n",
       "3                  [16, 17, 18, 20, 21]  \n",
       "4  [22, 23, 24, 25, 26, 27, 28, 29, 30]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens_list_test = [tokens for tokens in test_data['indexed_token']]\n",
    "indexed_tokens_list_train = [tokens for tokens in train_data['indexed_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_sequences(sequences, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(seq) for seq in sequences])\n",
    "    padded_seqs = torch.zeros((len(sequences), max_len)).long()\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = min(len(seq), max_len)\n",
    "        padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_list = pad_sequences(indexed_tokens_list_train,121)\n",
    "test_input_list = pad_sequences(indexed_tokens_list_test,121)\n",
    "train_answer_list = [int(label) for label in train_data['label']]\n",
    "train_answer_list = torch.tensor(train_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_input_list, './processed_data/train_input.pt')\n",
    "torch.save(test_input_list,  './processed_data/test_input.pt')\n",
    "torch.save(train_answer_list, './processed_data/train_label.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
