{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./input/dataset/train.csv')\n",
    "test_df = pd.read_csv('./input/dataset/test.csv')\n",
    "\n",
    "del test_df['ID']\n",
    "del train_df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일을 읽어서 딕셔너리로 변환하는 함수\n",
    "def csv_to_dict(filename):\n",
    "    # csv 파일을 데이터프레임으로 읽기\n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    # 딕셔너리로 변환\n",
    "    my_dict = df.to_dict(orient='records')\n",
    "    # before와 after를 키-값으로 매핑한 딕셔너리 생성\n",
    "    new_dict = {}\n",
    "    for item in my_dict:\n",
    "        new_dict[item['before'].strip()] = item['after'].strip()\n",
    "    return new_dict\n",
    "\n",
    "# 예시 파일 이름과 경로\n",
    "filename = './input/word_dict.csv'\n",
    "\n",
    "# csv 파일을 딕셔너리로 변환\n",
    "word_dict = csv_to_dict(filename)\n",
    "word_key_set = set(word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, how are yu today? I hope everything is going well.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"HelloOOO, how are yoOooou today? I hope everythOoOing is going well.\"\n",
    "clean_text = re.sub(r'[Oo]{3,}', '', text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \n",
    "    # 가장먼저 word_dict 적용\n",
    "    for key,value in word_dict.items():\n",
    "        text = text.replace(key,value)\n",
    "    \n",
    "    # 이후 문장 맨앞뒤에 있는 . 특수문자 제거\",\n",
    "    text = text.replace(',\".()[]~!*-=<>@%$^&','')\n",
    "    \n",
    "    # 한국어,공백,숫자 뺴고 모두 삭제 알파벳 O 는 제외함\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\s\\dOo]+\", \"\", text)\n",
    "    \n",
    "    # 문장 맨앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    # 문장 맨앞 ㄴ 삭제\n",
    "    text = re.sub(r\"^ㄴ+\", \"\", text)\n",
    "\n",
    "    # 동일문자 3번이상 반복될시 세번째 나오는 글씨부터 그이상 삭제\n",
    "    pattern = re.compile(r\"([ㄱ-ㅎㅏ-ㅣ가-힣])\\1\\1+\")\n",
    "    text = pattern.sub(lambda x: x.group(1)*2, text)\n",
    "    \n",
    "    # 마지막으로 word_dict 한번더적용\n",
    "    for key,value in word_dict.items():\n",
    "        text = text.replace(key,value)\n",
    "    \n",
    "    # 5.18은 냅두고 18 은 -> 시발로 변경\n",
    "    if (\"5.18\" not in text) and (\"18\" in text):\n",
    "        text = text.replace(\"18\",'시발') \n",
    "    \n",
    "    # OoOo 처럼 예외적인경우 다없애기\n",
    "    text = re.sub(r'[Oo]{3,}', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "\n",
    "test_df['processed_text'] = test_df['text'].map(text_preprocessing)\n",
    "train_df['processed_text'] = train_df['text'].map(text_preprocessing)\n",
    "\n",
    "train_df['tokenized_text'] = train_df['processed_text'].apply(lambda x: okt.morphs(x))\n",
    "test_df['tokenized_text'] = test_df['processed_text'].apply(lambda x: okt.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speech_text = \" \".join(train_df.processed_text.to_numpy().tolist())\n",
    "test_speech_text = \" \".join(test_df.processed_text.to_numpy().tolist())\n",
    "\n",
    "all_speech_text = train_speech_text + test_speech_text\n",
    "with open('./middle/all_speech_text.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(all_speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('./middle/all_speech_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_morphs = okt.morphs(text)\n",
    "counted_morphs = Counter(all_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary로 변환\n",
    "top_2000_tokens = counted_morphs.most_common(2000)\n",
    "top_2000_dict = {}\n",
    "for word, count in top_2000_tokens:\n",
    "    top_2000_dict[word] = count\n",
    "\n",
    "# DataFrame으로 변환하여 출력\n",
    "top_2000_df = pd.DataFrame.from_dict(top_2000_dict, orient='index', columns=['count'])\n",
    "top_2000_df.to_excel(\"./middle/token_counter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 갯수 : 80096\n",
      "after 갯수 : 80074\n"
     ]
    }
   ],
   "source": [
    "# 토큰에서 \n",
    "# 들, 이, 가 ,은, 에 , 도 ,는 , 을 , 다, 아 , 의 , 한 , 를 ,1,2,3,4,5,6,7,8,9 제거\n",
    "\n",
    "# 최소한 제거 \n",
    "stop_words = [\"들\",\"이\",\"가\",\"은\",\"에\",\"도\",\"는\",\"을\",\"다\",\"아\",\n",
    "              \"의\",\"한\",\"를\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\n",
    "              \"8\",\"9\"]\n",
    "\n",
    "print( f\"before 갯수 : {len(counted_morphs)}\")\n",
    "subtracted_counted_morphs = Counter(\n",
    "    {word: count for word, count in counted_morphs.items() \n",
    "     if word not in stop_words}\n",
    ")\n",
    "\n",
    "print(f\"after 갯수 : {len(subtracted_counted_morphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = [\"들\",\"이\",\"가\",\"은\",\"에\",\"도\",\"는\",\"을\",\"다\",\"아\",\n",
    "              \"의\",\"한\",\"를\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\n",
    "              \"8\",\"9\"]\n",
    "    stop_word_set = set(stop_words)\n",
    "    ret_list = []\n",
    "    for tmp_token in text:\n",
    "        if tmp_token in stop_word_set:\n",
    "            continue\n",
    "        else:\n",
    "            ret_list.append(tmp_token)\n",
    "    \n",
    "    return ret_list\n",
    "\n",
    "# stop_words 제거\n",
    "test_df['tokenized_text'] = test_df['tokenized_text'].map(remove_stopwords)\n",
    "train_df['tokenized_text'] = train_df['tokenized_text'].map(remove_stopwords)\n",
    "token_counter_df = pd.DataFrame(subtracted_counted_morphs.items(),columns=[\"단어\",\"빈도수\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel('./middle/after_tokenized_test.xlsx',encoding='utf-8', index=False)\n",
    "train_df.to_excel('./middle/after_tokenized_train.xlsx',encoding='utf-8', index=False)\n",
    "token_counter_df.to_excel('./middle/all_tokens.xlsx',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한남충들 미투에 얼마나 관심을 가졌다고 지겹다고 웅앵웅이노 앙 재기민기띠ㅋㅋ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['한남충',\n",
       " '들',\n",
       " '미투',\n",
       " '에',\n",
       " '얼마나',\n",
       " '관심',\n",
       " '을',\n",
       " '가졌다고',\n",
       " '지겹다고',\n",
       " '웅앵웅',\n",
       " '이노',\n",
       " '앙',\n",
       " '재기',\n",
       " '민',\n",
       " '기',\n",
       " '띠',\n",
       " 'ㅋㅋ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"한남충들 미투에 얼마나 관심을 가졌다고 지겹다고 웅앵웅이노 앙 재기민기띠ㅋㅋ\"\n",
    "\n",
    "a = text_preprocessing(a)\n",
    "print(a)\n",
    "\n",
    "okt.morphs(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
