{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.load('./processed_data/train_input.pt')\n",
    "train_output = torch.load('./processed_data/train_label.pt')\n",
    "test_input = torch.load('./processed_data/test_input.pt')\n",
    "vocab_number = 80096\n",
    "label_number = 7\n",
    "batch_size = 32\n",
    "n_hidden = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65863, 121])\n",
      "torch.Size([13491, 121])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,tokens, labels=None ,is_test=False):\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input = self.tokens[idx]\n",
    "        label = None\n",
    "        if not self.is_test:\n",
    "            label = self.labels[idx]\n",
    "        return (input, label) if not self.is_test else (input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextDataset(tokens=train_input, labels=train_output, is_test=False)\n",
    "dataset_test = TextDataset(tokens=test_input, is_test=True)\n",
    "loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextLSTM, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_number, 125)\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_size=125, \n",
    "      hidden_size=125, \n",
    "      num_layers=1,\n",
    "      dropout=0.5,\n",
    "      bidirectional=True,\n",
    "    )\n",
    "    self.fc = nn.Linear(2*125, label_number)\n",
    "    \n",
    "  def forward(self, hidden_and_cell,X):\n",
    "    X = self.embedding(X)\n",
    "    X = X.transpose(0,1)\n",
    "    outputs, (hidden_forward, hidden_backward) = self.lstm(X, hidden_and_cell)\n",
    "    \n",
    "    # forward, backward 방향의 hidden state 를 합침\n",
    "    outputs = torch.cat(\n",
    "      (hidden_forward[-1], hidden_backward[-1]), dim=1\n",
    "    )  \n",
    "    outputs = self.fc(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#  GPU 장비 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = TextLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 오차 함수 조정 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0006, weight_decay=0.001) \n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(loader_train)*3, \n",
    "    num_training_steps=len(loader_train)*20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, criterion, optimizer, scheduler=None, epochs=20, \n",
    "          save_file='./models/bidirectional_classifier.pth'):\n",
    "    \n",
    "    hidden = torch.zeros(2, batch_size, n_hidden, requires_grad=True).to(device)\n",
    "    cell = torch.zeros(2, batch_size, n_hidden, requires_grad=True).to(device)\n",
    "\n",
    "    train_loss_min = np.inf\n",
    "    # 총 epoch 만큼 반복\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        preds_list = []\n",
    "        true_list = []\n",
    "        print(f'에폭 [{epoch+1}/{epochs}] \\n---------------------')\n",
    "        # == [훈련] =========\n",
    "        model.train() # 모델을 훈련 상태로 설정\n",
    "        epoch_train_loss = 0 # 에폭별 손실값 초기화(훈련데이터 용)\n",
    "        \n",
    "        # 미니배치 단위로 훈련\n",
    "        for tokens , labels in tqdm(loader_train):\n",
    "            tokens = tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model((hidden,cell),tokens)\n",
    "\n",
    "            preds_list.extend(outputs)\n",
    "            true_list.extend(labels)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "            if scheduler != None: # 스케줄러 학습률 갱신\n",
    "                scheduler.step()\n",
    "                \n",
    "        print(f'\\t 훈련 데이터 손실값 : {epoch_train_loss/len(loader_train):.4f}')\n",
    "        \n",
    "        if epoch_train_loss <= train_loss_min:\n",
    "            print(f'\\t### 훈련 데이터 손실값 감소 ({train_loss_min:.4f} --> {epoch_train_loss:.4f}). 모델저장')\n",
    "            torch.save(model.state_dict(), save_file)\n",
    "            train_loss_min = epoch_train_loss\n",
    "        # == [최적 모델 가중치 찾기] ==\n",
    "        # 현 에폭에서의 검증 데이터 손실값이 지금까지 가장 작다면\n",
    "            # 현 에폭의 모델 가중치 (현재까지의 최적 모델 가중치 ) 저장\n",
    "    return torch.load(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭 [1/20] \n",
      "---------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d403470e38f7468c879c8f82e7cf123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2059 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33036\\781471458.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_state_dict = train(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     save_file=f'./models/bidirectional_classifier.pth')\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33036\\106274731.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader_train, criterion, optimizer, scheduler, epochs, save_file)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_state_dict = train(\n",
    "    model=model, loader_train=loader_train,\n",
    "    criterion=criterion, optimizer=optimizer,\n",
    "    scheduler=scheduler, epochs=20,\n",
    "    save_file=f'./bidirectional_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 모델을 평가 상태로 설정\n",
    "final_preds = [] # 예측값 저장용 리스트 초기화\n",
    "\n",
    "hidden = torch.zeros(2, batch_size, n_hidden, requires_grad=True).to(device)\n",
    "cell = torch.zeros(2, batch_size, n_hidden, requires_grad=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tokens in loader_test: # batch 갯수때문에 복수형\n",
    "        tokens = tokens.to(device)\n",
    "        outputs = model((hidden,cell),tokens)\n",
    "        pred = torch.max(outputs.cpu(), dim=1)[1].numpy() # 예측값\n",
    "\n",
    "        final_preds.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 32개에 맞춰서 마지막 테스트 데이터 예측\n",
    "import numpy as np\n",
    "last_input = test_input[-32:]\n",
    "\n",
    "last_input = last_input.to(device)\n",
    "outputs = model((hidden,cell),tokens)\n",
    "last_pred = torch.max(outputs.cpu(), dim=1)[1].numpy() # 예측값\n",
    "print(last_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13491\n",
      "13491\n"
     ]
    }
   ],
   "source": [
    "final_output = np.load('./output/final_output.npy')\n",
    "print(len(final_output))\n",
    "print(len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = final_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.read_csv(\"input/dataset/test.csv\")\n",
    "submission_df['label']= final_output\n",
    "submission_df = submission_df[['ID','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./output/jhkim_1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
