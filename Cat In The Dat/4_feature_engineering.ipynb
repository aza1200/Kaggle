{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input directory path : ./input/\n",
      "train.shape : (300000, 24)\n",
      "test.shape : (200000, 23)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "# 캐글 ↓제출경로\n",
    "data_path = '/kaggle/input/cat-in-the-dat/'\n",
    "if not path.exists(data_path):\n",
    "    data_path = './input/'\n",
    "print(f'input directory path : {data_path}')\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col='id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col='id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')\n",
    "\n",
    "print(f'train.shape : {train.shape}')\n",
    "print(f'test.shape : {test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 피처 인코딩\n",
    "# 순서형 피처 인코딩\n",
    "# 명목형 날짜 피처 인코딩\n",
    "# 모든 작업이 끝난후 합치기 \n",
    "\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거\n",
    "\n",
    "all_data['bin_3'] = all_data['bin_3'].map({'F': 0, 'T':1})\n",
    "all_data['bin_4'] = all_data['bin_4'].map({'N': 0, 'Y':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord1dict = {'Novice':0, 'Contributor':1, 'Expert':2, 'Master':3, 'Grandmaster':4}\n",
    "ord2dict = {'Freezing':0, 'Cold':1 , 'Warm':2, 'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}\n",
    "\n",
    "all_data['ord_1'] = all_data['ord_1'].map(ord1dict)\n",
    "all_data['ord_2'] = all_data['ord_2'].map(ord2dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord_3\n",
      "['a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o']\n",
      "ord_4\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
      " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']\n",
      "ord_5\n",
      "['AP' 'Ai' 'Aj' 'BA' 'BE' 'Bb' 'Bd' 'Bn' 'CL' 'CM' 'CU' 'CZ' 'Cl' 'DH'\n",
      " 'DN' 'Dc' 'Dx' 'Ed' 'Eg' 'Er' 'FI' 'Fd' 'Fo' 'GD' 'GJ' 'Gb' 'Gx' 'Hj'\n",
      " 'IK' 'Id' 'JX' 'Jc' 'Jf' 'Jt' 'KR' 'KZ' 'Kf' 'Kq' 'LE' 'MC' 'MO' 'MV'\n",
      " 'Mf' 'Ml' 'Mx' 'NV' 'Nf' 'Nk' 'OR' 'Ob' 'Os' 'PA' 'PQ' 'PZ' 'Ps' 'QM'\n",
      " 'Qb' 'Qh' 'Qo' 'RG' 'RL' 'RP' 'Rm' 'Ry' 'SB' 'Sc' 'TR' 'TZ' 'To' 'UO'\n",
      " 'Uk' 'Uu' 'Vf' 'Vx' 'WE' 'Wc' 'Wv' 'XI' 'Xh' 'Xi' 'YC' 'Yb' 'Ye' 'ZR'\n",
      " 'ZS' 'Zc' 'Zq' 'aF' 'aM' 'aO' 'aP' 'ac' 'av' 'bF' 'bJ' 'be' 'cA' 'cG'\n",
      " 'cW' 'ck' 'cp' 'dB' 'dE' 'dN' 'dO' 'dP' 'dQ' 'dZ' 'dh' 'eG' 'eQ' 'eb'\n",
      " 'eg' 'ek' 'ex' 'fO' 'fh' 'gJ' 'gM' 'hL' 'hT' 'hh' 'hp' 'iT' 'ih' 'jS'\n",
      " 'jV' 'je' 'jp' 'kC' 'kE' 'kK' 'kL' 'kU' 'kW' 'ke' 'kr' 'kw' 'lF' 'lL'\n",
      " 'll' 'lx' 'mb' 'mc' 'mm' 'nX' 'nh' 'oC' 'oG' 'oH' 'oK' 'od' 'on' 'pa'\n",
      " 'ps' 'qA' 'qJ' 'qK' 'qP' 'qX' 'qo' 'qv' 'qw' 'rZ' 'ri' 'rp' 'sD' 'sV'\n",
      " 'sY' 'sn' 'su' 'tM' 'tP' 'tv' 'uJ' 'uS' 'ud' 'us' 'ut' 'ux' 'uy' 'vK'\n",
      " 'vq' 'vy' 'wu' 'wy' 'xP' 'xy' 'yN' 'yY' 'yc' 'zU']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_345 = ['ord_3', 'ord_4', 'ord_5']\n",
    "\n",
    "ord_encoder = OrdinalEncoder() # Ordinal Encoder 객체생성\n",
    "# ordinal 인코딩 적용\n",
    "all_data[ord_345] = ord_encoder.fit_transform(all_data[ord_345])\n",
    "\n",
    "for feature, categories in zip(ord_345, ord_encoder.categories_):\n",
    "    print(feature)\n",
    "    print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_features = ['nom_' + str(i) for i in range(10)] # 명목형 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x16276 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5000000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정상 실행되는 코드\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder() # OneHotEncoder 객체 생성\n",
    "encoded_nom_matrix = onehot_encoder.fit_transform(all_data[nom_features])\n",
    "\n",
    "encoded_nom_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(nom_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x19 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1000000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜 피처 인코딩\n",
    "date_features = ['day', 'month']\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "encoded_date_matrix = onehot_encoder.fit_transform(all_data[date_features])\n",
    "\n",
    "all_data = all_data.drop(date_features, axis=1) # 기존 날짜 피처 삭제\n",
    "\n",
    "encoded_date_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 스케일링\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ord_features = ['ord_' + str(i) for i in range(6)]\n",
    "# min-max 정규화\n",
    "all_data[ord_features] = MinMaxScaler().fit_transform(all_data[ord_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 및 스케일링된 피처 합치기 \n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data),\n",
    "                               encoded_nom_matrix,\n",
    "                               encoded_date_matrix],\n",
    "                               format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x16306 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9163718 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_sprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X_train = all_data_sprs[:num_train] # 0 ~ num_train -1 행\n",
    "X_test = all_data_sprs[num_train:] # num_train ~ 마지막 행\n",
    "\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test_size=0 should be either positive and smaller than the number of samples 300000 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17792\\1239073989.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 훈련 데이터, 검증 데이터 분리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m X_train, X_valid, y_train, y_valid = train_test_split(X_train, y,\n\u001b[0m\u001b[0;32m      5\u001b[0m                                                       \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                       \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[0;32m   2421\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m     )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m     ):\n\u001b[1;32m-> 2043\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2044\u001b[0m             \u001b[1;34m\"test_size={0} should be either positive and smaller\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m             \u001b[1;34m\" than the number of samples {1} or a float in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: test_size=0 should be either positive and smaller than the number of samples 300000 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련 데이터, 검증 데이터 분리\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y,\n",
    "                                                      test_size=0.1,\n",
    "                                                      stratify=y,\n",
    "                                                      random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼파라미터:  {'C': 0.125, 'max_iter': 800, 'random_state': 42, 'solver': 'liblinear'}\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 로지스틱 회귀 모델 생성\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# 하이퍼 파라미터 값 목록\n",
    "lr_params = {\n",
    "    'C': [0.1, 0.125, 0.2], 'max_iter':[800, 900, 1000],\n",
    "    'solver': ['liblinear'], 'random_state': [42]\n",
    "}\n",
    "\n",
    "# 그리드 서치 객체 생성\n",
    "gridsearch_logistic_model = GridSearchCV(\n",
    "    estimator = logistic_model,\n",
    "    param_grid=lr_params,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# 그리드서치 수행\n",
    "gridsearch_logistic_model.fit(X_train, y_train)\n",
    "\n",
    "print('최적 하이퍼파라미터: ', gridsearch_logistic_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_preds = gridsearch_logistic_model.predict_proba(X_valid)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 ROC AUC : 0.8045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score # ROC AUC 점수 계산 함수\n",
    "\n",
    "# 검증 데이터 ROC AUC\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print(f'검증 데이터 ROC AUC : {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타깃값 1일 확률 예측 \n",
    "y_preds = gridsearch_logistic_model.best_estimator_.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission['target'] = y_preds\n",
    "\n",
    "if data_path == \"./input/\":\n",
    "    submission.to_csv('./output/'+'submission.csv')\n",
    "else:\n",
    "    submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
